{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3ab07f",
   "metadata": {},
   "source": [
    "## non iid fedAVG FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6356e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 101, (1,): 1102}\n",
      "Client client_2: {(0,): 1047, (1,): 156}\n",
      "Client client_3: {(0,): 391, (1,): 812}\n",
      "Client client_4: {(0,): 169, (1,): 1034}\n",
      "Client client_5: {(0,): 469, (1,): 734}\n",
      "Client client_6: {(0,): 736, (1,): 467}\n",
      "Client client_7: {(0,): 443, (1,): 760}\n",
      "Client client_8: {(0,): 1004, (1,): 199}\n",
      "Client client_9: {(0,): 530, (1,): 673}\n",
      "Client client_10: {(0,): 1005, (1,): 198}\n",
      "|=======================|\n",
      "|Traditional FedAvg-non-iid-FGSM  2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 37.001% | global_loss: 0.7028960585594177 | global_f1: 0.5392657427668368 | global_precision: 0.3691744340878828 | global_recall: 1.0 | global_auc: 0.5221705600831152| flobal_FPR: 0.0 \n",
      "comm_round: 1 | global_acc: 37.068% | global_loss: 0.7003544569015503 | global_f1: 0.5390796201607012 | global_precision: 0.36924616410940625 | global_recall: 0.9981965734896303 | global_auc: 0.6234969665112529| flobal_FPR: 0.0018034265103697023 \n",
      "comm_round: 2 | global_acc: 37.633% | global_loss: 0.6981492638587952 | global_f1: 0.541095890410959 | global_precision: 0.3712655253440752 | global_recall: 0.9972948602344455 | global_auc: 0.6904606904777846| flobal_FPR: 0.002705139765554554 \n",
      "comm_round: 3 | global_acc: 38.497% | global_loss: 0.6962838768959045 | global_f1: 0.5445593303791236 | global_precision: 0.37453437182526245 | global_recall: 0.9972948602344455 | global_auc: 0.7464086978529347| flobal_FPR: 0.002705139765554554 \n",
      "comm_round: 4 | global_acc: 39.727% | global_loss: 0.6947066783905029 | global_f1: 0.5495652173913044 | global_precision: 0.3792866941015089 | global_recall: 0.9972948602344455 | global_auc: 0.7815294557289181| flobal_FPR: 0.002705139765554554 \n",
      "comm_round: 5 | global_acc: 44.282% | global_loss: 0.6922109127044678 | global_f1: 0.568708183221822 | global_precision: 0.39791141519625495 | global_recall: 0.9963931469792606 | global_auc: 0.8245709502082392| flobal_FPR: 0.0036068530207394047 \n",
      "comm_round: 6 | global_acc: 47.074% | global_loss: 0.6908934712409973 | global_f1: 0.5808320168509742 | global_precision: 0.41018966158423203 | global_recall: 0.9945897204688909 | global_auc: 0.8413905852399179| flobal_FPR: 0.005410279531109108 \n",
      "comm_round: 7 | global_acc: 49.668% | global_loss: 0.6894821524620056 | global_f1: 0.5923532579429187 | global_precision: 0.42226487523992323 | global_recall: 0.9918845807033363 | global_auc: 0.8548956287087646| flobal_FPR: 0.008115419296663661 \n",
      "comm_round: 8 | global_acc: 55.352% | global_loss: 0.6871672868728638 | global_f1: 0.6194389345423633 | global_precision: 0.4516528925619835 | global_recall: 0.9855725879170424 | global_auc: 0.8692100298624259| flobal_FPR: 0.014427412082957619 \n",
      "comm_round: 9 | global_acc: 58.777% | global_loss: 0.6854008436203003 | global_f1: 0.6376388077147866 | global_precision: 0.47168179853004755 | global_recall: 0.9837691614066727 | global_auc: 0.8759904482022953| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 10 | global_acc: 61.436% | global_loss: 0.6840146780014038 | global_f1: 0.652486518873577 | global_precision: 0.48855989232839836 | global_recall: 0.981965734896303 | global_auc: 0.880124131584608| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 11 | global_acc: 63.231% | global_loss: 0.6831379532814026 | global_f1: 0.6638297872340425 | global_precision: 0.5006877579092159 | global_recall: 0.9846708746618575 | global_auc: 0.883980273419972| flobal_FPR: 0.015329125338142471 \n",
      "comm_round: 12 | global_acc: 67.055% | global_loss: 0.6806652545928955 | global_f1: 0.6870855699400061 | global_precision: 0.5286686103012633 | global_recall: 0.9810640216411182 | global_auc: 0.8886847094788153| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 13 | global_acc: 67.520% | global_loss: 0.6798827648162842 | global_f1: 0.6905289832119101 | global_precision: 0.5322265625 | global_recall: 0.9828674481514879 | global_auc: 0.8869261549550782| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 14 | global_acc: 69.914% | global_loss: 0.677922785282135 | global_f1: 0.706645056726094 | global_precision: 0.5516194331983806 | global_recall: 0.9828674481514879 | global_auc: 0.8942934703899494| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 15 | global_acc: 73.969% | global_loss: 0.6724570393562317 | global_f1: 0.7334014300306436 | global_precision: 0.5891684901531729 | global_recall: 0.9711451758340848 | global_auc: 0.9032258447448258| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 16 | global_acc: 75.266% | global_loss: 0.6698684692382812 | global_f1: 0.7427385892116182 | global_precision: 0.6023555804823332 | global_recall: 0.9684400360685302 | global_auc: 0.9084967599576637| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 17 | global_acc: 72.806% | global_loss: 0.6725572943687439 | global_f1: 0.7258713136729222 | global_precision: 0.5776 | global_recall: 0.9765554553651938 | global_auc: 0.9057707274152644| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 18 | global_acc: 73.271% | global_loss: 0.6706752181053162 | global_f1: 0.728744939271255 | global_precision: 0.5822102425876011 | global_recall: 0.9738503155996393 | global_auc: 0.8980539328040814| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 19 | global_acc: 75.366% | global_loss: 0.6657581925392151 | global_f1: 0.7426189649183743 | global_precision: 0.603954802259887 | global_recall: 0.9639314697926059 | global_auc: 0.9048713883392664| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 20 | global_acc: 77.194% | global_loss: 0.6601575613021851 | global_f1: 0.7560455192034139 | global_precision: 0.624192601291838 | global_recall: 0.9585211902614968 | global_auc: 0.9083184591007274| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 21 | global_acc: 77.693% | global_loss: 0.6519675254821777 | global_f1: 0.7553773240977033 | global_precision: 0.6340269277845777 | global_recall: 0.9341749323715058 | global_auc: 0.9116173810809257| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 22 | global_acc: 78.191% | global_loss: 0.6404123902320862 | global_f1: 0.7575757575757577 | global_precision: 0.6418284283030683 | global_recall: 0.9242560865644724 | global_auc: 0.9119549893613031| flobal_FPR: 0.0757439134355275 \n",
      "comm_round: 23 | global_acc: 79.089% | global_loss: 0.6318061351776123 | global_f1: 0.7660840461137969 | global_precision: 0.6518987341772152 | global_recall: 0.9287646528403968 | global_auc: 0.9190257698157305| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 24 | global_acc: 80.020% | global_loss: 0.6248258948326111 | global_f1: 0.7769944341372913 | global_precision: 0.6601513240857503 | global_recall: 0.9440937781785392 | global_auc: 0.9283736255283144| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 25 | global_acc: 79.555% | global_loss: 0.6168999671936035 | global_f1: 0.7733136748986362 | global_precision: 0.6539900249376559 | global_recall: 0.9458972046889089 | global_auc: 0.9208676580289279| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 26 | global_acc: 79.322% | global_loss: 0.6034071445465088 | global_f1: 0.7708179808400885 | global_precision: 0.6517133956386293 | global_recall: 0.9431920649233544 | global_auc: 0.9160238576518135| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 27 | global_acc: 78.524% | global_loss: 0.6007847785949707 | global_f1: 0.7659420289855072 | global_precision: 0.6402180496668686 | global_recall: 0.9531109107303878 | global_auc: 0.918629281891518| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 28 | global_acc: 79.854% | global_loss: 0.5801153779029846 | global_f1: 0.7775330396475771 | global_precision: 0.6557275541795665 | global_recall: 0.9549143372407575 | global_auc: 0.9318297181706854| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 29 | global_acc: 79.422% | global_loss: 0.5800706744194031 | global_f1: 0.7751543770432254 | global_precision: 0.6490267639902676 | global_recall: 0.9621280432822362 | global_auc: 0.9383933739507908| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 30 | global_acc: 80.319% | global_loss: 0.5563676953315735 | global_f1: 0.7826725403817916 | global_precision: 0.660061919504644 | global_recall: 0.9612263300270514 | global_auc: 0.939890768763969| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 31 | global_acc: 80.153% | global_loss: 0.5482358932495117 | global_f1: 0.7813987550347858 | global_precision: 0.657829839704069 | global_recall: 0.9621280432822362 | global_auc: 0.9479988281051533| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 32 | global_acc: 81.084% | global_loss: 0.5007355809211731 | global_f1: 0.7896487985212569 | global_precision: 0.6691729323308271 | global_recall: 0.9630297565374211 | global_auc: 0.9518730137023377| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 33 | global_acc: 80.718% | global_loss: 0.4884917438030243 | global_f1: 0.7873900293255133 | global_precision: 0.663372452130945 | global_recall: 0.9684400360685302 | global_auc: 0.9587220458207086| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 34 | global_acc: 81.549% | global_loss: 0.4676801860332489 | global_f1: 0.7945205479452055 | global_precision: 0.6739949748743719 | global_recall: 0.9675383228133454 | global_auc: 0.9682106903590757| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 35 | global_acc: 81.848% | global_loss: 0.43377885222435 | global_f1: 0.7965722801788376 | global_precision: 0.6787301587301587 | global_recall: 0.9639314697926059 | global_auc: 0.966542117226522| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 36 | global_acc: 81.682% | global_loss: 0.42158302664756775 | global_f1: 0.7956989247311828 | global_precision: 0.6756926952141058 | global_recall: 0.9675383228133454 | global_auc: 0.973184120919795| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 37 | global_acc: 84.973% | global_loss: 0.3743235468864441 | global_f1: 0.8257517347725519 | global_precision: 0.7212121212121212 | global_recall: 0.9657348963029756 | global_auc: 0.9786532800947392| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 38 | global_acc: 84.309% | global_loss: 0.36599212884902954 | global_f1: 0.8195718654434252 | global_precision: 0.7113470471134705 | global_recall: 0.9666366095581606 | global_auc: 0.9786770218866081| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 39 | global_acc: 89.794% | global_loss: 0.3398442268371582 | global_f1: 0.8743348342202211 | global_precision: 0.800599700149925 | global_recall: 0.9630297565374211 | global_auc: 0.9813242316800024| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 40 | global_acc: 91.090% | global_loss: 0.3100171685218811 | global_f1: 0.8878661087866109 | global_precision: 0.8282591725214676 | global_recall: 0.9567177637511272 | global_auc: 0.9824856801382342| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 41 | global_acc: 91.789% | global_loss: 0.29600757360458374 | global_f1: 0.8956485002112378 | global_precision: 0.8426073131955485 | global_recall: 0.9558160504959423 | global_auc: 0.9829230039444613| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 42 | global_acc: 93.451% | global_loss: 0.27463576197624207 | global_f1: 0.9137855579868709 | global_precision: 0.8877551020408163 | global_recall: 0.9413886384129847 | global_auc: 0.9838679272608477| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 43 | global_acc: 93.517% | global_loss: 0.24633267521858215 | global_f1: 0.9153278332609639 | global_precision: 0.8827470686767169 | global_recall: 0.9504057709648331 | global_auc: 0.984956251000123| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 44 | global_acc: 93.750% | global_loss: 0.2276352345943451 | global_f1: 0.918189730200174 | global_precision: 0.8873002523128679 | global_recall: 0.951307484220018 | global_auc: 0.9854690737044933| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 45 | global_acc: 93.750% | global_loss: 0.21459278464317322 | global_f1: 0.9182608695652174 | global_precision: 0.8866498740554156 | global_recall: 0.9522091974752029 | global_auc: 0.9859799970655145| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 46 | global_acc: 93.949% | global_loss: 0.2067219763994217 | global_f1: 0.9204545454545454 | global_precision: 0.8931297709923665 | global_recall: 0.9495040577096483 | global_auc: 0.9864011764532706| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 47 | global_acc: 94.781% | global_loss: 0.1787770837545395 | global_f1: 0.9300044583147571 | global_precision: 0.9197530864197531 | global_recall: 0.9404869251577999 | global_auc: 0.9876960537818062| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 48 | global_acc: 95.113% | global_loss: 0.1650846004486084 | global_f1: 0.9344042838018741 | global_precision: 0.9249116607773852 | global_recall: 0.9440937781785392 | global_auc: 0.9886414519340301| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 49 | global_acc: 94.980% | global_loss: 0.16289883852005005 | global_f1: 0.9325591782045557 | global_precision: 0.9238938053097345 | global_recall: 0.9413886384129847 | global_auc: 0.988718850175523| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 50 | global_acc: 95.113% | global_loss: 0.15797190368175507 | global_f1: 0.9341692789968652 | global_precision: 0.9279359430604982 | global_recall: 0.9404869251577999 | global_auc: 0.9888745963301837| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 51 | global_acc: 94.880% | global_loss: 0.1635792851448059 | global_f1: 0.9317375886524822 | global_precision: 0.916303400174368 | global_recall: 0.9477006311992786 | global_auc: 0.9889334759740189| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 52 | global_acc: 94.947% | global_loss: 0.15847042202949524 | global_f1: 0.9323241317898486 | global_precision: 0.920844327176781 | global_recall: 0.9440937781785392 | global_auc: 0.9889087845104751| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 53 | global_acc: 94.980% | global_loss: 0.1561521589756012 | global_f1: 0.9332153914197258 | global_precision: 0.9157986111111112 | global_recall: 0.951307484220018 | global_auc: 0.9894591192459985| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 54 | global_acc: 95.180% | global_loss: 0.14519621431827545 | global_f1: 0.9354120267260579 | global_precision: 0.9242957746478874 | global_recall: 0.9467989179440938 | global_auc: 0.9898812483054296| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 55 | global_acc: 95.113% | global_loss: 0.15083956718444824 | global_f1: 0.9348115299334812 | global_precision: 0.9197207678883071 | global_recall: 0.9504057709648331 | global_auc: 0.989689414627128| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 56 | global_acc: 95.080% | global_loss: 0.1483355313539505 | global_f1: 0.9338695263628238 | global_precision: 0.9255978742249779 | global_recall: 0.9422903516681695 | global_auc: 0.989563108294385| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 57 | global_acc: 95.246% | global_loss: 0.14315985143184662 | global_f1: 0.936359590565198 | global_precision: 0.9244288224956063 | global_recall: 0.9486023444544635 | global_auc: 0.9901998631523117| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 58 | global_acc: 95.180% | global_loss: 0.1405179351568222 | global_f1: 0.9351810460438087 | global_precision: 0.9273049645390071 | global_recall: 0.9431920649233544 | global_auc: 0.9901936902864257| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 59 | global_acc: 95.379% | global_loss: 0.12976285815238953 | global_f1: 0.9375280898876405 | global_precision: 0.9345878136200717 | global_recall: 0.9404869251577999 | global_auc: 0.9907359528127138| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 60 | global_acc: 95.479% | global_loss: 0.14044128358364105 | global_f1: 0.9397697077059344 | global_precision: 0.9234116623150566 | global_recall: 0.9567177637511272 | global_auc: 0.9906143948383446| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 61 | global_acc: 95.312% | global_loss: 0.13961194455623627 | global_f1: 0.9377483443708609 | global_precision: 0.9186851211072664 | global_recall: 0.957619477006312 | global_auc: 0.9907653926346314| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 62 | global_acc: 95.246% | global_loss: 0.1439848393201828 | global_f1: 0.9373081981587023 | global_precision: 0.9121160409556314 | global_recall: 0.9639314697926059 | global_auc: 0.9907867602473135| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 63 | global_acc: 95.678% | global_loss: 0.13041208684444427 | global_f1: 0.9425795053003534 | global_precision: 0.9238095238095239 | global_recall: 0.9621280432822362 | global_auc: 0.99122598339689| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 64 | global_acc: 95.944% | global_loss: 0.12186046689748764 | global_f1: 0.9455842997323818 | global_precision: 0.9355692850838482 | global_recall: 0.9558160504959423 | global_auc: 0.9915417492287479| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 65 | global_acc: 95.944% | global_loss: 0.11676139384508133 | global_f1: 0.9453405017921147 | global_precision: 0.9394479073909172 | global_recall: 0.951307484220018 | global_auc: 0.9918057579543313| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 66 | global_acc: 95.977% | global_loss: 0.11412598192691803 | global_f1: 0.9457642312864187 | global_precision: 0.9402852049910874 | global_recall: 0.951307484220018 | global_auc: 0.99182047786529| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 67 | global_acc: 96.177% | global_loss: 0.11029358953237534 | global_f1: 0.9483146067415731 | global_precision: 0.9453405017921147 | global_recall: 0.951307484220018 | global_auc: 0.9921305456670992| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 68 | global_acc: 96.011% | global_loss: 0.11036858707666397 | global_f1: 0.9461883408071748 | global_precision: 0.9411239964317574 | global_recall: 0.951307484220018 | global_auc: 0.9921538126231308| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 69 | global_acc: 96.077% | global_loss: 0.11330354958772659 | global_f1: 0.9470377019748654 | global_precision: 0.9428060768543343 | global_recall: 0.951307484220018 | global_auc: 0.9918494428513702| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 70 | global_acc: 96.177% | global_loss: 0.10986217856407166 | global_f1: 0.9483146067415731 | global_precision: 0.9453405017921147 | global_recall: 0.951307484220018 | global_auc: 0.9920408016938345| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 71 | global_acc: 96.210% | global_loss: 0.11096298694610596 | global_f1: 0.9493333333333334 | global_precision: 0.9360210341805434 | global_recall: 0.9630297565374211 | global_auc: 0.9926471670581688| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 72 | global_acc: 96.210% | global_loss: 0.11000409722328186 | global_f1: 0.9492430988423864 | global_precision: 0.9375549692172384 | global_recall: 0.9612263300270514 | global_auc: 0.9922620751940534| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 73 | global_acc: 96.044% | global_loss: 0.11285458505153656 | global_f1: 0.9473684210526314 | global_precision: 0.9296875 | global_recall: 0.9657348963029756 | global_auc: 0.9924634055891027| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 74 | global_acc: 96.011% | global_loss: 0.11453869193792343 | global_f1: 0.9469496021220158 | global_precision: 0.9288811795316565 | global_recall: 0.9657348963029756 | global_auc: 0.9925279832629864| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 75 | global_acc: 96.077% | global_loss: 0.11168818920850754 | global_f1: 0.9473684210526315 | global_precision: 0.9373345101500441 | global_recall: 0.957619477006312 | global_auc: 0.9920493487389073| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 76 | global_acc: 96.343% | global_loss: 0.10763464868068695 | global_f1: 0.951067615658363 | global_precision: 0.9385425812115891 | global_recall: 0.9639314697926059 | global_auc: 0.9925422283381078| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 77 | global_acc: 96.243% | global_loss: 0.10747702419757843 | global_f1: 0.9498446515756768 | global_precision: 0.9353146853146853 | global_recall: 0.9648331830477908 | global_auc: 0.9928594186774777| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 78 | global_acc: 96.609% | global_loss: 0.0995020791888237 | global_f1: 0.9543010752688171 | global_precision: 0.9483526268922529 | global_recall: 0.9603246167718665 | global_auc: 0.9928252304971864| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 79 | global_acc: 96.443% | global_loss: 0.1022656038403511 | global_f1: 0.9522534582775546 | global_precision: 0.9425795053003534 | global_recall: 0.9621280432822362 | global_auc: 0.9927791714209604| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 80 | global_acc: 96.343% | global_loss: 0.10397746413946152 | global_f1: 0.9510240427426535 | global_precision: 0.9393139841688655 | global_recall: 0.9630297565374211 | global_auc: 0.9928869591560457| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 81 | global_acc: 96.509% | global_loss: 0.09781690686941147 | global_f1: 0.9530621367903442 | global_precision: 0.9450354609929078 | global_recall: 0.9612263300270514 | global_auc: 0.9931447950157433| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 82 | global_acc: 96.410% | global_loss: 0.0999065488576889 | global_f1: 0.951828724353256 | global_precision: 0.941747572815534 | global_recall: 0.9621280432822362 | global_auc: 0.9931481188666048| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 83 | global_acc: 96.476% | global_loss: 0.09727292507886887 | global_f1: 0.9526785714285714 | global_precision: 0.9434129089301503 | global_recall: 0.9621280432822362 | global_auc: 0.9932240926005856| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 84 | global_acc: 96.676% | global_loss: 0.09454875439405441 | global_f1: 0.9551971326164875 | global_precision: 0.9492430988423864 | global_recall: 0.9612263300270514 | global_auc: 0.9933489744258167| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 85 | global_acc: 96.676% | global_loss: 0.09429353475570679 | global_f1: 0.9551971326164875 | global_precision: 0.9492430988423864 | global_recall: 0.9612263300270514 | global_auc: 0.9932934186328432| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 86 | global_acc: 96.476% | global_loss: 0.09630665928125381 | global_f1: 0.9527629233511586 | global_precision: 0.9418502202643172 | global_recall: 0.9639314697926059 | global_auc: 0.9935398584324434| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 87 | global_acc: 96.676% | global_loss: 0.09246194362640381 | global_f1: 0.9552372426141451 | global_precision: 0.9484444444444444 | global_recall: 0.9621280432822362 | global_auc: 0.993678035661121| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 88 | global_acc: 96.709% | global_loss: 0.09238888323307037 | global_f1: 0.9555854643337819 | global_precision: 0.9508928571428571 | global_recall: 0.9603246167718665 | global_auc: 0.9934895258336812| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 89 | global_acc: 96.775% | global_loss: 0.09233808517456055 | global_f1: 0.9565606806986117 | global_precision: 0.9501779359430605 | global_recall: 0.9630297565374211 | global_auc: 0.9936642654218371| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 90 | global_acc: 96.476% | global_loss: 0.09383933991193771 | global_f1: 0.9526785714285714 | global_precision: 0.9434129089301503 | global_recall: 0.9621280432822362 | global_auc: 0.9936666396010239| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 91 | global_acc: 96.576% | global_loss: 0.09237325191497803 | global_f1: 0.9540383757251227 | global_precision: 0.9443462897526502 | global_recall: 0.9639314697926059 | global_auc: 0.9938622719660246| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 92 | global_acc: 96.775% | global_loss: 0.09148448705673218 | global_f1: 0.9565995525727069 | global_precision: 0.9493783303730018 | global_recall: 0.9639314697926059 | global_auc: 0.9938959853104786| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 93 | global_acc: 96.941% | global_loss: 0.08903643488883972 | global_f1: 0.9587073608617595 | global_precision: 0.9544235924932976 | global_recall: 0.9630297565374211 | global_auc: 0.9939225761173719| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 94 | global_acc: 96.875% | global_loss: 0.08892118185758591 | global_f1: 0.9578096947935368 | global_precision: 0.9535299374441466 | global_recall: 0.9621280432822362 | global_auc: 0.993907856206413| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 95 | global_acc: 96.875% | global_loss: 0.09133829921483994 | global_f1: 0.9578096947935368 | global_precision: 0.9535299374441466 | global_recall: 0.9621280432822362 | global_auc: 0.9935023464012904| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 96 | global_acc: 96.609% | global_loss: 0.09237606078386307 | global_f1: 0.9544235924932974 | global_precision: 0.9459698848538529 | global_recall: 0.9630297565374211 | global_auc: 0.9936903813928929| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 97 | global_acc: 96.875% | global_loss: 0.08892223984003067 | global_f1: 0.9578096947935368 | global_precision: 0.9535299374441466 | global_recall: 0.9621280432822362 | global_auc: 0.9938969349821534| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 98 | global_acc: 96.642% | global_loss: 0.09289652109146118 | global_f1: 0.9548502458649977 | global_precision: 0.9468085106382979 | global_recall: 0.9630297565374211 | global_auc: 0.993610608972213| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 99 | global_acc: 96.509% | global_loss: 0.09485547244548798 | global_f1: 0.953229398663697 | global_precision: 0.9419014084507042 | global_recall: 0.9648331830477908 | global_auc: 0.993751635215915| flobal_FPR: 0.0351668169522092 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-non-iid-FGSM  2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-non-iid-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-non-iid-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-non-iid-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d522b1",
   "metadata": {},
   "source": [
    "## iid fedAVG FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32db8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.098% | global_loss: 0.6723200678825378 | global_f1: 0.0 | global_precision: 0.0 | global_recall: 0.0 | global_auc: 0.6072350261705772| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6674184799194336 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.670632970416303| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6640610098838806 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7049638388768044| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6605637073516846 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7445190886380807| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6579413414001465 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7682255052372019| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6552866101264954 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7901346681918393| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.654147207736969 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8009521408211147| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6522844433784485 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8142181044458405| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6509563326835632 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8239850027849122| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.6498494744300842 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8326384110853275| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.6487898230552673 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8402082440048414| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.6483033895492554 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8449860422005602| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.6477658152580261 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8505240525719245| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.6455743908882141 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8578341502883915| flobal_FPR: 1.0 \n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.6452203989028931 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.861597224299629| flobal_FPR: 1.0 \n",
      "comm_round: 15 | global_acc: 63.132% | global_loss: 0.6436472535133362 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8674505256670139| flobal_FPR: 1.0 \n",
      "comm_round: 16 | global_acc: 63.132% | global_loss: 0.6406890749931335 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8743902514303243| flobal_FPR: 1.0 \n",
      "comm_round: 17 | global_acc: 63.132% | global_loss: 0.6404842734336853 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8779949676897955| flobal_FPR: 1.0 \n",
      "comm_round: 18 | global_acc: 63.132% | global_loss: 0.6387587785720825 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8833238128747938| flobal_FPR: 1.0 \n",
      "comm_round: 19 | global_acc: 63.132% | global_loss: 0.6341935396194458 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8889259260842045| flobal_FPR: 1.0 \n",
      "comm_round: 20 | global_acc: 63.132% | global_loss: 0.6331222653388977 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8929086116702303| flobal_FPR: 1.0 \n",
      "comm_round: 21 | global_acc: 63.132% | global_loss: 0.6286905407905579 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8990840891532774| flobal_FPR: 1.0 \n",
      "comm_round: 22 | global_acc: 63.132% | global_loss: 0.6247714161872864 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.903687385178759| flobal_FPR: 1.0 \n",
      "comm_round: 23 | global_acc: 63.132% | global_loss: 0.617411732673645 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9103343746483248| flobal_FPR: 1.0 \n",
      "comm_round: 24 | global_acc: 63.132% | global_loss: 0.6177417039871216 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9139476379528687| flobal_FPR: 1.0 \n",
      "comm_round: 25 | global_acc: 63.132% | global_loss: 0.6139364838600159 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9185611429488539| flobal_FPR: 1.0 \n",
      "comm_round: 26 | global_acc: 63.132% | global_loss: 0.6116952300071716 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9234773557911692| flobal_FPR: 1.0 \n",
      "comm_round: 27 | global_acc: 63.132% | global_loss: 0.6028212904930115 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9299289503136527| flobal_FPR: 1.0 \n",
      "comm_round: 28 | global_acc: 63.132% | global_loss: 0.5899582505226135 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9361222341406017| flobal_FPR: 1.0 \n",
      "comm_round: 29 | global_acc: 63.132% | global_loss: 0.5743610858917236 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9414719721024449| flobal_FPR: 1.0 \n",
      "comm_round: 30 | global_acc: 63.132% | global_loss: 0.5674996376037598 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9457827692521004| flobal_FPR: 1.0 \n",
      "comm_round: 31 | global_acc: 63.132% | global_loss: 0.555363655090332 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9508806068022133| flobal_FPR: 1.0 \n",
      "comm_round: 32 | global_acc: 63.132% | global_loss: 0.5544762015342712 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9530159435629117| flobal_FPR: 1.0 \n",
      "comm_round: 33 | global_acc: 63.132% | global_loss: 0.535701334476471 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.958275225297734| flobal_FPR: 1.0 \n",
      "comm_round: 34 | global_acc: 63.132% | global_loss: 0.5245283246040344 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9622192117630132| flobal_FPR: 1.0 \n",
      "comm_round: 35 | global_acc: 63.331% | global_loss: 0.516430139541626 | global_f1: 0.010762331838565023 | global_precision: 1.0 | global_recall: 0.005410279531109108 | global_auc: 0.9646399248619771| flobal_FPR: 0.9945897204688909 \n",
      "comm_round: 36 | global_acc: 69.614% | global_loss: 0.4973706305027008 | global_f1: 0.299079754601227 | global_precision: 1.0 | global_recall: 0.17583408476104598 | global_auc: 0.9681850492238571| flobal_FPR: 0.824165915238954 \n",
      "comm_round: 37 | global_acc: 79.255% | global_loss: 0.4709744155406952 | global_f1: 0.6090225563909775 | global_precision: 0.997946611909651 | global_recall: 0.4382326420198377 | global_auc: 0.9718189678873272| flobal_FPR: 0.5617673579801623 \n",
      "comm_round: 38 | global_acc: 82.214% | global_loss: 0.4674805998802185 | global_f1: 0.6828689982216953 | global_precision: 0.9965397923875432 | global_recall: 0.5193868349864743 | global_auc: 0.9731594294562512| flobal_FPR: 0.4806131650135257 \n",
      "comm_round: 39 | global_acc: 85.206% | global_loss: 0.4554491341114044 | global_f1: 0.7504206393718452 | global_precision: 0.9925816023738873 | global_recall: 0.6032461677186655 | global_auc: 0.9747453811531008| flobal_FPR: 0.39675383228133454 \n",
      "comm_round: 40 | global_acc: 89.195% | global_loss: 0.436135470867157 | global_f1: 0.8299319727891156 | global_precision: 0.9887780548628429 | global_recall: 0.715058611361587 | global_auc: 0.9765131949756671| flobal_FPR: 0.284941388638413 \n",
      "comm_round: 41 | global_acc: 90.160% | global_loss: 0.4227071702480316 | global_f1: 0.8474226804123712 | global_precision: 0.9891696750902527 | global_recall: 0.7412082957619477 | global_auc: 0.9778370372902828| flobal_FPR: 0.2587917042380523 \n",
      "comm_round: 42 | global_acc: 92.021% | global_loss: 0.4084501564502716 | global_f1: 0.8803589232303091 | global_precision: 0.9843924191750278 | global_recall: 0.7962128043282236 | global_auc: 0.9788598336839995| flobal_FPR: 0.2037871956717764 \n",
      "comm_round: 43 | global_acc: 92.819% | global_loss: 0.3734321594238281 | global_f1: 0.8946341463414633 | global_precision: 0.9744952178533475 | global_recall: 0.8268710550045085 | global_auc: 0.9814937480739471| flobal_FPR: 0.17312894499549145 \n",
      "comm_round: 44 | global_acc: 93.019% | global_loss: 0.3669774532318115 | global_f1: 0.8984526112185687 | global_precision: 0.9687174139728885 | global_recall: 0.8376916140667268 | global_auc: 0.9811342973450504| flobal_FPR: 0.1623083859332732 \n",
      "comm_round: 45 | global_acc: 93.983% | global_loss: 0.35934025049209595 | global_f1: 0.9144208037825059 | global_precision: 0.9612326043737575 | global_recall: 0.8719567177637512 | global_auc: 0.981107706538157| flobal_FPR: 0.12804328223624886 \n",
      "comm_round: 46 | global_acc: 93.916% | global_loss: 0.35071876645088196 | global_f1: 0.9137199434229137 | global_precision: 0.9575098814229249 | global_recall: 0.8737601442741209 | global_auc: 0.9811271748074897| flobal_FPR: 0.12623985572587917 \n",
      "comm_round: 47 | global_acc: 93.285% | global_loss: 0.35762274265289307 | global_f1: 0.9042654028436019 | global_precision: 0.9530469530469531 | global_recall: 0.8602344454463481 | global_auc: 0.9787563194714508| flobal_FPR: 0.13976555455365194 \n",
      "comm_round: 48 | global_acc: 93.983% | global_loss: 0.3282126486301422 | global_f1: 0.9153810191678354 | global_precision: 0.9504854368932039 | global_recall: 0.8827772768259693 | global_auc: 0.9815758946738139| flobal_FPR: 0.11722272317403065 \n",
      "comm_round: 49 | global_acc: 93.983% | global_loss: 0.2951369285583496 | global_f1: 0.9156963204471356 | global_precision: 0.9470134874759152 | global_recall: 0.8863841298467088 | global_auc: 0.983611041072825| flobal_FPR: 0.11361587015329125 \n",
      "comm_round: 50 | global_acc: 94.149% | global_loss: 0.2739275097846985 | global_f1: 0.9182915506035282 | global_precision: 0.9464114832535885 | global_recall: 0.8917944093778178 | global_auc: 0.9843812248010557| flobal_FPR: 0.10820559062218214 \n",
      "comm_round: 51 | global_acc: 94.249% | global_loss: 0.2547823190689087 | global_f1: 0.9197960129809921 | global_precision: 0.9465648854961832 | global_recall: 0.8944995491433724 | global_auc: 0.9849040190580112| flobal_FPR: 0.1055004508566276 \n",
      "comm_round: 52 | global_acc: 94.315% | global_loss: 0.23704271018505096 | global_f1: 0.9207232267037552 | global_precision: 0.9475190839694656 | global_recall: 0.8954012623985572 | global_auc: 0.9854401087184133| flobal_FPR: 0.10459873760144274 \n",
      "comm_round: 53 | global_acc: 94.348% | global_loss: 0.22753529250621796 | global_f1: 0.9215143120960295 | global_precision: 0.9441816461684012 | global_recall: 0.8999098286744815 | global_auc: 0.9854780955854037| flobal_FPR: 0.10009017132551848 \n",
      "comm_round: 54 | global_acc: 94.415% | global_loss: 0.21183738112449646 | global_f1: 0.922509225092251 | global_precision: 0.9442870632672332 | global_recall: 0.9017132551848512 | global_auc: 0.9861590101762068| flobal_FPR: 0.09828674481514878 \n",
      "comm_round: 55 | global_acc: 94.814% | global_loss: 0.2041693776845932 | global_f1: 0.9285714285714286 | global_precision: 0.9432558139534883 | global_recall: 0.9143372407574392 | global_auc: 0.9862710714338285| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 56 | global_acc: 94.515% | global_loss: 0.1908920705318451 | global_f1: 0.9239981575310917 | global_precision: 0.9444444444444444 | global_recall: 0.9044183949504058 | global_auc: 0.9868522704987819| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 57 | global_acc: 94.814% | global_loss: 0.1955408751964569 | global_f1: 0.9286367795059469 | global_precision: 0.9424326833797586 | global_recall: 0.915238954012624 | global_auc: 0.9857855517901073| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 58 | global_acc: 94.947% | global_loss: 0.17975357174873352 | global_f1: 0.9304029304029304 | global_precision: 0.9451162790697675 | global_recall: 0.9161406672678089 | global_auc: 0.987348473948844| flobal_FPR: 0.08385933273219116 \n",
      "comm_round: 59 | global_acc: 95.146% | global_loss: 0.17315758764743805 | global_f1: 0.9330889092575618 | global_precision: 0.9487418452935694 | global_recall: 0.9179440937781785 | global_auc: 0.9876362244662964| flobal_FPR: 0.08205590622182146 \n",
      "comm_round: 60 | global_acc: 95.312% | global_loss: 0.1623287945985794 | global_f1: 0.9354691075514875 | global_precision: 0.949814126394052 | global_recall: 0.9215509467989179 | global_auc: 0.9886120121121125| flobal_FPR: 0.07844905320108206 \n",
      "comm_round: 61 | global_acc: 95.379% | global_loss: 0.15392765402793884 | global_f1: 0.9365586490187129 | global_precision: 0.9482439926062847 | global_recall: 0.9251577998196574 | global_auc: 0.9893090711213867| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 62 | global_acc: 95.213% | global_loss: 0.1542685478925705 | global_f1: 0.9340054995417049 | global_precision: 0.9496738117427772 | global_recall: 0.9188458070333634 | global_auc: 0.9889681389901476| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 63 | global_acc: 95.678% | global_loss: 0.13895376026630402 | global_f1: 0.940530649588289 | global_precision: 0.9545032497678737 | global_recall: 0.9269612263300271 | global_auc: 0.9904049922340599| flobal_FPR: 0.07303877366997295 \n",
      "comm_round: 64 | global_acc: 95.578% | global_loss: 0.14102953672409058 | global_f1: 0.9390187987161852 | global_precision: 0.9552238805970149 | global_recall: 0.9233543733092876 | global_auc: 0.9898921695296894| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 65 | global_acc: 96.011% | global_loss: 0.12849777936935425 | global_f1: 0.9453551912568307 | global_precision: 0.9549218031278749 | global_recall: 0.9359783588818755 | global_auc: 0.9912378542928246| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 66 | global_acc: 96.044% | global_loss: 0.12261596322059631 | global_f1: 0.9458844929513415 | global_precision: 0.9541284403669725 | global_recall: 0.9377817853922452 | global_auc: 0.9917012940701077| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 67 | global_acc: 96.277% | global_loss: 0.11590878665447235 | global_f1: 0.9489516864175023 | global_precision: 0.9594470046082949 | global_recall: 0.9386834986474302 | global_auc: 0.9921832524450483| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 68 | global_acc: 96.277% | global_loss: 0.11624332517385483 | global_f1: 0.9489516864175023 | global_precision: 0.9594470046082949 | global_recall: 0.9386834986474302 | global_auc: 0.9920697666799145| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 69 | global_acc: 96.343% | global_loss: 0.11357709020376205 | global_f1: 0.9498632634457612 | global_precision: 0.96036866359447 | global_recall: 0.939585211902615 | global_auc: 0.9922321605362986| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 70 | global_acc: 96.210% | global_loss: 0.11246835440397263 | global_f1: 0.947992700729927 | global_precision: 0.9593721144967683 | global_recall: 0.9368800721370604 | global_auc: 0.9923603662123912| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 71 | global_acc: 96.376% | global_loss: 0.11128642410039902 | global_f1: 0.9502055733211512 | global_precision: 0.9629629629629629 | global_recall: 0.9377817853922452 | global_auc: 0.9922967382101824| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 72 | global_acc: 96.410% | global_loss: 0.10591835528612137 | global_f1: 0.9506849315068492 | global_precision: 0.9629972247918593 | global_recall: 0.9386834986474302 | global_auc: 0.9927464077481813| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 73 | global_acc: 96.476% | global_loss: 0.10522998869419098 | global_f1: 0.951686417502279 | global_precision: 0.9622119815668203 | global_recall: 0.9413886384129847 | global_auc: 0.9928404252439825| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 74 | global_acc: 96.609% | global_loss: 0.10017360746860504 | global_f1: 0.9535941765241128 | global_precision: 0.9623507805325987 | global_recall: 0.9449954914337241 | global_auc: 0.9933114623946636| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 75 | global_acc: 96.609% | global_loss: 0.09903624653816223 | global_f1: 0.953551912568306 | global_precision: 0.9632014719411224 | global_recall: 0.9440937781785392 | global_auc: 0.9933727162176856| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 76 | global_acc: 96.809% | global_loss: 0.09535111486911774 | global_f1: 0.9564032697547684 | global_precision: 0.9634034766697164 | global_recall: 0.9495040577096483 | global_auc: 0.9936694886160482| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 77 | global_acc: 96.842% | global_loss: 0.09267070144414902 | global_f1: 0.9569160997732425 | global_precision: 0.9625912408759124 | global_recall: 0.951307484220018 | global_auc: 0.9939854918658246| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 78 | global_acc: 96.742% | global_loss: 0.09388726949691772 | global_f1: 0.9554545454545454 | global_precision: 0.9633363886342805 | global_recall: 0.9477006311992786 | global_auc: 0.993802917486352| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 79 | global_acc: 96.842% | global_loss: 0.09225460141897202 | global_f1: 0.9568378009995456 | global_precision: 0.9642857142857143 | global_recall: 0.9495040577096483 | global_auc: 0.9939349218491437| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 80 | global_acc: 96.775% | global_loss: 0.09010043740272522 | global_f1: 0.9558086560364465 | global_precision: 0.9659300184162063 | global_recall: 0.9458972046889089 | global_auc: 0.994123906512421| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 81 | global_acc: 97.074% | global_loss: 0.0876692607998848 | global_f1: 0.9600725952813066 | global_precision: 0.9662100456621004 | global_recall: 0.9540126239855726 | global_auc: 0.9943171646982347| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 82 | global_acc: 97.041% | global_loss: 0.08713410794734955 | global_f1: 0.9596371882086169 | global_precision: 0.9653284671532847 | global_recall: 0.9540126239855726 | global_auc: 0.9944130815373855| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 83 | global_acc: 97.008% | global_loss: 0.08381403237581253 | global_f1: 0.9591280653950954 | global_precision: 0.9661482159194876 | global_recall: 0.9522091974752029 | global_auc: 0.9946419524110026| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 84 | global_acc: 96.875% | global_loss: 0.08588647842407227 | global_f1: 0.9571558796718322 | global_precision: 0.967741935483871 | global_recall: 0.9467989179440938 | global_auc: 0.9945066241973494| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 85 | global_acc: 96.975% | global_loss: 0.08605556935071945 | global_f1: 0.9585421412300684 | global_precision: 0.9686924493554327 | global_recall: 0.9486023444544635 | global_auc: 0.9945398627059661| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 86 | global_acc: 97.108% | global_loss: 0.0839664563536644 | global_f1: 0.9605083976395824 | global_precision: 0.9670932358318098 | global_recall: 0.9540126239855726 | global_auc: 0.9947544885044618| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 87 | global_acc: 97.074% | global_loss: 0.08218572288751602 | global_f1: 0.9601449275362319 | global_precision: 0.9645131938125568 | global_recall: 0.9558160504959423 | global_auc: 0.9949035869573991| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 88 | global_acc: 97.141% | global_loss: 0.081178680062294 | global_f1: 0.9610154125113328 | global_precision: 0.9662716499544212 | global_recall: 0.9558160504959423 | global_auc: 0.9949643659445838| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 89 | global_acc: 97.108% | global_loss: 0.08192597329616547 | global_f1: 0.9605083976395824 | global_precision: 0.9670932358318098 | global_recall: 0.9540126239855726 | global_auc: 0.9948879173747656| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 90 | global_acc: 97.074% | global_loss: 0.08214099705219269 | global_f1: 0.9599636032757052 | global_precision: 0.9687786960514233 | global_recall: 0.951307484220018 | global_auc: 0.994840908626865| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 91 | global_acc: 97.108% | global_loss: 0.08157233148813248 | global_f1: 0.9603283173734609 | global_precision: 0.9714022140221402 | global_recall: 0.9495040577096483 | global_auc: 0.9948689239412704| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 92 | global_acc: 97.141% | global_loss: 0.08131932467222214 | global_f1: 0.9607664233576643 | global_precision: 0.9722991689750693 | global_recall: 0.9495040577096483 | global_auc: 0.9948656000904087| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 93 | global_acc: 97.174% | global_loss: 0.08001348376274109 | global_f1: 0.9612403100775194 | global_precision: 0.9723247232472325 | global_recall: 0.9504057709648331 | global_auc: 0.9950417641860767| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 94 | global_acc: 97.241% | global_loss: 0.07860807329416275 | global_f1: 0.9621867881548974 | global_precision: 0.9723756906077348 | global_recall: 0.9522091974752029 | global_auc: 0.9951761427280553| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 95 | global_acc: 97.274% | global_loss: 0.07855938374996185 | global_f1: 0.9626593806921676 | global_precision: 0.9724011039558418 | global_recall: 0.9531109107303878 | global_auc: 0.9951614228170965| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 96 | global_acc: 97.241% | global_loss: 0.07818488776683807 | global_f1: 0.9622898682417084 | global_precision: 0.9697802197802198 | global_recall: 0.9549143372407575 | global_auc: 0.9951637969962834| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 97 | global_acc: 97.307% | global_loss: 0.07785424590110779 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.9952373965510773| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 98 | global_acc: 97.440% | global_loss: 0.07575464993715286 | global_f1: 0.9649840836743975 | global_precision: 0.9733944954128441 | global_recall: 0.9567177637511272 | global_auc: 0.9954273308860294| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 99 | global_acc: 97.507% | global_loss: 0.07562892884016037 | global_f1: 0.9659245797364834 | global_precision: 0.9734432234432234 | global_recall: 0.9585211902614968 | global_auc: 0.9954610442304834| flobal_FPR: 0.04147880973850315 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5238cac1",
   "metadata": {},
   "source": [
    "## iid fedAVG PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0127a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6759015321731567 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.5020311102943935| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6696034669876099 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.589835853999376| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6648380756378174 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6415454766900713| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6602829694747925 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6940402404378746| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6562026143074036 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7312236851914371| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6524076461791992 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7590754186508868| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6480634212493896 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7863343195673677| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6435959935188293 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8062930943199662| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6382352709770203 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8238416023620234| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.632305383682251 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8372718591864828| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.6252844333648682 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8501762353210438| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.6168356537818909 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8638503203479978| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.6065055131912231 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8774600651189867| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.5940881967544556 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8908594576140164| flobal_FPR: 1.0 \n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.5801997780799866 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.90483268921852| flobal_FPR: 1.0 \n",
      "comm_round: 15 | global_acc: 63.132% | global_loss: 0.5630567669868469 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9200276734326025| flobal_FPR: 1.0 \n",
      "comm_round: 16 | global_acc: 63.132% | global_loss: 0.5405433177947998 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9349479651147607| flobal_FPR: 1.0 \n",
      "comm_round: 17 | global_acc: 63.132% | global_loss: 0.5149455666542053 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9471170579551386| flobal_FPR: 1.0 \n",
      "comm_round: 18 | global_acc: 63.098% | global_loss: 0.49196165800094604 | global_f1: 0.0 | global_precision: 0.0 | global_recall: 0.0 | global_auc: 0.958847402481777| flobal_FPR: 1.0 \n",
      "comm_round: 19 | global_acc: 64.096% | global_loss: 0.4686942398548126 | global_f1: 0.05263157894736842 | global_precision: 0.967741935483871 | global_recall: 0.027051397655545536 | global_auc: 0.9680378501142693| flobal_FPR: 0.9729486023444545 \n",
      "comm_round: 20 | global_acc: 76.596% | global_loss: 0.44644004106521606 | global_f1: 0.5362318840579711 | global_precision: 0.9951100244498777 | global_recall: 0.36699729486023447 | global_auc: 0.9745910595059523| flobal_FPR: 0.6330027051397655 \n",
      "comm_round: 21 | global_acc: 89.860% | global_loss: 0.423091322183609 | global_f1: 0.8422141748577341 | global_precision: 0.9878640776699029 | global_recall: 0.7339945897204689 | global_auc: 0.9784464890875603| flobal_FPR: 0.2660054102795311 \n",
      "comm_round: 22 | global_acc: 93.418% | global_loss: 0.39946940541267395 | global_f1: 0.9040697674418605 | global_precision: 0.9769633507853404 | global_recall: 0.8412984670874661 | global_auc: 0.9809044767997585| flobal_FPR: 0.1587015329125338 \n",
      "comm_round: 23 | global_acc: 94.315% | global_loss: 0.37384048104286194 | global_f1: 0.9189957366177168 | global_precision: 0.9680638722554891 | global_recall: 0.8746618575293057 | global_auc: 0.982399260015831| flobal_FPR: 0.12533814247069433 \n",
      "comm_round: 24 | global_acc: 94.614% | global_loss: 0.3493725657463074 | global_f1: 0.9241573033707865 | global_precision: 0.9610516066212269 | global_recall: 0.8899909828674482 | global_auc: 0.9838327894088816| flobal_FPR: 0.11000901713255185 \n",
      "comm_round: 25 | global_acc: 94.681% | global_loss: 0.32586583495140076 | global_f1: 0.9259259259259259 | global_precision: 0.9514747859181731 | global_recall: 0.9017132551848512 | global_auc: 0.984401642742063| flobal_FPR: 0.09828674481514878 \n",
      "comm_round: 26 | global_acc: 94.681% | global_loss: 0.2975366413593292 | global_f1: 0.9261311172668514 | global_precision: 0.9489120151371807 | global_recall: 0.9044183949504058 | global_auc: 0.9850004107329993| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 27 | global_acc: 94.714% | global_loss: 0.27444136142730713 | global_f1: 0.9268965517241379 | global_precision: 0.9455909943714822 | global_recall: 0.90892696122633 | global_auc: 0.9852553975776726| flobal_FPR: 0.09107303877366997 \n",
      "comm_round: 28 | global_acc: 94.781% | global_loss: 0.2490605264902115 | global_f1: 0.9278160919540229 | global_precision: 0.9465290806754222 | global_recall: 0.9098286744815148 | global_auc: 0.9857378307884508| flobal_FPR: 0.09017132551848513 \n",
      "comm_round: 29 | global_acc: 94.880% | global_loss: 0.22834469377994537 | global_f1: 0.9293577981651376 | global_precision: 0.945845004668534 | global_recall: 0.9134355275022543 | global_auc: 0.986075439068828| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 30 | global_acc: 94.880% | global_loss: 0.208571657538414 | global_f1: 0.9294225481209899 | global_precision: 0.9450139794967382 | global_recall: 0.9143372407574392 | global_auc: 0.9865521742495575| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 31 | global_acc: 94.880% | global_loss: 0.19033695757389069 | global_f1: 0.9294871794871795 | global_precision: 0.9441860465116279 | global_recall: 0.915238954012624 | global_auc: 0.9870754433423504| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 32 | global_acc: 94.980% | global_loss: 0.17665600776672363 | global_f1: 0.9308924485125859 | global_precision: 0.9451672862453532 | global_recall: 0.9170423805229937 | global_auc: 0.9873603448447785| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 33 | global_acc: 95.047% | global_loss: 0.16440726816654205 | global_f1: 0.9319323892188215 | global_precision: 0.9444444444444444 | global_recall: 0.9197475202885482 | global_auc: 0.9877701281724377| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 34 | global_acc: 95.146% | global_loss: 0.15342330932617188 | global_f1: 0.93327239488117 | global_precision: 0.9462465245597775 | global_recall: 0.9206492335437331 | global_auc: 0.9882933972652305| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 35 | global_acc: 95.412% | global_loss: 0.14311836659908295 | global_f1: 0.936986301369863 | global_precision: 0.9491211840888066 | global_recall: 0.9251577998196574 | global_auc: 0.9890778260685825| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 36 | global_acc: 95.412% | global_loss: 0.13610520958900452 | global_f1: 0.936986301369863 | global_precision: 0.9491211840888066 | global_recall: 0.9251577998196574 | global_auc: 0.989471939813608| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 37 | global_acc: 95.445% | global_loss: 0.13090689480304718 | global_f1: 0.9372423270728355 | global_precision: 0.952513966480447 | global_recall: 0.9224526600541028 | global_auc: 0.9896348085058293| flobal_FPR: 0.0775473399458972 \n",
      "comm_round: 38 | global_acc: 95.811% | global_loss: 0.12414665520191193 | global_f1: 0.9425182481751824 | global_precision: 0.9538319482917821 | global_recall: 0.9314697926059513 | global_auc: 0.9903831497855404| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 39 | global_acc: 96.210% | global_loss: 0.11886772513389587 | global_f1: 0.9481818181818181 | global_precision: 0.9560036663611365 | global_recall: 0.9404869251577999 | global_auc: 0.991143361961186| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 40 | global_acc: 96.343% | global_loss: 0.11393732577562332 | global_f1: 0.9499089253187614 | global_precision: 0.9595216191352346 | global_recall: 0.9404869251577999 | global_auc: 0.9917274100411634| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 41 | global_acc: 96.443% | global_loss: 0.11088809370994568 | global_f1: 0.9512528473804102 | global_precision: 0.9613259668508287 | global_recall: 0.9413886384129847 | global_auc: 0.9919306397795623| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 42 | global_acc: 96.443% | global_loss: 0.10793071240186691 | global_f1: 0.9512528473804102 | global_precision: 0.9613259668508287 | global_recall: 0.9413886384129847 | global_auc: 0.992127696652075| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 43 | global_acc: 96.443% | global_loss: 0.10596280544996262 | global_f1: 0.9512083903328773 | global_precision: 0.9621771217712177 | global_recall: 0.9404869251577999 | global_auc: 0.9922316857004612| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 44 | global_acc: 96.543% | global_loss: 0.10398851335048676 | global_f1: 0.9525114155251141 | global_precision: 0.9648473635522664 | global_recall: 0.9404869251577999 | global_auc: 0.9923684384216266| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 45 | global_acc: 96.543% | global_loss: 0.10422563552856445 | global_f1: 0.9523809523809523 | global_precision: 0.9674418604651163 | global_recall: 0.9377817853922452 | global_auc: 0.9922748957616628| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 46 | global_acc: 96.709% | global_loss: 0.0993846207857132 | global_f1: 0.9548974943052391 | global_precision: 0.9650092081031307 | global_recall: 0.9449954914337241 | global_auc: 0.9928608431849898| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 47 | global_acc: 96.742% | global_loss: 0.09840977191925049 | global_f1: 0.9553327256153146 | global_precision: 0.9658986175115207 | global_recall: 0.9449954914337241 | global_auc: 0.9929031035745166| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 48 | global_acc: 97.008% | global_loss: 0.0963384360074997 | global_f1: 0.9590909090909091 | global_precision: 0.9670027497708524 | global_recall: 0.951307484220018 | global_auc: 0.9931685368076122| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 49 | global_acc: 96.842% | global_loss: 0.0969100296497345 | global_f1: 0.9567592171142466 | global_precision: 0.9659926470588235 | global_recall: 0.9477006311992786 | global_auc: 0.9930707206251119| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 50 | global_acc: 96.908% | global_loss: 0.09581734985113144 | global_f1: 0.9576695493855257 | global_precision: 0.9669117647058824 | global_recall: 0.9486023444544635 | global_auc: 0.9932065236746026| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 51 | global_acc: 97.041% | global_loss: 0.09475775063037872 | global_f1: 0.959563834620627 | global_precision: 0.967032967032967 | global_recall: 0.9522091974752029 | global_auc: 0.9933038650212654| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 52 | global_acc: 97.041% | global_loss: 0.09349025785923004 | global_f1: 0.9595270577535243 | global_precision: 0.9678899082568807 | global_recall: 0.951307484220018 | global_auc: 0.9934477402799917| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 53 | global_acc: 97.041% | global_loss: 0.09147155284881592 | global_f1: 0.959563834620627 | global_precision: 0.967032967032967 | global_recall: 0.9522091974752029 | global_auc: 0.993654293869252| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 54 | global_acc: 97.108% | global_loss: 0.09206928312778473 | global_f1: 0.9605083976395824 | global_precision: 0.9670932358318098 | global_recall: 0.9540126239855726 | global_auc: 0.9936272282265214| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 55 | global_acc: 97.108% | global_loss: 0.09177999198436737 | global_f1: 0.9605083976395824 | global_precision: 0.9670932358318098 | global_recall: 0.9540126239855726 | global_auc: 0.993680409840308| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 56 | global_acc: 97.041% | global_loss: 0.08993962407112122 | global_f1: 0.9596005447117567 | global_precision: 0.9661791590493601 | global_recall: 0.9531109107303878 | global_auc: 0.9939256625503148| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 57 | global_acc: 97.174% | global_loss: 0.08805885910987854 | global_f1: 0.9614512471655328 | global_precision: 0.9671532846715328 | global_recall: 0.9558160504959423 | global_auc: 0.9941485979759649| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 58 | global_acc: 97.207% | global_loss: 0.08740957081317902 | global_f1: 0.9618874773139746 | global_precision: 0.9680365296803652 | global_recall: 0.9558160504959423 | global_auc: 0.9942316942475062| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 59 | global_acc: 97.207% | global_loss: 0.08643563091754913 | global_f1: 0.9618874773139746 | global_precision: 0.9680365296803652 | global_recall: 0.9558160504959423 | global_auc: 0.9943233375641207| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 60 | global_acc: 97.307% | global_loss: 0.08527527749538422 | global_f1: 0.9631985461154021 | global_precision: 0.9706959706959707 | global_recall: 0.9558160504959423 | global_auc: 0.9944515432402135| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 61 | global_acc: 97.340% | global_loss: 0.08552613854408264 | global_f1: 0.9637023593466425 | global_precision: 0.9698630136986301 | global_recall: 0.957619477006312 | global_auc: 0.9944244775974826| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 62 | global_acc: 97.407% | global_loss: 0.08547981828451157 | global_f1: 0.9646418857660924 | global_precision: 0.9699179580674567 | global_recall: 0.9594229035166817 | global_auc: 0.9944365859113358| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 63 | global_acc: 97.440% | global_loss: 0.08482921123504639 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9945360640192669| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 64 | global_acc: 97.407% | global_loss: 0.08486883342266083 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9945825979313302| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 65 | global_acc: 97.440% | global_loss: 0.08427266776561737 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9946528736352624| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 66 | global_acc: 97.473% | global_loss: 0.0840817242860794 | global_f1: 0.9654859218891917 | global_precision: 0.9725526075022873 | global_recall: 0.9585211902614968 | global_auc: 0.9946884863230658| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 67 | global_acc: 97.440% | global_loss: 0.08291503041982651 | global_f1: 0.9650159018627896 | global_precision: 0.9725274725274725 | global_recall: 0.957619477006312 | global_auc: 0.9948883922106029| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 68 | global_acc: 97.440% | global_loss: 0.08268991112709045 | global_f1: 0.9650159018627896 | global_precision: 0.9725274725274725 | global_recall: 0.957619477006312 | global_auc: 0.9949092849874478| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 69 | global_acc: 97.440% | global_loss: 0.0831744596362114 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9948627510753845| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 70 | global_acc: 97.407% | global_loss: 0.08338886499404907 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9948461318210761| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 71 | global_acc: 97.374% | global_loss: 0.08327288180589676 | global_f1: 0.9641398093508853 | global_precision: 0.9707495429616088 | global_recall: 0.957619477006312 | global_auc: 0.9948874425389281| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 72 | global_acc: 97.407% | global_loss: 0.08334045857191086 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9948798451655302| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 73 | global_acc: 97.407% | global_loss: 0.08266931027173996 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9950180223942079| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 74 | global_acc: 97.374% | global_loss: 0.08237717300653458 | global_f1: 0.9641398093508853 | global_precision: 0.9707495429616088 | global_recall: 0.957619477006312 | global_auc: 0.9950593331120599| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 75 | global_acc: 97.407% | global_loss: 0.08208104223012924 | global_f1: 0.9646418857660924 | global_precision: 0.9699179580674567 | global_recall: 0.9594229035166817 | global_auc: 0.9951096657108223| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 76 | global_acc: 97.473% | global_loss: 0.08221279084682465 | global_f1: 0.9655797101449276 | global_precision: 0.9699727024567789 | global_recall: 0.9612263300270514 | global_auc: 0.995102068337424| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 77 | global_acc: 97.440% | global_loss: 0.08257219940423965 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9950783265455551| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 78 | global_acc: 97.473% | global_loss: 0.08125873655080795 | global_f1: 0.9655797101449276 | global_precision: 0.9699727024567789 | global_recall: 0.9612263300270514 | global_auc: 0.9952297991776793| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 79 | global_acc: 97.473% | global_loss: 0.08160388469696045 | global_f1: 0.9655797101449276 | global_precision: 0.9699727024567789 | global_recall: 0.9612263300270514 | global_auc: 0.9952079567291598| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 80 | global_acc: 97.507% | global_loss: 0.08116094768047333 | global_f1: 0.9660479855138072 | global_precision: 0.97 | global_recall: 0.9621280432822362 | global_auc: 0.9952673112088324| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 81 | global_acc: 97.540% | global_loss: 0.08149499446153641 | global_f1: 0.9665158371040724 | global_precision: 0.9700272479564033 | global_recall: 0.9630297565374211 | global_auc: 0.9952563899845726| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 82 | global_acc: 97.540% | global_loss: 0.08304658532142639 | global_f1: 0.9664855072463768 | global_precision: 0.9708826205641492 | global_recall: 0.9621280432822362 | global_auc: 0.9951229611142688| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 83 | global_acc: 97.573% | global_loss: 0.0819617509841919 | global_f1: 0.9669533725667723 | global_precision: 0.9709090909090909 | global_recall: 0.9630297565374211 | global_auc: 0.995245468760313| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 84 | global_acc: 97.573% | global_loss: 0.0817403644323349 | global_f1: 0.9669533725667723 | global_precision: 0.9709090909090909 | global_recall: 0.9630297565374211 | global_auc: 0.9952806066122789| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 85 | global_acc: 97.573% | global_loss: 0.08077902346849442 | global_f1: 0.9669234254644313 | global_precision: 0.9717668488160291 | global_recall: 0.9621280432822362 | global_auc: 0.9953815092277223| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 86 | global_acc: 97.640% | global_loss: 0.08089164644479752 | global_f1: 0.9678296329859537 | global_precision: 0.9726775956284153 | global_recall: 0.9630297565374211 | global_auc: 0.9953632280479832| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 87 | global_acc: 97.606% | global_loss: 0.08091941475868225 | global_f1: 0.967391304347826 | global_precision: 0.9717925386715196 | global_recall: 0.9630297565374211 | global_auc: 0.9953964665565997| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 88 | global_acc: 97.606% | global_loss: 0.0808509886264801 | global_f1: 0.967391304347826 | global_precision: 0.9717925386715196 | global_recall: 0.9630297565374211 | global_auc: 0.995413085810908| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 89 | global_acc: 97.606% | global_loss: 0.08147113025188446 | global_f1: 0.967391304347826 | global_precision: 0.9717925386715196 | global_recall: 0.9630297565374211 | global_auc: 0.9953848330785839| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 90 | global_acc: 97.673% | global_loss: 0.08005405217409134 | global_f1: 0.9682683590208523 | global_precision: 0.9735642661804923 | global_recall: 0.9630297565374211 | global_auc: 0.9954638932455077| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 91 | global_acc: 97.706% | global_loss: 0.08005115389823914 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.9954543965287602| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 92 | global_acc: 97.706% | global_loss: 0.08046772330999374 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.995433028916078| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 93 | global_acc: 97.706% | global_loss: 0.08036453276872635 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.9954826492610841| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 94 | global_acc: 97.706% | global_loss: 0.08061524480581284 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.9954240070351676| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 95 | global_acc: 97.673% | global_loss: 0.07975433766841888 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9955496011141548| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 96 | global_acc: 97.673% | global_loss: 0.08006465435028076 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.995516600023457| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 97 | global_acc: 97.706% | global_loss: 0.07977399230003357 | global_f1: 0.9687358405074762 | global_precision: 0.9735883424408015 | global_recall: 0.9639314697926059 | global_auc: 0.9955752422493733| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 98 | global_acc: 97.739% | global_loss: 0.0799768716096878 | global_f1: 0.9692028985507246 | global_precision: 0.9736123748862603 | global_recall: 0.9648331830477908 | global_auc: 0.9955393921436511| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 99 | global_acc: 97.773% | global_loss: 0.07941356301307678 | global_f1: 0.9696695337256677 | global_precision: 0.9736363636363636 | global_recall: 0.9657348963029756 | global_auc: 0.9956177400568188| flobal_FPR: 0.034265103697024346 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, pgd_attack]\n",
    "    \n",
    "                    # Randomly choose between normal training and PGD attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "    \n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.01\n",
    "                        alpha = 0.01\n",
    "                        num_iter = 5\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "    \n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-PGD-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-PGD-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-PGD-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb1f186",
   "metadata": {},
   "source": [
    "## non-iid fedAVG PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "153b54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 812, (1,): 391}\n",
      "Client client_2: {(0,): 940, (1,): 263}\n",
      "Client client_3: {(0,): 693, (1,): 510}\n",
      "Client client_4: {(0,): 112, (1,): 1091}\n",
      "Client client_5: {(0,): 444, (1,): 759}\n",
      "Client client_6: {(0,): 639, (1,): 564}\n",
      "Client client_7: {(0,): 673, (1,): 530}\n",
      "Client client_8: {(0,): 683, (1,): 520}\n",
      "Client client_9: {(0,): 858, (1,): 345}\n",
      "Client client_10: {(0,): 439, (1,): 764}\n",
      "|=======================|\n",
      "|Traditional FedAvg non iid pgd 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 39.129% | global_loss: 0.7014480829238892 | global_f1: 0.5475660983444527 | global_precision: 0.37712729748127977 | global_recall: 0.9990982867448152 | global_auc: 0.6259459323425409| flobal_FPR: 0.0009017132551848512 \n",
      "comm_round: 1 | global_acc: 43.251% | global_loss: 0.6941555142402649 | global_f1: 0.555584483207498 | global_precision: 0.3905563689604685 | global_recall: 0.9621280432822362 | global_auc: 0.7145227591191036| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 2 | global_acc: 50.731% | global_loss: 0.6891863942146301 | global_f1: 0.5820642978003384 | global_precision: 0.42347148132950346 | global_recall: 0.9305680793507665 | global_auc: 0.7770982876944867| flobal_FPR: 0.06943192064923355 \n",
      "comm_round: 3 | global_acc: 66.789% | global_loss: 0.6829493045806885 | global_f1: 0.6577595066803699 | global_precision: 0.5303867403314917 | global_recall: 0.8656447249774571 | global_auc: 0.8237613551055062| flobal_FPR: 0.13435527502254282 \n",
      "comm_round: 4 | global_acc: 79.787% | global_loss: 0.67670738697052 | global_f1: 0.7121212121212122 | global_precision: 0.7497507477567298 | global_recall: 0.6780883678990082 | global_auc: 0.8518065841686883| flobal_FPR: 0.3219116321009919 \n",
      "comm_round: 5 | global_acc: 81.682% | global_loss: 0.671384334564209 | global_f1: 0.6957482054113748 | global_precision: 0.8974358974358975 | global_recall: 0.5680793507664562 | global_auc: 0.8728026378080439| flobal_FPR: 0.43192064923354373 \n",
      "comm_round: 6 | global_acc: 79.920% | global_loss: 0.6663315892219543 | global_f1: 0.633495145631068 | global_precision: 0.9684601113172542 | global_recall: 0.47069431920649235 | global_auc: 0.887858495121774| flobal_FPR: 0.5293056807935077 \n",
      "comm_round: 7 | global_acc: 79.488% | global_loss: 0.6591631174087524 | global_f1: 0.6198398028342575 | global_precision: 0.9785992217898832 | global_recall: 0.45356176735798015 | global_auc: 0.9037799781670481| flobal_FPR: 0.5464382326420198 \n",
      "comm_round: 8 | global_acc: 79.422% | global_loss: 0.6512763500213623 | global_f1: 0.6162430254184749 | global_precision: 0.9861111111111112 | global_recall: 0.44815148782687103 | global_auc: 0.9148871006571254| flobal_FPR: 0.5518485121731289 \n",
      "comm_round: 9 | global_acc: 80.585% | global_loss: 0.6439508199691772 | global_f1: 0.6469165659008463 | global_precision: 0.981651376146789 | global_recall: 0.4824165915238954 | global_auc: 0.922222602090892| flobal_FPR: 0.5175834084761046 \n",
      "comm_round: 10 | global_acc: 82.912% | global_loss: 0.6343082785606384 | global_f1: 0.7032332563510392 | global_precision: 0.9775280898876404 | global_recall: 0.5491433724075744 | global_auc: 0.9283458476318276| flobal_FPR: 0.4508566275924256 \n",
      "comm_round: 11 | global_acc: 83.677% | global_loss: 0.6218520402908325 | global_f1: 0.7199087278950371 | global_precision: 0.9798136645962733 | global_recall: 0.5689810640216412 | global_auc: 0.9336670954434277| flobal_FPR: 0.4310189359783589 \n",
      "comm_round: 12 | global_acc: 85.771% | global_loss: 0.6039695739746094 | global_f1: 0.7645764576457645 | global_precision: 0.9802538787023978 | global_recall: 0.6266907123534716 | global_auc: 0.9392233869945313| flobal_FPR: 0.37330928764652843 \n",
      "comm_round: 13 | global_acc: 87.334% | global_loss: 0.5836483836174011 | global_f1: 0.7980922098569159 | global_precision: 0.967866323907455 | global_recall: 0.678990081154193 | global_auc: 0.9439501403377317| flobal_FPR: 0.32100991884580704 \n",
      "comm_round: 14 | global_acc: 89.628% | global_loss: 0.5599575638771057 | global_f1: 0.8422649140546006 | global_precision: 0.9585730724971231 | global_recall: 0.7511271415689811 | global_auc: 0.9500216287723926| flobal_FPR: 0.24887285843101895 \n",
      "comm_round: 15 | global_acc: 90.392% | global_loss: 0.5287567973136902 | global_f1: 0.8611244593945219 | global_precision: 0.9218106995884774 | global_recall: 0.8079350766456267 | global_auc: 0.9554019936457468| flobal_FPR: 0.1920649233543733 \n",
      "comm_round: 16 | global_acc: 90.525% | global_loss: 0.4979051351547241 | global_f1: 0.8698035632709 | global_precision: 0.8814814814814815 | global_recall: 0.8584310189359784 | global_auc: 0.9613464634939085| flobal_FPR: 0.14156898106402163 \n",
      "comm_round: 17 | global_acc: 90.559% | global_loss: 0.4741285741329193 | global_f1: 0.8755477651183173 | global_precision: 0.8516624040920716 | global_recall: 0.9008115419296664 | global_auc: 0.9689238937868204| flobal_FPR: 0.09918845807033363 \n",
      "comm_round: 18 | global_acc: 91.356% | global_loss: 0.44422072172164917 | global_f1: 0.8864628820960698 | global_precision: 0.859441151566469 | global_recall: 0.915238954012624 | global_auc: 0.974550698459775| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 19 | global_acc: 91.922% | global_loss: 0.41264447569847107 | global_f1: 0.8944854537559704 | global_precision: 0.8626465661641541 | global_recall: 0.9287646528403968 | global_auc: 0.9786062713468386| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 20 | global_acc: 92.553% | global_loss: 0.3788720667362213 | global_f1: 0.9028620988725066 | global_precision: 0.8696741854636592 | global_recall: 0.9386834986474302 | global_auc: 0.9816058093315688| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 21 | global_acc: 93.484% | global_loss: 0.34688737988471985 | global_f1: 0.9150779896013865 | global_precision: 0.8807339449541285 | global_recall: 0.9522091974752029 | global_auc: 0.9837549163315512| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 22 | global_acc: 93.949% | global_loss: 0.31567370891571045 | global_f1: 0.9207317073170732 | global_precision: 0.8904802021903959 | global_recall: 0.9531109107303878 | global_auc: 0.9850640387352083| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 23 | global_acc: 93.384% | global_loss: 0.29667991399765015 | global_f1: 0.9144086021505375 | global_precision: 0.8741776315789473 | global_recall: 0.9585211902614968 | global_auc: 0.9858252005825285| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 24 | global_acc: 92.985% | global_loss: 0.27626267075538635 | global_f1: 0.9099445155783183 | global_precision: 0.8638573743922204 | global_recall: 0.9612263300270514 | global_auc: 0.9861105769207941| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 25 | global_acc: 94.082% | global_loss: 0.24666619300842285 | global_f1: 0.9223385689354276 | global_precision: 0.893491124260355 | global_recall: 0.9531109107303878 | global_auc: 0.9863964280948968| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 26 | global_acc: 94.315% | global_loss: 0.22541595995426178 | global_f1: 0.924901185770751 | global_precision: 0.901541095890411 | global_recall: 0.9495040577096483 | global_auc: 0.9864776250230889| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 27 | global_acc: 94.548% | global_loss: 0.2052338719367981 | global_f1: 0.9278169014084507 | global_precision: 0.9062768701633706 | global_recall: 0.9504057709648331 | global_auc: 0.9869728788014764| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 28 | global_acc: 94.182% | global_loss: 0.1904408484697342 | global_f1: 0.922463447053611 | global_precision: 0.9067944250871081 | global_recall: 0.9386834986474302 | global_auc: 0.9872283404819868| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 29 | global_acc: 94.382% | global_loss: 0.1775941401720047 | global_f1: 0.9249222567747669 | global_precision: 0.9115586690017513 | global_recall: 0.9386834986474302 | global_auc: 0.9875194148503008| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 30 | global_acc: 94.448% | global_loss: 0.1658790111541748 | global_f1: 0.9254796965640338 | global_precision: 0.916077738515901 | global_recall: 0.9350766456266907 | global_auc: 0.9878185614278504| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 31 | global_acc: 94.947% | global_loss: 0.155532568693161 | global_f1: 0.9325044404973356 | global_precision: 0.9186351706036745 | global_recall: 0.9467989179440938 | global_auc: 0.9885664278717241| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 32 | global_acc: 95.246% | global_loss: 0.14683644473552704 | global_f1: 0.936472678809418 | global_precision: 0.9229422066549913 | global_recall: 0.9504057709648331 | global_auc: 0.9894524715442753| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 33 | global_acc: 95.346% | global_loss: 0.14122773706912994 | global_f1: 0.9377224199288255 | global_precision: 0.9253731343283582 | global_recall: 0.9504057709648331 | global_auc: 0.9898741257678689| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 34 | global_acc: 95.412% | global_loss: 0.1371474415063858 | global_f1: 0.9385026737967914 | global_precision: 0.9277533039647577 | global_recall: 0.9495040577096483 | global_auc: 0.9898575065135606| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 35 | global_acc: 95.445% | global_loss: 0.1322939246892929 | global_f1: 0.9388665774207943 | global_precision: 0.9293286219081273 | global_recall: 0.9486023444544635 | global_auc: 0.9901329112992411| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 36 | global_acc: 95.545% | global_loss: 0.12675458192825317 | global_f1: 0.9403383793410508 | global_precision: 0.9287598944591029 | global_recall: 0.9522091974752029 | global_auc: 0.9911030009150087| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 37 | global_acc: 95.678% | global_loss: 0.12270248681306839 | global_f1: 0.9420160570918823 | global_precision: 0.9320388349514563 | global_recall: 0.9522091974752029 | global_auc: 0.9916922721891973| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 38 | global_acc: 95.545% | global_loss: 0.1211467906832695 | global_f1: 0.9402852049910873 | global_precision: 0.9295154185022027 | global_recall: 0.951307484220018 | global_auc: 0.9915407995570732| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 39 | global_acc: 95.745% | global_loss: 0.11884762346744537 | global_f1: 0.9427549194991055 | global_precision: 0.935226264418811 | global_recall: 0.9504057709648331 | global_auc: 0.9915370008703742| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 40 | global_acc: 95.678% | global_loss: 0.11584655940532684 | global_f1: 0.9420160570918823 | global_precision: 0.9320388349514563 | global_recall: 0.9522091974752029 | global_auc: 0.9919823968858366| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 41 | global_acc: 95.911% | global_loss: 0.11365687847137451 | global_f1: 0.9450647610540419 | global_precision: 0.9362831858407079 | global_recall: 0.9540126239855726 | global_auc: 0.9922131671028034| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 42 | global_acc: 96.077% | global_loss: 0.11124997586011887 | global_f1: 0.9472743521000895 | global_precision: 0.9388839681133747 | global_recall: 0.9558160504959423 | global_auc: 0.9926728081933874| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 43 | global_acc: 96.144% | global_loss: 0.1104060560464859 | global_f1: 0.9481216457960645 | global_precision: 0.940550133096717 | global_recall: 0.9558160504959423 | global_auc: 0.9926110795345279| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 44 | global_acc: 96.310% | global_loss: 0.10810975730419159 | global_f1: 0.9502465262214255 | global_precision: 0.9447415329768271 | global_recall: 0.9558160504959423 | global_auc: 0.9926481167298437| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 45 | global_acc: 96.277% | global_loss: 0.10693342983722687 | global_f1: 0.9498207885304659 | global_precision: 0.943900267141585 | global_recall: 0.9558160504959423 | global_auc: 0.9928242808255116| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 46 | global_acc: 96.376% | global_loss: 0.10537968575954437 | global_f1: 0.9513175524787851 | global_precision: 0.9424778761061947 | global_recall: 0.9603246167718665 | global_auc: 0.9931281757614349| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 47 | global_acc: 96.376% | global_loss: 0.10605315119028091 | global_f1: 0.9512304250559284 | global_precision: 0.9440497335701599 | global_recall: 0.9585211902614968 | global_auc: 0.9929515368299294| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 48 | global_acc: 96.410% | global_loss: 0.10521218925714493 | global_f1: 0.9516129032258065 | global_precision: 0.9456812110418522 | global_recall: 0.957619477006312 | global_auc: 0.9930175390113255| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 49 | global_acc: 96.543% | global_loss: 0.1042129322886467 | global_f1: 0.9534050179211471 | global_precision: 0.9474621549421193 | global_recall: 0.9594229035166817 | global_auc: 0.99312722608976| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 50 | global_acc: 96.543% | global_loss: 0.1030244305729866 | global_f1: 0.9534050179211471 | global_precision: 0.9474621549421193 | global_recall: 0.9594229035166817 | global_auc: 0.9932554317658527| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 51 | global_acc: 96.543% | global_loss: 0.10329022258520126 | global_f1: 0.9534050179211471 | global_precision: 0.9474621549421193 | global_recall: 0.9594229035166817 | global_auc: 0.993276799378535| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 52 | global_acc: 96.443% | global_loss: 0.10349344462156296 | global_f1: 0.9519533004041311 | global_precision: 0.9481216457960644 | global_recall: 0.9558160504959423 | global_auc: 0.9931072829845902| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 53 | global_acc: 96.443% | global_loss: 0.10263589024543762 | global_f1: 0.9519533004041311 | global_precision: 0.9481216457960644 | global_recall: 0.9558160504959423 | global_auc: 0.9932036746595783| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 54 | global_acc: 96.410% | global_loss: 0.10248839855194092 | global_f1: 0.9515260323159784 | global_precision: 0.9472743521000894 | global_recall: 0.9558160504959423 | global_auc: 0.99313814731402| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 55 | global_acc: 96.543% | global_loss: 0.10149052739143372 | global_f1: 0.9532794249775383 | global_precision: 0.9498657117278424 | global_recall: 0.9567177637511272 | global_auc: 0.9933741407251978| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 56 | global_acc: 96.676% | global_loss: 0.10133960098028183 | global_f1: 0.9550359712230215 | global_precision: 0.9524663677130045 | global_recall: 0.957619477006312 | global_auc: 0.993559326701776| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 57 | global_acc: 96.609% | global_loss: 0.10222490131855011 | global_f1: 0.9541778975741241 | global_precision: 0.9507609668755596 | global_recall: 0.957619477006312 | global_auc: 0.9936599918993005| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 58 | global_acc: 96.509% | global_loss: 0.10250237584114075 | global_f1: 0.9528936742934051 | global_precision: 0.9482142857142857 | global_recall: 0.957619477006312 | global_auc: 0.9935298868798583| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 59 | global_acc: 96.476% | global_loss: 0.10368815809488297 | global_f1: 0.952423698384201 | global_precision: 0.9481680071492404 | global_recall: 0.9567177637511272 | global_auc: 0.9933575214708894| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 60 | global_acc: 96.543% | global_loss: 0.10244086384773254 | global_f1: 0.9532794249775383 | global_precision: 0.9498657117278424 | global_recall: 0.9567177637511272 | global_auc: 0.993418300458074| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 61 | global_acc: 96.609% | global_loss: 0.1026824489235878 | global_f1: 0.9541778975741241 | global_precision: 0.9507609668755596 | global_recall: 0.957619477006312 | global_auc: 0.9933746155610351| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 62 | global_acc: 96.709% | global_loss: 0.1013735830783844 | global_f1: 0.9555455770094298 | global_precision: 0.9516994633273703 | global_recall: 0.9594229035166817 | global_auc: 0.9936818343478201| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 63 | global_acc: 96.775% | global_loss: 0.10202929377555847 | global_f1: 0.9565217391304348 | global_precision: 0.9509803921568627 | global_recall: 0.9621280432822362 | global_auc: 0.9937136483489245| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 64 | global_acc: 96.775% | global_loss: 0.10296550393104553 | global_f1: 0.9565217391304348 | global_precision: 0.9509803921568627 | global_recall: 0.9621280432822362 | global_auc: 0.9937288430957207| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 65 | global_acc: 96.842% | global_loss: 0.10228049755096436 | global_f1: 0.9573799910273665 | global_precision: 0.9526785714285714 | global_recall: 0.9621280432822362 | global_auc: 0.9937967446204661| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 66 | global_acc: 96.842% | global_loss: 0.1016700491309166 | global_f1: 0.9573799910273665 | global_precision: 0.9526785714285714 | global_recall: 0.9621280432822362 | global_auc: 0.9937701538135728| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 67 | global_acc: 96.842% | global_loss: 0.10324989259243011 | global_f1: 0.9574181981174361 | global_precision: 0.9518716577540107 | global_recall: 0.9630297565374211 | global_auc: 0.9936932304079172| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 68 | global_acc: 96.842% | global_loss: 0.10207391530275345 | global_f1: 0.9573799910273665 | global_precision: 0.9526785714285714 | global_recall: 0.9621280432822362 | global_auc: 0.9938157380539612| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 69 | global_acc: 96.941% | global_loss: 0.10152508318424225 | global_f1: 0.9587073608617595 | global_precision: 0.9544235924932976 | global_recall: 0.9630297565374211 | global_auc: 0.9938760422053086| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 70 | global_acc: 96.975% | global_loss: 0.10287198424339294 | global_f1: 0.9592111160914387 | global_precision: 0.9536541889483066 | global_recall: 0.9648331830477908 | global_auc: 0.9937630312760121| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 71 | global_acc: 96.908% | global_loss: 0.10178151726722717 | global_f1: 0.9583519928347515 | global_precision: 0.9519572953736655 | global_recall: 0.9648331830477908 | global_auc: 0.9940042478814013| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 72 | global_acc: 97.008% | global_loss: 0.10242634266614914 | global_f1: 0.9596774193548386 | global_precision: 0.9536954585930543 | global_recall: 0.9657348963029756 | global_auc: 0.9939164032514858| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 73 | global_acc: 96.941% | global_loss: 0.10185689479112625 | global_f1: 0.9587813620071686 | global_precision: 0.9528049866429208 | global_recall: 0.9648331830477908 | global_auc: 0.9939971253438404| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 74 | global_acc: 96.809% | global_loss: 0.1035313680768013 | global_f1: 0.9571428571428573 | global_precision: 0.9478337754199824 | global_recall: 0.9666366095581606 | global_auc: 0.9939149787439738| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 75 | global_acc: 97.041% | global_loss: 0.10464008152484894 | global_f1: 0.9601075750784401 | global_precision: 0.9545454545454546 | global_recall: 0.9657348963029756 | global_auc: 0.9937202960506479| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 76 | global_acc: 97.008% | global_loss: 0.10494562238454819 | global_f1: 0.9596412556053812 | global_precision: 0.9545049063336307 | global_recall: 0.9648331830477908 | global_auc: 0.9936072851213514| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 77 | global_acc: 96.875% | global_loss: 0.1034904271364212 | global_f1: 0.9579230080572964 | global_precision: 0.9511111111111111 | global_recall: 0.9648331830477908 | global_auc: 0.9939107052214373| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 78 | global_acc: 96.975% | global_loss: 0.10415839403867722 | global_f1: 0.9592476489028213 | global_precision: 0.952846975088968 | global_recall: 0.9657348963029756 | global_auc: 0.9938292708753267| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 79 | global_acc: 96.941% | global_loss: 0.10336081683635712 | global_f1: 0.9587813620071686 | global_precision: 0.9528049866429208 | global_recall: 0.9648331830477908 | global_auc: 0.9939114174751933| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 80 | global_acc: 96.809% | global_loss: 0.10479369014501572 | global_f1: 0.9570277529095791 | global_precision: 0.9502222222222222 | global_recall: 0.9639314697926059 | global_auc: 0.9938831647428693| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 81 | global_acc: 96.875% | global_loss: 0.10669767111539841 | global_f1: 0.9579230080572964 | global_precision: 0.9511111111111111 | global_recall: 0.9648331830477908 | global_auc: 0.9937397643199805| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 82 | global_acc: 96.975% | global_loss: 0.10489526391029358 | global_f1: 0.9592476489028213 | global_precision: 0.952846975088968 | global_recall: 0.9657348963029756 | global_auc: 0.9938342566516192| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 83 | global_acc: 96.941% | global_loss: 0.10356560349464417 | global_f1: 0.9587443946188341 | global_precision: 0.9536128456735058 | global_recall: 0.9639314697926059 | global_auc: 0.9938209612481725| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 84 | global_acc: 96.941% | global_loss: 0.10271290689706802 | global_f1: 0.9587443946188341 | global_precision: 0.9536128456735058 | global_recall: 0.9639314697926059 | global_auc: 0.9938551494284638| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 85 | global_acc: 97.041% | global_loss: 0.10294114798307419 | global_f1: 0.9601433049708912 | global_precision: 0.9537366548042705 | global_recall: 0.9666366095581606 | global_auc: 0.9939615126560369| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 86 | global_acc: 97.074% | global_loss: 0.10159797966480255 | global_f1: 0.960573476702509 | global_precision: 0.9545859305431879 | global_recall: 0.9666366095581606 | global_auc: 0.994016593613173| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 87 | global_acc: 97.074% | global_loss: 0.10222695022821426 | global_f1: 0.960573476702509 | global_precision: 0.9545859305431879 | global_recall: 0.9666366095581606 | global_auc: 0.9940004491947022| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 88 | global_acc: 97.008% | global_loss: 0.10188793390989304 | global_f1: 0.9596774193548386 | global_precision: 0.9536954585930543 | global_recall: 0.9657348963029756 | global_auc: 0.9939425192225418| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 89 | global_acc: 97.074% | global_loss: 0.10363339632749557 | global_f1: 0.9605381165919281 | global_precision: 0.9553969669937555 | global_recall: 0.9657348963029756 | global_auc: 0.9938318824724323| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 90 | global_acc: 96.975% | global_loss: 0.10415208339691162 | global_f1: 0.9592476489028213 | global_precision: 0.952846975088968 | global_recall: 0.9657348963029756 | global_auc: 0.9938674951602358| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 91 | global_acc: 97.041% | global_loss: 0.10310257971286774 | global_f1: 0.9601075750784401 | global_precision: 0.9545454545454546 | global_recall: 0.9657348963029756 | global_auc: 0.993940857297111| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 92 | global_acc: 97.141% | global_loss: 0.10334974527359009 | global_f1: 0.9614695340501791 | global_precision: 0.9554764024933214 | global_recall: 0.9675383228133454 | global_auc: 0.9940099459114496| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 93 | global_acc: 97.108% | global_loss: 0.10307516157627106 | global_f1: 0.9610040340654415 | global_precision: 0.9554367201426025 | global_recall: 0.9666366095581606 | global_auc: 0.9939949885825723| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 94 | global_acc: 97.207% | global_loss: 0.10269364714622498 | global_f1: 0.9623992837958818 | global_precision: 0.9555555555555556 | global_recall: 0.9693417493237151 | global_auc: 0.9940108955831246| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 95 | global_acc: 97.108% | global_loss: 0.10493059456348419 | global_f1: 0.9609690444145357 | global_precision: 0.95625 | global_recall: 0.9657348963029756 | global_auc: 0.9938238102631967| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 96 | global_acc: 97.108% | global_loss: 0.10456755012273788 | global_f1: 0.9609690444145357 | global_precision: 0.95625 | global_recall: 0.9657348963029756 | global_auc: 0.993856573935976| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 97 | global_acc: 97.141% | global_loss: 0.10428304225206375 | global_f1: 0.9614349775784753 | global_precision: 0.9562890276538805 | global_recall: 0.9666366095581606 | global_auc: 0.9938800783099263| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 98 | global_acc: 97.074% | global_loss: 0.10466957837343216 | global_f1: 0.9605026929982047 | global_precision: 0.9562109025915997 | global_recall: 0.9648331830477908 | global_auc: 0.9938010181430026| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 99 | global_acc: 97.074% | global_loss: 0.10511047393083572 | global_f1: 0.9605026929982047 | global_precision: 0.9562109025915997 | global_recall: 0.9648331830477908 | global_auc: 0.9937965072025474| flobal_FPR: 0.0351668169522092 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg non iid pgd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    selected_training_approach = random.choice([train_model, pgd_attack])\n",
    "\n",
    "                    # List of training approaches    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.1  # Set your desired value for epsilon\n",
    "                        alpha = 0.01   # Set your desired value for alpha\n",
    "                        num_iter = 10   # Set your desired number of iterations\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-non-iid-pgd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-non-iid-pgd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-non-iid-pgd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
