{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 200\n",
      "---------------------------------------------\n",
      "asds\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6406036019325256 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8489551949652206| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.574404776096344 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9114141513425271| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 64.262% | global_loss: 0.4796163737773895 | global_f1: 0.05949256342957131 | global_precision: 1.0 | global_recall: 0.030658250676284943 | global_auc: 0.9619860673668595| flobal_FPR: 0.9693417493237151 \n",
      "comm_round: 3 | global_acc: 90.392% | global_loss: 0.3806454539299011 | global_f1: 0.850954100051573 | global_precision: 0.9939759036144579 | global_recall: 0.7439134355275022 | global_auc: 0.9821784613514493| flobal_FPR: 0.2560865644724977 \n",
      "comm_round: 4 | global_acc: 94.781% | global_loss: 0.2814492881298065 | global_f1: 0.9255571360834519 | global_precision: 0.976 | global_recall: 0.8800721370604148 | global_auc: 0.9870417299978964| flobal_FPR: 0.11992786293958521 \n",
      "comm_round: 5 | global_acc: 95.113% | global_loss: 0.19824478030204773 | global_f1: 0.9314685314685315 | global_precision: 0.9642857142857143 | global_recall: 0.9008115419296664 | global_auc: 0.9891718435643838| flobal_FPR: 0.09918845807033363 \n",
      "comm_round: 6 | global_acc: 95.545% | global_loss: 0.14773741364479065 | global_f1: 0.938305709023941 | global_precision: 0.9586077140169332 | global_recall: 0.9188458070333634 | global_auc: 0.9907212329017551| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 7 | global_acc: 96.144% | global_loss: 0.12134511768817902 | global_f1: 0.9470802919708029 | global_precision: 0.9584487534626038 | global_recall: 0.9359783588818755 | global_auc: 0.9918451693288338| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 8 | global_acc: 96.476% | global_loss: 0.10682891309261322 | global_f1: 0.951686417502279 | global_precision: 0.9622119815668203 | global_recall: 0.9413886384129847 | global_auc: 0.9926633114766397| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 9 | global_acc: 96.642% | global_loss: 0.09800510853528976 | global_f1: 0.9540282203004097 | global_precision: 0.9632352941176471 | global_recall: 0.9449954914337241 | global_auc: 0.9931918037636438| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 10 | global_acc: 96.809% | global_loss: 0.09219343215227127 | global_f1: 0.9562443026435733 | global_precision: 0.9668202764976959 | global_recall: 0.9458972046889089 | global_auc: 0.993531786223208| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 11 | global_acc: 97.041% | global_loss: 0.08799514174461365 | global_f1: 0.9593792788680967 | global_precision: 0.9713493530499075 | global_recall: 0.9477006311992786 | global_auc: 0.9938627468018619| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 12 | global_acc: 97.241% | global_loss: 0.08483345806598663 | global_f1: 0.9621523027815777 | global_precision: 0.9732472324723247 | global_recall: 0.951307484220018 | global_auc: 0.9941234316765835| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 13 | global_acc: 97.274% | global_loss: 0.0822945088148117 | global_f1: 0.9626253418413857 | global_precision: 0.9732718894009217 | global_recall: 0.9522091974752029 | global_auc: 0.9944016854772885| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 14 | global_acc: 97.241% | global_loss: 0.08031506091356277 | global_f1: 0.9621523027815777 | global_precision: 0.9732472324723247 | global_recall: 0.951307484220018 | global_auc: 0.9945811734238181| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 15 | global_acc: 97.374% | global_loss: 0.07860340923070908 | global_f1: 0.9640091116173121 | global_precision: 0.9742173112338858 | global_recall: 0.9540126239855726 | global_auc: 0.9946941843531145| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 16 | global_acc: 97.374% | global_loss: 0.07720673084259033 | global_f1: 0.964074579354252 | global_precision: 0.9724770642201835 | global_recall: 0.9558160504959423 | global_auc: 0.994858477552848| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 17 | global_acc: 97.440% | global_loss: 0.07590330392122269 | global_f1: 0.9649522075557578 | global_precision: 0.9742647058823529 | global_recall: 0.9558160504959423 | global_auc: 0.9949591427503726| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 18 | global_acc: 97.374% | global_loss: 0.07500196248292923 | global_f1: 0.9641398093508853 | global_precision: 0.9707495429616088 | global_recall: 0.957619477006312 | global_auc: 0.9950759523663681| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 19 | global_acc: 97.374% | global_loss: 0.07403548806905746 | global_f1: 0.9641072239890959 | global_precision: 0.9716117216117216 | global_recall: 0.9567177637511272 | global_auc: 0.995138155861065| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 20 | global_acc: 97.440% | global_loss: 0.07326973229646683 | global_f1: 0.9649840836743975 | global_precision: 0.9733944954128441 | global_recall: 0.9567177637511272 | global_auc: 0.9951770923997301| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 21 | global_acc: 97.573% | global_loss: 0.0725112184882164 | global_f1: 0.9668934240362811 | global_precision: 0.9726277372262774 | global_recall: 0.9612263300270514 | global_auc: 0.99523692171524| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 22 | global_acc: 97.540% | global_loss: 0.07213862240314484 | global_f1: 0.9663023679417122 | global_precision: 0.9760809567617296 | global_recall: 0.9567177637511272 | global_auc: 0.99527680792558| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 23 | global_acc: 97.606% | global_loss: 0.07141731679439545 | global_f1: 0.9672429481346679 | global_precision: 0.9761248852157943 | global_recall: 0.9585211902614968 | global_auc: 0.9953599041971214| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 24 | global_acc: 97.706% | global_loss: 0.07104314118623734 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.9954909588882384| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 25 | global_acc: 97.706% | global_loss: 0.07059236615896225 | global_f1: 0.9686221009549795 | global_precision: 0.9770642201834863 | global_recall: 0.9603246167718665 | global_auc: 0.9955042542916851| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 26 | global_acc: 97.739% | global_loss: 0.07019232213497162 | global_f1: 0.9691749773345422 | global_precision: 0.9744758432087511 | global_recall: 0.9639314697926059 | global_auc: 0.995636733490314| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 27 | global_acc: 97.806% | global_loss: 0.06986968219280243 | global_f1: 0.9700272479564032 | global_precision: 0.9771271729185728 | global_recall: 0.9630297565374211 | global_auc: 0.9956766197006539| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 28 | global_acc: 97.806% | global_loss: 0.06959361582994461 | global_f1: 0.97 | global_precision: 0.9780018331805683 | global_recall: 0.9621280432822362 | global_auc: 0.9957136568959696| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 29 | global_acc: 97.773% | global_loss: 0.06949344277381897 | global_f1: 0.9695592912312585 | global_precision: 0.9771062271062271 | global_recall: 0.9621280432822362 | global_auc: 0.9957411973745376| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 30 | global_acc: 97.872% | global_loss: 0.0694788247346878 | global_f1: 0.9709618874773139 | global_precision: 0.9771689497716894 | global_recall: 0.9648331830477908 | global_auc: 0.9957996021825354| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 31 | global_acc: 97.906% | global_loss: 0.06935235112905502 | global_f1: 0.9714285714285713 | global_precision: 0.9771897810218978 | global_recall: 0.9657348963029756 | global_auc: 0.9958309413478025| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 32 | global_acc: 97.972% | global_loss: 0.06926773488521576 | global_f1: 0.9723856948845632 | global_precision: 0.9763636363636363 | global_recall: 0.9684400360685302 | global_auc: 0.9958632301847443| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 33 | global_acc: 97.872% | global_loss: 0.0692809596657753 | global_f1: 0.9710669077757685 | global_precision: 0.9737080689029919 | global_recall: 0.9684400360685302 | global_auc: 0.9958812739465648| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 34 | global_acc: 97.872% | global_loss: 0.0692000612616539 | global_f1: 0.9710669077757685 | global_precision: 0.9737080689029919 | global_recall: 0.9684400360685302 | global_auc: 0.9959254336794412| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 35 | global_acc: 97.872% | global_loss: 0.06914734095335007 | global_f1: 0.9710669077757685 | global_precision: 0.9737080689029919 | global_recall: 0.9684400360685302 | global_auc: 0.9959510748146597| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 36 | global_acc: 97.906% | global_loss: 0.06914873421192169 | global_f1: 0.9715061058344641 | global_precision: 0.9745916515426497 | global_recall: 0.9684400360685302 | global_auc: 0.9959572476805455| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 37 | global_acc: 97.972% | global_loss: 0.06948603689670563 | global_f1: 0.9723856948845632 | global_precision: 0.9763636363636363 | global_recall: 0.9684400360685302 | global_auc: 0.9959738669348539| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 38 | global_acc: 97.872% | global_loss: 0.06962532550096512 | global_f1: 0.9710407239819004 | global_precision: 0.9745685740236149 | global_recall: 0.9675383228133454 | global_auc: 0.9960270485486404| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 39 | global_acc: 97.972% | global_loss: 0.0696864053606987 | global_f1: 0.9724604966139955 | global_precision: 0.9737793851717902 | global_recall: 0.9711451758340848 | global_auc: 0.9960374949370628| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 40 | global_acc: 98.105% | global_loss: 0.06960077583789825 | global_f1: 0.9741730856366108 | global_precision: 0.9790528233151184 | global_recall: 0.9693417493237151 | global_auc: 0.9960123286376817| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 41 | global_acc: 98.172% | global_loss: 0.06964776664972305 | global_f1: 0.9751243781094527 | global_precision: 0.9782214156079855 | global_recall: 0.9720468890892696 | global_auc: 0.9960365452653881| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 42 | global_acc: 98.138% | global_loss: 0.06994429230690002 | global_f1: 0.9746835443037976 | global_precision: 0.9773345421577516 | global_recall: 0.9720468890892696 | global_auc: 0.9960503155046723| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 43 | global_acc: 98.238% | global_loss: 0.0708080530166626 | global_f1: 0.9761368752814047 | global_precision: 0.9748201438848921 | global_recall: 0.9774571686203787 | global_auc: 0.9961077706409951| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 44 | global_acc: 98.172% | global_loss: 0.07029546797275543 | global_f1: 0.9751018560434586 | global_precision: 0.9790909090909091 | global_recall: 0.9711451758340848 | global_auc: 0.9960688341023299| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 45 | global_acc: 98.238% | global_loss: 0.07028766721487045 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9960707334456794| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 46 | global_acc: 98.305% | global_loss: 0.07102601230144501 | global_f1: 0.9770166741775576 | global_precision: 0.9765765765765766 | global_recall: 0.9774571686203787 | global_auc: 0.9960750069682158| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 47 | global_acc: 98.238% | global_loss: 0.07097724825143814 | global_f1: 0.9760722347629797 | global_precision: 0.9773960216998192 | global_recall: 0.9747520288548241 | global_auc: 0.9960949500733859| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 48 | global_acc: 98.238% | global_loss: 0.07104559242725372 | global_f1: 0.9760072430964237 | global_precision: 0.98 | global_recall: 0.9720468890892696 | global_auc: 0.9960977990884102| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 49 | global_acc: 98.305% | global_loss: 0.07121404260396957 | global_f1: 0.9769335142469472 | global_precision: 0.9800362976406534 | global_recall: 0.9738503155996393 | global_auc: 0.9960977990884102| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 50 | global_acc: 98.305% | global_loss: 0.07151081413030624 | global_f1: 0.9769335142469472 | global_precision: 0.9800362976406534 | global_recall: 0.9738503155996393 | global_auc: 0.9960731076248663| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 51 | global_acc: 98.338% | global_loss: 0.07177325338125229 | global_f1: 0.977416440831075 | global_precision: 0.9791855203619909 | global_recall: 0.975653742110009 | global_auc: 0.9960811798341018| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 52 | global_acc: 98.305% | global_loss: 0.07208526134490967 | global_f1: 0.9769126301493889 | global_precision: 0.980909090909091 | global_recall: 0.9729486023444545 | global_auc: 0.996076431475728| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 53 | global_acc: 98.338% | global_loss: 0.07224047183990479 | global_f1: 0.9773960216998192 | global_precision: 0.9800543970988214 | global_recall: 0.9747520288548241 | global_auc: 0.9960916262225243| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 54 | global_acc: 98.371% | global_loss: 0.07260917872190475 | global_f1: 0.9778581111613194 | global_precision: 0.980072463768116 | global_recall: 0.975653742110009 | global_auc: 0.9960579128780702| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 55 | global_acc: 98.404% | global_loss: 0.07268799096345901 | global_f1: 0.9783001808318263 | global_precision: 0.9809610154125114 | global_recall: 0.975653742110009 | global_auc: 0.9960792804907523| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 56 | global_acc: 98.404% | global_loss: 0.0728052631020546 | global_f1: 0.9783001808318263 | global_precision: 0.9809610154125114 | global_recall: 0.975653742110009 | global_auc: 0.9960868778641503| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 57 | global_acc: 98.438% | global_loss: 0.073076032102108 | global_f1: 0.9787426503844414 | global_precision: 0.9818511796733213 | global_recall: 0.975653742110009 | global_auc: 0.9960493658329974| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 58 | global_acc: 98.371% | global_loss: 0.07324577867984772 | global_f1: 0.9778380823156942 | global_precision: 0.9809437386569873 | global_recall: 0.9747520288548241 | global_auc: 0.9960612367289319| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 59 | global_acc: 98.438% | global_loss: 0.07354306429624557 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9960244369515349| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 60 | global_acc: 98.238% | global_loss: 0.07465806603431702 | global_f1: 0.9761583445793971 | global_precision: 0.973967684021544 | global_recall: 0.9783588818755635 | global_auc: 0.9960621864006067| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 61 | global_acc: 98.404% | global_loss: 0.07387100905179977 | global_f1: 0.9782805429864253 | global_precision: 0.9818346957311535 | global_recall: 0.9747520288548241 | global_auc: 0.9960493658329973| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 62 | global_acc: 98.404% | global_loss: 0.07408799231052399 | global_f1: 0.978319783197832 | global_precision: 0.9800904977375565 | global_recall: 0.9765554553651938 | global_auc: 0.9960260988769658| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 63 | global_acc: 98.438% | global_loss: 0.07434029877185822 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9960417684595992| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 64 | global_acc: 98.438% | global_loss: 0.07477444410324097 | global_f1: 0.9787234042553192 | global_precision: 0.9827272727272728 | global_recall: 0.9747520288548241 | global_auc: 0.9960075802793079| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 65 | global_acc: 98.338% | global_loss: 0.07524921745061874 | global_f1: 0.9774774774774774 | global_precision: 0.9765976597659766 | global_recall: 0.9783588818755635 | global_auc: 0.9960455671462984| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 66 | global_acc: 98.438% | global_loss: 0.07510583102703094 | global_f1: 0.9787234042553192 | global_precision: 0.9827272727272728 | global_recall: 0.9747520288548241 | global_auc: 0.995993810040024| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 67 | global_acc: 98.438% | global_loss: 0.07530535757541656 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9960066306076332| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 68 | global_acc: 98.471% | global_loss: 0.07552191615104675 | global_f1: 0.9792231255645889 | global_precision: 0.9809954751131221 | global_recall: 0.9774571686203787 | global_auc: 0.9959990332342351| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 69 | global_acc: 98.438% | global_loss: 0.07626363635063171 | global_f1: 0.9788383610986042 | global_precision: 0.9775179856115108 | global_recall: 0.9801623083859333 | global_auc: 0.9960223001902666| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 70 | global_acc: 98.471% | global_loss: 0.0760781392455101 | global_f1: 0.9792605951307484 | global_precision: 0.9792605951307484 | global_recall: 0.9792605951307484 | global_auc: 0.9960042564284461| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 71 | global_acc: 98.504% | global_loss: 0.07647924870252609 | global_f1: 0.9796103307657453 | global_precision: 0.9845173041894353 | global_recall: 0.9747520288548241 | global_auc: 0.9959805146365774| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 72 | global_acc: 98.471% | global_loss: 0.0763050764799118 | global_f1: 0.9792605951307484 | global_precision: 0.9792605951307484 | global_recall: 0.9792605951307484 | global_auc: 0.9960042564284463| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 73 | global_acc: 98.471% | global_loss: 0.07683096826076508 | global_f1: 0.9791477787851314 | global_precision: 0.9845031905195989 | global_recall: 0.9738503155996393 | global_auc: 0.9959824139799267| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 74 | global_acc: 98.438% | global_loss: 0.07714676856994629 | global_f1: 0.9788573999100315 | global_precision: 0.9766606822262118 | global_recall: 0.9810640216411182 | global_auc: 0.9960128034735192| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 75 | global_acc: 98.404% | global_loss: 0.07738323509693146 | global_f1: 0.9783001808318263 | global_precision: 0.9809610154125114 | global_recall: 0.975653742110009 | global_auc: 0.9959904861891622| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 76 | global_acc: 98.438% | global_loss: 0.07738518714904785 | global_f1: 0.9787234042553192 | global_precision: 0.9827272727272728 | global_recall: 0.9747520288548241 | global_auc: 0.9959695934123175| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 77 | global_acc: 98.504% | global_loss: 0.07718247175216675 | global_f1: 0.9797205948625507 | global_precision: 0.9792792792792793 | global_recall: 0.9801623083859333 | global_auc: 0.9960298975636648| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 78 | global_acc: 98.504% | global_loss: 0.07751018553972244 | global_f1: 0.9797205948625507 | global_precision: 0.9792792792792793 | global_recall: 0.9801623083859333 | global_auc: 0.9960128034735191| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 79 | global_acc: 98.438% | global_loss: 0.0781882032752037 | global_f1: 0.9788573999100315 | global_precision: 0.9766606822262118 | global_recall: 0.9810640216411182 | global_auc: 0.9960016448313407| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 80 | global_acc: 98.438% | global_loss: 0.07800369709730148 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9960085299509827| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 81 | global_acc: 98.471% | global_loss: 0.07812761515378952 | global_f1: 0.9792792792792793 | global_precision: 0.9783978397839784 | global_recall: 0.9801623083859333 | global_auc: 0.9959966590550482| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 82 | global_acc: 98.438% | global_loss: 0.07836983352899551 | global_f1: 0.9788001804239964 | global_precision: 0.9792418772563177 | global_recall: 0.9783588818755635 | global_auc: 0.996038919444575| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 83 | global_acc: 98.471% | global_loss: 0.07854264974594116 | global_f1: 0.9792605951307484 | global_precision: 0.9792605951307484 | global_recall: 0.9792605951307484 | global_auc: 0.9960289478919901| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 84 | global_acc: 98.471% | global_loss: 0.07884824275970459 | global_f1: 0.9792605951307484 | global_precision: 0.9792605951307484 | global_recall: 0.9792605951307484 | global_auc: 0.9960218253544293| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 85 | global_acc: 98.537% | global_loss: 0.0791269838809967 | global_f1: 0.9801980198019802 | global_precision: 0.9784366576819407 | global_recall: 0.981965734896303 | global_auc: 0.9959985583983977| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 86 | global_acc: 98.504% | global_loss: 0.07932554185390472 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9960018822492593| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 87 | global_acc: 98.471% | global_loss: 0.08036812394857407 | global_f1: 0.97931654676259 | global_precision: 0.9766816143497757 | global_recall: 0.981965734896303 | global_auc: 0.9959524993221718| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 88 | global_acc: 98.537% | global_loss: 0.07963903993368149 | global_f1: 0.98018018018018 | global_precision: 0.9792979297929792 | global_recall: 0.9810640216411182 | global_auc: 0.9960279982203153| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 89 | global_acc: 98.537% | global_loss: 0.0798817127943039 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9960011699955033| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 90 | global_acc: 98.537% | global_loss: 0.0805451050400734 | global_f1: 0.9801980198019802 | global_precision: 0.9784366576819407 | global_recall: 0.981965734896303 | global_auc: 0.9959539238296841| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 91 | global_acc: 98.504% | global_loss: 0.0804421454668045 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9960109041301696| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 92 | global_acc: 98.570% | global_loss: 0.08055265992879868 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9960118538018443| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 93 | global_acc: 98.570% | global_loss: 0.08072154223918915 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.996001407413422| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 94 | global_acc: 98.504% | global_loss: 0.08112959563732147 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9960265737128031| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 95 | global_acc: 98.570% | global_loss: 0.08123984932899475 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9960182640856491| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 96 | global_acc: 98.504% | global_loss: 0.08154435455799103 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9959567728447083| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 97 | global_acc: 98.570% | global_loss: 0.08160416036844254 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9959985583983977| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 98 | global_acc: 98.570% | global_loss: 0.08178430050611496 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9959548735013587| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 99 | global_acc: 98.504% | global_loss: 0.08220285177230835 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9959228220823356| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 100 | global_acc: 98.537% | global_loss: 0.08210255950689316 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9960251492052911| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 101 | global_acc: 98.537% | global_loss: 0.08243756741285324 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9959202104852299| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 102 | global_acc: 98.438% | global_loss: 0.08361220359802246 | global_f1: 0.9788573999100315 | global_precision: 0.9766606822262118 | global_recall: 0.9810640216411182 | global_auc: 0.9959558231730334| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 103 | global_acc: 98.504% | global_loss: 0.08330278098583221 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9959505999788222| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 104 | global_acc: 98.570% | global_loss: 0.08308097720146179 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9959358800678635| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 105 | global_acc: 98.537% | global_loss: 0.08312952518463135 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9959297072019776| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 106 | global_acc: 98.537% | global_loss: 0.08345019817352295 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9959392039187251| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 107 | global_acc: 98.504% | global_loss: 0.08361461758613586 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9959275704407095| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 108 | global_acc: 98.570% | global_loss: 0.08390796184539795 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9959667443972933| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 109 | global_acc: 98.570% | global_loss: 0.08389649540185928 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9959465638742047| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 110 | global_acc: 98.504% | global_loss: 0.08400826901197433 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9959382542470504| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 111 | global_acc: 98.504% | global_loss: 0.08453194051980972 | global_f1: 0.9796656122910077 | global_precision: 0.9818840579710145 | global_recall: 0.9774571686203787 | global_auc: 0.9960716831173543| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 112 | global_acc: 98.537% | global_loss: 0.08446680009365082 | global_f1: 0.9801264679313461 | global_precision: 0.9819004524886877 | global_recall: 0.9783588818755635 | global_auc: 0.9959434774412617| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 113 | global_acc: 98.537% | global_loss: 0.08448781073093414 | global_f1: 0.9801264679313461 | global_precision: 0.9819004524886877 | global_recall: 0.9783588818755635 | global_auc: 0.9959363549037008| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 114 | global_acc: 98.570% | global_loss: 0.08483964204788208 | global_f1: 0.9806044203879115 | global_precision: 0.9810469314079422 | global_recall: 0.9801623083859333 | global_auc: 0.995967694068968| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 115 | global_acc: 98.570% | global_loss: 0.08487897366285324 | global_f1: 0.98058690744921 | global_precision: 0.9819168173598554 | global_recall: 0.9792605951307484 | global_auc: 0.9959641328001876| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 116 | global_acc: 98.471% | global_loss: 0.085686095058918 | global_f1: 0.9792605951307484 | global_precision: 0.9792605951307484 | global_recall: 0.9792605951307484 | global_auc: 0.9959795649649025| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 117 | global_acc: 98.570% | global_loss: 0.08529997617006302 | global_f1: 0.9806044203879115 | global_precision: 0.9810469314079422 | global_recall: 0.9801623083859333 | global_auc: 0.9959691185764801| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 118 | global_acc: 98.537% | global_loss: 0.08568155020475388 | global_f1: 0.9801084990958407 | global_precision: 0.9827742520398912 | global_recall: 0.9774571686203787 | global_auc: 0.9959420529337495| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 119 | global_acc: 98.570% | global_loss: 0.08557234704494476 | global_f1: 0.9805693628558518 | global_precision: 0.9827898550724637 | global_recall: 0.9783588818755635 | global_auc: 0.995922584664417| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 120 | global_acc: 98.604% | global_loss: 0.08567266166210175 | global_f1: 0.9810469314079423 | global_precision: 0.98193315266486 | global_recall: 0.9801623083859333 | global_auc: 0.9959629457105942| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 121 | global_acc: 98.570% | global_loss: 0.08589381724596024 | global_f1: 0.9806044203879115 | global_precision: 0.9810469314079422 | global_recall: 0.9801623083859333 | global_auc: 0.9959736295169352| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 122 | global_acc: 98.570% | global_loss: 0.0860019102692604 | global_f1: 0.9805517865219356 | global_precision: 0.9836660617059891 | global_recall: 0.9774571686203787 | global_auc: 0.9959211601569048| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 123 | global_acc: 98.570% | global_loss: 0.0863122045993805 | global_f1: 0.9806044203879115 | global_precision: 0.9810469314079422 | global_recall: 0.9801623083859333 | global_auc: 0.9959805146365772| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 124 | global_acc: 98.504% | global_loss: 0.08762096613645554 | global_f1: 0.9797205948625507 | global_precision: 0.9792792792792793 | global_recall: 0.9801623083859333 | global_auc: 0.9960052061001211| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 125 | global_acc: 98.570% | global_loss: 0.08653606474399567 | global_f1: 0.9805693628558518 | global_precision: 0.9827898550724637 | global_recall: 0.9783588818755635 | global_auc: 0.9959477509637981| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 126 | global_acc: 98.570% | global_loss: 0.08671959489583969 | global_f1: 0.9805517865219356 | global_precision: 0.9836660617059891 | global_recall: 0.9774571686203787 | global_auc: 0.995930182037815| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 127 | global_acc: 98.637% | global_loss: 0.08663497120141983 | global_f1: 0.9814898419864561 | global_precision: 0.9828209764918626 | global_recall: 0.9801623083859333 | global_auc: 0.995957010262627| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 128 | global_acc: 98.604% | global_loss: 0.08756362646818161 | global_f1: 0.9810640216411182 | global_precision: 0.9810640216411182 | global_recall: 0.9810640216411182 | global_auc: 0.9959914358608368| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 129 | global_acc: 98.471% | global_loss: 0.08891885727643967 | global_f1: 0.9792792792792793 | global_precision: 0.9783978397839784 | global_recall: 0.9801623083859333 | global_auc: 0.9960230124440228| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 130 | global_acc: 98.570% | global_loss: 0.08725239336490631 | global_f1: 0.9805517865219356 | global_precision: 0.9836660617059891 | global_recall: 0.9774571686203787 | global_auc: 0.9959498877250663| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 131 | global_acc: 98.637% | global_loss: 0.08757517486810684 | global_f1: 0.9814898419864561 | global_precision: 0.9828209764918626 | global_recall: 0.9801623083859333 | global_auc: 0.9959574850984643| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 132 | global_acc: 98.570% | global_loss: 0.08772210776805878 | global_f1: 0.9805517865219356 | global_precision: 0.9836660617059891 | global_recall: 0.9774571686203787 | global_auc: 0.9959299446198964| flobal_FPR: 0.02254283137962128 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 136\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# fit local model with client's data\u001b[39;00m\n\u001b[0;32m    132\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(clients_batched[client]\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    133\u001b[0m                                         torch\u001b[38;5;241m.\u001b[39mtensor(clients_batched[client]\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)),\n\u001b[0;32m    134\u001b[0m                           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 136\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# scale the model weights and add to the list\u001b[39;00m\n\u001b[0;32m    140\u001b[0m scaling_factor \u001b[38;5;241m=\u001b[39m weight_scalling_factor(clients_batched, client)\n",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            print(\"asds\")\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                # ...\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((all_avg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_avg =[]\n",
    "\n",
    "all_std =[]\n",
    "\n",
    "n_clients = [5,10,15]\n",
    "n_round = [10,20]\n",
    "\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
    "\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d==1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d==2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d==3:\n",
    "        use_data = Tuandromd_data\n",
    "        \n",
    "        \n",
    "    print('===================================================================================================')\n",
    "    print('Working with:',dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round: #number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients: #number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            # from sklearn.utils import shuffle\n",
    "            # use_data = shuffle(use_data)\n",
    "            # use_data\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "\n",
    "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
    "\n",
    "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
    "\n",
    "\n",
    "            #Do feature scaling \n",
    "\n",
    "\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "\n",
    "            #binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "            #split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "            #create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            #process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "                #process and batch the test set  \n",
    "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "            #==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            #==============================================\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            all_results=list()\n",
    "\n",
    "            #create optimizer\n",
    "            lr = 0.01 \n",
    "            loss='binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "            optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, \n",
    "                            decay=lr / comms_round, \n",
    "                            momentum=0.9\n",
    "                           )\n",
    "\n",
    "            #initialize global model\n",
    "            smlp_global = SimpleMLP()\n",
    "            global_model = smlp_global.build(X.shape[1],1)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            #commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = global_model.get_weights()\n",
    "\n",
    "                #initial list to collect local model weights after scalling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                #randomize client data - using keys\n",
    "                client_names= list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                #loop through each client and create new local model\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP()\n",
    "                    local_model = smlp_local.build(X.shape[1],1)\n",
    "                    local_model.compile(loss=loss, \n",
    "                                  optimizer=optimizer, \n",
    "                                  metrics=metrics)\n",
    "\n",
    "                    #set local model weight to the weight of the global model\n",
    "                    local_model.set_weights(global_weights)\n",
    "\n",
    "                    #fit local model with client's data\n",
    "                    local_model.fit(clients_batched[client], epochs=32, verbose=0)\n",
    "\n",
    "                    #scale the model weights and add to list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    #clear session to free memory after each communication round\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                #update global model \n",
    "                global_model.set_weights(average_weights)\n",
    "\n",
    "                #test global model and print out metrics after each communications round\n",
    "                for(X_test, Y_test) in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
    "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/FedAvg-'+dataset[d]+'-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "            \n",
    "            \n",
    "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv(f'FedAvg-results.csv')     \n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6104c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-all-avg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# make a little extra space between the subplots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "s1 = np.array(all_results) #FedAvg\n",
    "\n",
    "t = range(0,s1.shape[0])\n",
    "\n",
    "ax1.plot(t, s1[:,0],label='Acc of FedAvg')\n",
    "ax1.set_xlim(0,s1.shape[0])\n",
    "ax1.set_xlabel('Rounds')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0.98,1)\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(t, s1[:,1],label='Error of FedAvg')\n",
    "ax2.set_xlim(0, s1.shape[0])\n",
    "ax2.set_xlabel('Rounds')\n",
    "ax2.set_ylabel('error')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
