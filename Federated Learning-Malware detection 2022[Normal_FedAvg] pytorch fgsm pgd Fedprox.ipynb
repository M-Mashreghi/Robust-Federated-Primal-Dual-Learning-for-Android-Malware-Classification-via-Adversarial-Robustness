{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3ab07f",
   "metadata": {},
   "source": [
    "## non iid FedProx FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6356e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 924, (1,): 279}\n",
      "Client client_2: {(0,): 476, (1,): 727}\n",
      "Client client_3: {(0,): 936, (1,): 267}\n",
      "Client client_4: {(0,): 497, (1,): 706}\n",
      "Client client_5: {(0,): 377, (1,): 826}\n",
      "Client client_6: {(0,): 820, (1,): 383}\n",
      "Client client_7: {(0,): 462, (1,): 741}\n",
      "Client client_8: {(0,): 594, (1,): 609}\n",
      "Client client_9: {(0,): 0, (1,): 1203}\n",
      "Client client_10: {(0,): 841, (1,): 362}\n",
      "|=======================|\n",
      "|Traditional FedProx-non-iid-FGSM  2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 37.400% | global_loss: 0.7007073163986206 | global_f1: 0.5401709401709401 | global_precision: 0.3703951774949766 | global_recall: 0.9972948602344455 | global_auc: 0.5129243192397308| flobal_FPR: 0.002705139765554554 \n",
      "comm_round: 1 | global_acc: 38.198% | global_loss: 0.6982423663139343 | global_f1: 0.5431309904153355 | global_precision: 0.3733108108108108 | global_recall: 0.9963931469792606 | global_auc: 0.6021927444134376| flobal_FPR: 0.0036068530207394047 \n",
      "comm_round: 2 | global_acc: 39.262% | global_loss: 0.6966512799263 | global_f1: 0.5472118959107807 | global_precision: 0.3773069036226931 | global_recall: 0.9954914337240758 | global_auc: 0.6625640850317025| flobal_FPR: 0.004508566275924256 \n",
      "comm_round: 3 | global_acc: 40.525% | global_loss: 0.6949528455734253 | global_f1: 0.5521902377972466 | global_precision: 0.3821898821898822 | global_recall: 0.9945897204688909 | global_auc: 0.7326721719133653| flobal_FPR: 0.005410279531109108 \n",
      "comm_round: 4 | global_acc: 43.152% | global_loss: 0.6930819749832153 | global_f1: 0.5631067961165048 | global_precision: 0.3928698752228164 | global_recall: 0.9936880072137061 | global_auc: 0.7810398999805791| flobal_FPR: 0.0063119927862939585 \n",
      "comm_round: 5 | global_acc: 45.612% | global_loss: 0.6916530132293701 | global_f1: 0.5701523909616395 | global_precision: 0.40229885057471265 | global_recall: 0.9783588818755635 | global_auc: 0.8069797069408179| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 6 | global_acc: 47.507% | global_loss: 0.6906801462173462 | global_f1: 0.5785962103015746 | global_precision: 0.41091736163760423 | global_recall: 0.9774571686203787 | global_auc: 0.8230680947829312| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 7 | global_acc: 51.463% | global_loss: 0.6893608570098877 | global_f1: 0.5960154952960708 | global_precision: 0.429940119760479 | global_recall: 0.9711451758340848 | global_auc: 0.8376220506165506| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 8 | global_acc: 54.987% | global_loss: 0.6880630254745483 | global_f1: 0.6124785346307956 | global_precision: 0.44863731656184486 | global_recall: 0.9648331830477908 | global_auc: 0.8486536742084843| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 9 | global_acc: 58.278% | global_loss: 0.6867891550064087 | global_f1: 0.6265992264207082 | global_precision: 0.4675843694493783 | global_recall: 0.9495040577096483 | global_auc: 0.8575915091754902| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 10 | global_acc: 59.142% | global_loss: 0.6865049600601196 | global_f1: 0.6314842578710644 | global_precision: 0.47304582210242585 | global_recall: 0.9495040577096483 | global_auc: 0.8609443250232314| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 11 | global_acc: 62.699% | global_loss: 0.685007631778717 | global_f1: 0.6515527950310559 | global_precision: 0.49692089057318806 | global_recall: 0.9458972046889089 | global_auc: 0.8684678614486008| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 12 | global_acc: 63.996% | global_loss: 0.6841391921043396 | global_f1: 0.6591123701605289 | global_precision: 0.5062862669245648 | global_recall: 0.9440937781785392 | global_auc: 0.8736452340014749| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 13 | global_acc: 67.121% | global_loss: 0.6818175911903381 | global_f1: 0.6781646599414254 | global_precision: 0.5305498981670062 | global_recall: 0.939585211902615 | global_auc: 0.879945593309753| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 14 | global_acc: 67.420% | global_loss: 0.6812779903411865 | global_f1: 0.6801566579634465 | global_precision: 0.5329923273657289 | global_recall: 0.939585211902615 | global_auc: 0.8864045477877162| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 15 | global_acc: 69.116% | global_loss: 0.6795018315315247 | global_f1: 0.6910542068506816 | global_precision: 0.5474183350895679 | global_recall: 0.9368800721370604 | global_auc: 0.8913917485877195| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 16 | global_acc: 70.878% | global_loss: 0.6777166724205017 | global_f1: 0.7032520325203253 | global_precision: 0.5632121540965817 | global_recall: 0.9359783588818755 | global_auc: 0.8975667512349295| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 17 | global_acc: 72.108% | global_loss: 0.6756281852722168 | global_f1: 0.7115847370230318 | global_precision: 0.575 | global_recall: 0.933273219116321 | global_auc: 0.9016078416289529| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 18 | global_acc: 73.172% | global_loss: 0.6743786931037903 | global_f1: 0.7216281476371162 | global_precision: 0.5843575418994413 | global_recall: 0.9431920649233544 | global_auc: 0.9056415720674968| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 19 | global_acc: 74.036% | global_loss: 0.6718550324440002 | global_f1: 0.7264448336252189 | global_precision: 0.5939289805269187 | global_recall: 0.9350766456266907 | global_auc: 0.9104806240862378| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 20 | global_acc: 75.266% | global_loss: 0.6673650741577148 | global_f1: 0.734475374732334 | global_precision: 0.6077968103957472 | global_recall: 0.9278629395852119 | global_auc: 0.9132997244527636| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 21 | global_acc: 74.900% | global_loss: 0.6655847430229187 | global_f1: 0.7331212442559208 | global_precision: 0.602906976744186 | global_recall: 0.9350766456266907 | global_auc: 0.9156636946691605| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 22 | global_acc: 75.997% | global_loss: 0.6604335904121399 | global_f1: 0.7410329985652798 | global_precision: 0.6152471709350804 | global_recall: 0.9314697926059513 | global_auc: 0.9197399229151503| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 23 | global_acc: 76.363% | global_loss: 0.6559798717498779 | global_f1: 0.7434139299891735 | global_precision: 0.6197352587244284 | global_recall: 0.9287646528403968 | global_auc: 0.9221354697147329| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 24 | global_acc: 76.596% | global_loss: 0.6539176106452942 | global_f1: 0.7454808387563269 | global_precision: 0.6222088111044055 | global_recall: 0.9296663660955816 | global_auc: 0.9258534343214193| flobal_FPR: 0.0703336339044184 \n",
      "comm_round: 25 | global_acc: 77.394% | global_loss: 0.6479704976081848 | global_f1: 0.7527272727272727 | global_precision: 0.6307129798903108 | global_recall: 0.933273219116321 | global_auc: 0.9279446113492413| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 26 | global_acc: 77.859% | global_loss: 0.6415585279464722 | global_f1: 0.7555066079295154 | global_precision: 0.6371517027863777 | global_recall: 0.9278629395852119 | global_auc: 0.9311193637579648| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 27 | global_acc: 77.460% | global_loss: 0.6397238969802856 | global_f1: 0.7543478260869565 | global_precision: 0.630526953361599 | global_recall: 0.9386834986474302 | global_auc: 0.9314503243366188| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 28 | global_acc: 78.989% | global_loss: 0.6222016215324402 | global_f1: 0.7660991857883049 | global_precision: 0.6497175141242938 | global_recall: 0.933273219116321 | global_auc: 0.9365291684532365| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 29 | global_acc: 79.754% | global_loss: 0.6094624400138855 | global_f1: 0.7731843575418994 | global_precision: 0.6586294416243654 | global_recall: 0.9359783588818755 | global_auc: 0.9411581056139365| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 30 | global_acc: 79.122% | global_loss: 0.6021955609321594 | global_f1: 0.7686072218128225 | global_precision: 0.6498442367601246 | global_recall: 0.9404869251577999 | global_auc: 0.9413112401714916| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 31 | global_acc: 79.255% | global_loss: 0.6025499701499939 | global_f1: 0.7710931768158474 | global_precision: 0.6499690785405071 | global_recall: 0.9477006311992786 | global_auc: 0.9460752681279264| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 32 | global_acc: 79.754% | global_loss: 0.5924233794212341 | global_f1: 0.7753596458871266 | global_precision: 0.6560549313358303 | global_recall: 0.9477006311992786 | global_auc: 0.948146976886416| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 33 | global_acc: 80.352% | global_loss: 0.5871263742446899 | global_f1: 0.7803790412486065 | global_precision: 0.6637168141592921 | global_recall: 0.9467989179440938 | global_auc: 0.9511037796457819| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 34 | global_acc: 81.316% | global_loss: 0.5588151216506958 | global_f1: 0.7880844645550528 | global_precision: 0.6772521062864549 | global_recall: 0.9422903516681695 | global_auc: 0.9536548351821067| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 35 | global_acc: 81.882% | global_loss: 0.5299151539802551 | global_f1: 0.7934823796892763 | global_precision: 0.6843137254901961 | global_recall: 0.9440937781785392 | global_auc: 0.9566911729442338| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 36 | global_acc: 82.414% | global_loss: 0.5129959583282471 | global_f1: 0.798782807151008 | global_precision: 0.6907894736842105 | global_recall: 0.9467989179440938 | global_auc: 0.9620145575171024| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 37 | global_acc: 82.846% | global_loss: 0.5040218234062195 | global_f1: 0.8021472392638037 | global_precision: 0.6977985323549033 | global_recall: 0.9431920649233544 | global_auc: 0.9636138046173985| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 38 | global_acc: 83.278% | global_loss: 0.46450039744377136 | global_f1: 0.8063149788217173 | global_precision: 0.7036290322580645 | global_recall: 0.9440937781785392 | global_auc: 0.9652885506158384| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 39 | global_acc: 84.475% | global_loss: 0.4285832643508911 | global_f1: 0.8170779475127302 | global_precision: 0.7222991689750693 | global_recall: 0.9404869251577999 | global_auc: 0.9698408017888016| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 40 | global_acc: 84.475% | global_loss: 0.4149853587150574 | global_f1: 0.8170779475127302 | global_precision: 0.7222991689750693 | global_recall: 0.9404869251577999 | global_auc: 0.9703692940758055| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 41 | global_acc: 85.140% | global_loss: 0.40147846937179565 | global_f1: 0.8232502965599051 | global_precision: 0.7330985915492958 | global_recall: 0.9386834986474302 | global_auc: 0.9705326376038644| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 42 | global_acc: 90.525% | global_loss: 0.38066336512565613 | global_f1: 0.8787749893662272 | global_precision: 0.8317230273752013 | global_recall: 0.9314697926059513 | global_auc: 0.9748650397841206| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 43 | global_acc: 90.293% | global_loss: 0.3646352291107178 | global_f1: 0.8753202391118702 | global_precision: 0.8313057583130575 | global_recall: 0.9242560865644724 | global_auc: 0.9721841166462725| flobal_FPR: 0.0757439134355275 \n",
      "comm_round: 44 | global_acc: 90.359% | global_loss: 0.3454355001449585 | global_f1: 0.8764906303236797 | global_precision: 0.8305084745762712 | global_recall: 0.9278629395852119 | global_auc: 0.9732012150099407| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 45 | global_acc: 91.556% | global_loss: 0.3189923167228699 | global_f1: 0.8915456874466268 | global_precision: 0.8467153284671532 | global_recall: 0.9413886384129847 | global_auc: 0.9768792934062871| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 46 | global_acc: 92.154% | global_loss: 0.2920193374156952 | global_f1: 0.8984509466437177 | global_precision: 0.8592592592592593 | global_recall: 0.9413886384129847 | global_auc: 0.97900655795775| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 47 | global_acc: 92.221% | global_loss: 0.27926287055015564 | global_f1: 0.8990509059534082 | global_precision: 0.8618693134822167 | global_recall: 0.939585211902615 | global_auc: 0.9784491006846656| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 48 | global_acc: 93.185% | global_loss: 0.2627854347229004 | global_f1: 0.9101271372205173 | global_precision: 0.8856655290102389 | global_recall: 0.9359783588818755 | global_auc: 0.9803911792595504| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 49 | global_acc: 93.617% | global_loss: 0.23111258447170258 | global_f1: 0.9151943462897527 | global_precision: 0.896969696969697 | global_recall: 0.9341749323715058 | global_auc: 0.9834980301435287| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 50 | global_acc: 93.850% | global_loss: 0.22321492433547974 | global_f1: 0.917741218319253 | global_precision: 0.9052631578947369 | global_recall: 0.9305680793507665 | global_auc: 0.9824809317798604| flobal_FPR: 0.06943192064923355 \n",
      "comm_round: 51 | global_acc: 94.149% | global_loss: 0.20205120742321014 | global_f1: 0.9207920792079207 | global_precision: 0.9191374663072777 | global_recall: 0.9224526600541028 | global_auc: 0.9850934785571258| flobal_FPR: 0.0775473399458972 \n",
      "comm_round: 52 | global_acc: 94.315% | global_loss: 0.18485134840011597 | global_f1: 0.9231460674157304 | global_precision: 0.9202508960573477 | global_recall: 0.9260595130748422 | global_auc: 0.9867986140491578| flobal_FPR: 0.0739404869251578 \n",
      "comm_round: 53 | global_acc: 94.382% | global_loss: 0.17710241675376892 | global_f1: 0.9237708615245828 | global_precision: 0.924187725631769 | global_recall: 0.9233543733092876 | global_auc: 0.9870407803262218| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 54 | global_acc: 94.415% | global_loss: 0.17348860204219818 | global_f1: 0.9239819004524887 | global_precision: 0.927338782924614 | global_recall: 0.9206492335437331 | global_auc: 0.986845147961221| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 55 | global_acc: 94.315% | global_loss: 0.17432750761508942 | global_f1: 0.9227293267058292 | global_precision: 0.9248188405797102 | global_recall: 0.9206492335437331 | global_auc: 0.9852093385014467| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 56 | global_acc: 94.415% | global_loss: 0.16566681861877441 | global_f1: 0.92520035618878 | global_precision: 0.9138082673702727 | global_recall: 0.9368800721370604 | global_auc: 0.9872202682727513| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 57 | global_acc: 94.348% | global_loss: 0.16472211480140686 | global_f1: 0.9239713774597497 | global_precision: 0.9165927240461402 | global_recall: 0.9314697926059513 | global_auc: 0.9865616709663051| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 58 | global_acc: 94.648% | global_loss: 0.15401822328567505 | global_f1: 0.9275753486279803 | global_precision: 0.9254937163375224 | global_recall: 0.9296663660955816 | global_auc: 0.9876841828858716| flobal_FPR: 0.0703336339044184 \n",
      "comm_round: 59 | global_acc: 95.013% | global_loss: 0.1491061896085739 | global_f1: 0.9337455830388693 | global_precision: 0.9151515151515152 | global_recall: 0.9531109107303878 | global_auc: 0.9890327166640313| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 60 | global_acc: 95.213% | global_loss: 0.14316412806510925 | global_f1: 0.9355416293643689 | global_precision: 0.9288888888888889 | global_recall: 0.9422903516681695 | global_auc: 0.9888646247775988| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 61 | global_acc: 95.512% | global_loss: 0.13465288281440735 | global_f1: 0.9398663697104677 | global_precision: 0.9286971830985915 | global_recall: 0.951307484220018 | global_auc: 0.990075456162918| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 62 | global_acc: 95.479% | global_loss: 0.13450488448143005 | global_f1: 0.9392314566577301 | global_precision: 0.9309123117803366 | global_recall: 0.9477006311992786 | global_auc: 0.989695587493014| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 63 | global_acc: 95.379% | global_loss: 0.12892231345176697 | global_f1: 0.9376401973979362 | global_precision: 0.9330357142857143 | global_recall: 0.9422903516681695 | global_auc: 0.9898793489620801| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 64 | global_acc: 95.678% | global_loss: 0.12276685237884521 | global_f1: 0.9413357400722021 | global_precision: 0.9421860885275519 | global_recall: 0.9404869251577999 | global_auc: 0.9903086005590717| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 65 | global_acc: 95.811% | global_loss: 0.12319499999284744 | global_f1: 0.942883046237534 | global_precision: 0.9480401093892434 | global_recall: 0.9377817853922452 | global_auc: 0.9899652942486459| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 66 | global_acc: 95.678% | global_loss: 0.12315394729375839 | global_f1: 0.941546762589928 | global_precision: 0.9390134529147982 | global_recall: 0.9440937781785392 | global_auc: 0.9903461125902248| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 67 | global_acc: 95.645% | global_loss: 0.12029752135276794 | global_f1: 0.9414917373827603 | global_precision: 0.9327433628318584 | global_recall: 0.9504057709648331 | global_auc: 0.9910949287057731| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 68 | global_acc: 95.844% | global_loss: 0.11576181650161743 | global_f1: 0.9436683190626407 | global_precision: 0.9432432432432433 | global_recall: 0.9440937781785392 | global_auc: 0.9910916048549115| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 69 | global_acc: 95.844% | global_loss: 0.1161004826426506 | global_f1: 0.9434133091896785 | global_precision: 0.9472727272727273 | global_recall: 0.939585211902615 | global_auc: 0.9907369024843886| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 70 | global_acc: 96.011% | global_loss: 0.11397453397512436 | global_f1: 0.9461400359066428 | global_precision: 0.9419124218051832 | global_recall: 0.9504057709648331 | global_auc: 0.9914762218831895| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 71 | global_acc: 96.144% | global_loss: 0.10919622331857681 | global_f1: 0.9476534296028881 | global_precision: 0.948509485094851 | global_recall: 0.9467989179440938 | global_auc: 0.991620571977753| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 72 | global_acc: 96.144% | global_loss: 0.10895495861768723 | global_f1: 0.9477477477477476 | global_precision: 0.946894689468947 | global_recall: 0.9486023444544635 | global_auc: 0.9917934122225593| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 73 | global_acc: 96.110% | global_loss: 0.10814591497182846 | global_f1: 0.9473210265646106 | global_precision: 0.9460431654676259 | global_recall: 0.9486023444544635 | global_auc: 0.9919363378096108| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 74 | global_acc: 95.977% | global_loss: 0.10914960503578186 | global_f1: 0.9455690508322087 | global_precision: 0.9434470377019749 | global_recall: 0.9477006311992786 | global_auc: 0.9917459286388214| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 75 | global_acc: 96.210% | global_loss: 0.10683643817901611 | global_f1: 0.9485559566787003 | global_precision: 0.9494128274616079 | global_recall: 0.9477006311992786 | global_auc: 0.9916841999799619| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 76 | global_acc: 96.177% | global_loss: 0.10564383864402771 | global_f1: 0.9479873360470376 | global_precision: 0.9509981851179673 | global_recall: 0.9449954914337241 | global_auc: 0.9917563750272437| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 77 | global_acc: 96.110% | global_loss: 0.1061762273311615 | global_f1: 0.9473684210526315 | global_precision: 0.9452423698384201 | global_recall: 0.9495040577096483 | global_auc: 0.992032729484599| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 78 | global_acc: 96.177% | global_loss: 0.10502580553293228 | global_f1: 0.9481748535376295 | global_precision: 0.9477477477477477 | global_recall: 0.9486023444544635 | global_auc: 0.9919871452442104| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 79 | global_acc: 96.110% | global_loss: 0.10515262931585312 | global_f1: 0.9474157303370787 | global_precision: 0.9444444444444444 | global_recall: 0.9504057709648331 | global_auc: 0.9921861014600727| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 80 | global_acc: 96.343% | global_loss: 0.102300725877285 | global_f1: 0.9503610108303249 | global_precision: 0.9512195121951219 | global_recall: 0.9495040577096483 | global_auc: 0.9922345347154855| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 81 | global_acc: 96.277% | global_loss: 0.10327625274658203 | global_f1: 0.9493212669683257 | global_precision: 0.9527702089009991 | global_recall: 0.9458972046889089 | global_auc: 0.9918432699854844| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 82 | global_acc: 96.343% | global_loss: 0.10194303840398788 | global_f1: 0.9502712477396021 | global_precision: 0.9528558476881233 | global_recall: 0.9477006311992786 | global_auc: 0.992218865132852| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 83 | global_acc: 96.310% | global_loss: 0.10139431059360504 | global_f1: 0.9497054825555054 | global_precision: 0.9544626593806922 | global_recall: 0.9449954914337241 | global_auc: 0.9920113618719167| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 84 | global_acc: 96.376% | global_loss: 0.09998641908168793 | global_f1: 0.9508344609833107 | global_precision: 0.9512635379061372 | global_recall: 0.9504057709648331 | global_auc: 0.9926239001021372| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 85 | global_acc: 96.376% | global_loss: 0.09915325790643692 | global_f1: 0.9507010402532791 | global_precision: 0.9537205081669692 | global_recall: 0.9477006311992786 | global_auc: 0.9924947447543698| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 86 | global_acc: 96.310% | global_loss: 0.10306753218173981 | global_f1: 0.9497964721845318 | global_precision: 0.9528130671506352 | global_recall: 0.9467989179440938 | global_auc: 0.9918276004028508| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 87 | global_acc: 96.343% | global_loss: 0.09931648522615433 | global_f1: 0.9503161698283648 | global_precision: 0.9520361990950226 | global_recall: 0.9486023444544635 | global_auc: 0.992622475594625| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 88 | global_acc: 96.210% | global_loss: 0.10241779685020447 | global_f1: 0.9485559566787003 | global_precision: 0.9494128274616079 | global_recall: 0.9477006311992786 | global_auc: 0.9921068038752302| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 89 | global_acc: 96.210% | global_loss: 0.1020251214504242 | global_f1: 0.9486948694869486 | global_precision: 0.9469901168014375 | global_recall: 0.9504057709648331 | global_auc: 0.992402626601918| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 90 | global_acc: 96.343% | global_loss: 0.09858067333698273 | global_f1: 0.9504950495049506 | global_precision: 0.9487870619946092 | global_recall: 0.9522091974752029 | global_auc: 0.9930626484158764| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 91 | global_acc: 96.243% | global_loss: 0.10210093855857849 | global_f1: 0.9493500672344239 | global_precision: 0.9438502673796791 | global_recall: 0.9549143372407575 | global_auc: 0.9925165872028893| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 92 | global_acc: 96.376% | global_loss: 0.09666970372200012 | global_f1: 0.9507455942159965 | global_precision: 0.9528985507246377 | global_recall: 0.9486023444544635 | global_auc: 0.9929225718438492| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 93 | global_acc: 96.410% | global_loss: 0.0999358594417572 | global_f1: 0.9514388489208634 | global_precision: 0.9488789237668162 | global_recall: 0.9540126239855726 | global_auc: 0.9927582786441156| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 94 | global_acc: 96.476% | global_loss: 0.09876205772161484 | global_f1: 0.9521660649819494 | global_precision: 0.953026196928636 | global_recall: 0.951307484220018 | global_auc: 0.992747357419856| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 95 | global_acc: 96.144% | global_loss: 0.10271096974611282 | global_f1: 0.9478885893980232 | global_precision: 0.9444941808415398 | global_recall: 0.951307484220018 | global_auc: 0.9921400423838469| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 96 | global_acc: 96.210% | global_loss: 0.10284238308668137 | global_f1: 0.9490161001788909 | global_precision: 0.9414374445430346 | global_recall: 0.9567177637511272 | global_auc: 0.992517536874564| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 97 | global_acc: 96.410% | global_loss: 0.09860646724700928 | global_f1: 0.9514388489208634 | global_precision: 0.9488789237668162 | global_recall: 0.9540126239855726 | global_auc: 0.9929843005027087| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 98 | global_acc: 96.509% | global_loss: 0.09431888908147812 | global_f1: 0.9525101763907734 | global_precision: 0.9555353901996371 | global_recall: 0.9495040577096483 | global_auc: 0.9932611297959013| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 99 | global_acc: 96.576% | global_loss: 0.09522095322608948 | global_f1: 0.9534988713318284 | global_precision: 0.9547920433996383 | global_recall: 0.9522091974752029 | global_auc: 0.9933124120663384| flobal_FPR: 0.047790802524797116 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx-non-iid-FGSM  2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model_prox, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-non-iid-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-non-iid-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-non-iid-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d522b1",
   "metadata": {},
   "source": [
    "## iid FedProx FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32db8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 49.435% | global_loss: 0.691255509853363 | global_f1: 0.516989520482693 | global_precision: 0.3990196078431373 | global_recall: 0.7339945897204689 | global_auc: 0.5922399478440317| flobal_FPR: 0.2660054102795311 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.681009829044342 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6409804220435891| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6728878617286682 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6779525648495175| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6653454303741455 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7147461693805909| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6581996083259583 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7503925705285541| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6525289416313171 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7779985764421595| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6481643319129944 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7987403554905981| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6431032419204712 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8191939091857467| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6416483521461487 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.830105636728742| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.6390201449394226 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8422645205986161| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.6375678777694702 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8504920011529015| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.6349199414253235 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8587427486632184| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.628688395023346 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8708864377862962| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.6248764395713806 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8802036665873691| flobal_FPR: 1.0 \n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.6209361553192139 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8881709371027702| flobal_FPR: 1.0 \n",
      "comm_round: 15 | global_acc: 63.132% | global_loss: 0.6130717396736145 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8972830368220948| flobal_FPR: 1.0 \n",
      "comm_round: 16 | global_acc: 63.132% | global_loss: 0.6107770800590515 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9036558085955734| flobal_FPR: 1.0 \n",
      "comm_round: 17 | global_acc: 63.132% | global_loss: 0.6080934405326843 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9095271537247785| flobal_FPR: 1.0 \n",
      "comm_round: 18 | global_acc: 63.132% | global_loss: 0.6071482300758362 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9140580372850596| flobal_FPR: 1.0 \n",
      "comm_round: 19 | global_acc: 63.132% | global_loss: 0.6057308912277222 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9183973245849579| flobal_FPR: 1.0 \n",
      "comm_round: 20 | global_acc: 63.132% | global_loss: 0.6036766767501831 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9212320945341173| flobal_FPR: 1.0 \n",
      "comm_round: 21 | global_acc: 63.132% | global_loss: 0.599177360534668 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9247931258965495| flobal_FPR: 1.0 \n",
      "comm_round: 22 | global_acc: 63.132% | global_loss: 0.5919076800346375 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9298287599519657| flobal_FPR: 1.0 \n",
      "comm_round: 23 | global_acc: 63.132% | global_loss: 0.5869433879852295 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9334503328836639| flobal_FPR: 1.0 \n",
      "comm_round: 24 | global_acc: 63.132% | global_loss: 0.5817763805389404 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9371174900557505| flobal_FPR: 1.0 \n",
      "comm_round: 25 | global_acc: 63.132% | global_loss: 0.5696317553520203 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9417048790806799| flobal_FPR: 1.0 \n",
      "comm_round: 26 | global_acc: 63.132% | global_loss: 0.5531511306762695 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9470064212050289| flobal_FPR: 1.0 \n",
      "comm_round: 27 | global_acc: 63.132% | global_loss: 0.5440157055854797 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9513003616824575| flobal_FPR: 1.0 \n",
      "comm_round: 28 | global_acc: 63.132% | global_loss: 0.5254508256912231 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9573151072345514| flobal_FPR: 1.0 \n",
      "comm_round: 29 | global_acc: 63.132% | global_loss: 0.5159928202629089 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9614347829596612| flobal_FPR: 1.0 \n",
      "comm_round: 30 | global_acc: 63.132% | global_loss: 0.5099717378616333 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9643652323300528| flobal_FPR: 1.0 \n",
      "comm_round: 31 | global_acc: 65.359% | global_loss: 0.4736695885658264 | global_f1: 0.11394557823129252 | global_precision: 1.0 | global_recall: 0.060414788097385035 | global_auc: 0.970192180308463| flobal_FPR: 0.939585211902615 \n",
      "comm_round: 32 | global_acc: 75.665% | global_loss: 0.4589925706386566 | global_f1: 0.5074024226110364 | global_precision: 1.0 | global_recall: 0.33994589720468893 | global_auc: 0.9736983681316778| flobal_FPR: 0.6600541027953111 \n",
      "comm_round: 33 | global_acc: 80.718% | global_loss: 0.4378703534603119 | global_f1: 0.645909645909646 | global_precision: 1.0 | global_recall: 0.4770063119927863 | global_auc: 0.9767961971347456| flobal_FPR: 0.5229936880072137 \n",
      "comm_round: 34 | global_acc: 85.040% | global_loss: 0.43304845690727234 | global_f1: 0.7463359639233371 | global_precision: 0.9954887218045113 | global_recall: 0.5969341749323716 | global_auc: 0.9788883238342424| flobal_FPR: 0.4030658250676285 \n",
      "comm_round: 35 | global_acc: 87.666% | global_loss: 0.4204477369785309 | global_f1: 0.8010723860589812 | global_precision: 0.9880952380952381 | global_recall: 0.6735798016230838 | global_auc: 0.9803693368110311| flobal_FPR: 0.32642019837691616 \n",
      "comm_round: 36 | global_acc: 89.761% | global_loss: 0.40711709856987 | global_f1: 0.8402489626556017 | global_precision: 0.989010989010989 | global_recall: 0.7303877366997295 | global_auc: 0.9817705773671398| flobal_FPR: 0.2696122633002705 \n",
      "comm_round: 37 | global_acc: 90.824% | global_loss: 0.38331401348114014 | global_f1: 0.8598984771573603 | global_precision: 0.983739837398374 | global_recall: 0.763751127141569 | global_auc: 0.9830768507557726| flobal_FPR: 0.23624887285843102 \n",
      "comm_round: 38 | global_acc: 92.188% | global_loss: 0.3762126564979553 | global_f1: 0.8834903321764999 | global_precision: 0.9812775330396476 | global_recall: 0.8034265103697025 | global_auc: 0.983613415252012| flobal_FPR: 0.19657348963029755 \n",
      "comm_round: 39 | global_acc: 92.719% | global_loss: 0.37316790223121643 | global_f1: 0.8926996570308672 | global_precision: 0.9774678111587983 | global_recall: 0.8214607754733995 | global_auc: 0.9835236712787472| flobal_FPR: 0.17853922452660054 \n",
      "comm_round: 40 | global_acc: 93.185% | global_loss: 0.35195252299308777 | global_f1: 0.9003403014098201 | global_precision: 0.9767932489451476 | global_recall: 0.8349864743011722 | global_auc: 0.9841874917794046| flobal_FPR: 0.16501352569882777 \n",
      "comm_round: 41 | global_acc: 94.082% | global_loss: 0.32718342542648315 | global_f1: 0.9145873320537429 | global_precision: 0.9774358974358974 | global_recall: 0.8593327321911632 | global_auc: 0.9850032597480237| flobal_FPR: 0.1406672678088368 \n",
      "comm_round: 42 | global_acc: 94.382% | global_loss: 0.31968143582344055 | global_f1: 0.9195621132793906 | global_precision: 0.9737903225806451 | global_recall: 0.8710550045085663 | global_auc: 0.9850526426751112| flobal_FPR: 0.12894499549143373 \n",
      "comm_round: 43 | global_acc: 94.515% | global_loss: 0.3012141287326813 | global_f1: 0.9219858156028368 | global_precision: 0.9691848906560636 | global_recall: 0.8791704238052299 | global_auc: 0.9854491305993236| flobal_FPR: 0.12082957619477007 \n",
      "comm_round: 44 | global_acc: 94.515% | global_loss: 0.29359251260757446 | global_f1: 0.9224259520451339 | global_precision: 0.9636542239685658 | global_recall: 0.8845807033363391 | global_auc: 0.9852610956077211| flobal_FPR: 0.11541929666366095 \n",
      "comm_round: 45 | global_acc: 94.481% | global_loss: 0.2860051989555359 | global_f1: 0.9222846441947566 | global_precision: 0.9591041869522883 | global_recall: 0.8881875563570785 | global_auc: 0.9850725857802811| flobal_FPR: 0.11181244364292155 \n",
      "comm_round: 46 | global_acc: 94.614% | global_loss: 0.2618080973625183 | global_f1: 0.9242282507015903 | global_precision: 0.9601554907677357 | global_recall: 0.890892696122633 | global_auc: 0.9859339379892887| flobal_FPR: 0.109107303877367 \n",
      "comm_round: 47 | global_acc: 94.880% | global_loss: 0.24549293518066406 | global_f1: 0.9283054003724396 | global_precision: 0.9595765158806545 | global_recall: 0.8990081154192967 | global_auc: 0.9865270079501764| flobal_FPR: 0.10099188458070334 \n",
      "comm_round: 48 | global_acc: 94.814% | global_loss: 0.24033789336681366 | global_f1: 0.9275092936802974 | global_precision: 0.9568552253116012 | global_recall: 0.8999098286744815 | global_auc: 0.9861685068929543| flobal_FPR: 0.10009017132551848 \n",
      "comm_round: 49 | global_acc: 94.714% | global_loss: 0.23415814340114594 | global_f1: 0.9263547938860583 | global_precision: 0.9523809523809523 | global_recall: 0.9017132551848512 | global_auc: 0.9860146600816434| flobal_FPR: 0.09828674481514878 \n",
      "comm_round: 50 | global_acc: 95.146% | global_loss: 0.2090461552143097 | global_f1: 0.9324074074074074 | global_precision: 0.9581351094196003 | global_recall: 0.9080252479711451 | global_auc: 0.9876267277495487| flobal_FPR: 0.09197475202885483 \n",
      "comm_round: 51 | global_acc: 95.346% | global_loss: 0.19332769513130188 | global_f1: 0.9354838709677419 | global_precision: 0.9566446748350612 | global_recall: 0.915238954012624 | global_auc: 0.9883845657460075| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 52 | global_acc: 95.412% | global_loss: 0.190802201628685 | global_f1: 0.9365808823529412 | global_precision: 0.9550140581068416 | global_recall: 0.9188458070333634 | global_auc: 0.9879078305652779| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 53 | global_acc: 95.379% | global_loss: 0.17612777650356293 | global_f1: 0.9362092703074805 | global_precision: 0.9532710280373832 | global_recall: 0.9197475202885482 | global_auc: 0.9889121083613369| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 54 | global_acc: 95.844% | global_loss: 0.16723841428756714 | global_f1: 0.9429484253765402 | global_precision: 0.9547134935304991 | global_recall: 0.9314697926059513 | global_auc: 0.9893674759293845| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 55 | global_acc: 95.944% | global_loss: 0.162898987531662 | global_f1: 0.9443938012762079 | global_precision: 0.9548387096774194 | global_recall: 0.9341749323715058 | global_auc: 0.9893627275710105| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 56 | global_acc: 96.110% | global_loss: 0.15152233839035034 | global_f1: 0.9466484268125854 | global_precision: 0.9575645756457565 | global_recall: 0.9359783588818755 | global_auc: 0.9900844780438285| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 57 | global_acc: 96.044% | global_loss: 0.1451900154352188 | global_f1: 0.945736434108527 | global_precision: 0.9566420664206642 | global_recall: 0.9350766456266907 | global_auc: 0.9903560841428098| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 58 | global_acc: 96.110% | global_loss: 0.13956992328166962 | global_f1: 0.9466970387243736 | global_precision: 0.9567219152854513 | global_recall: 0.9368800721370604 | global_auc: 0.9906718499746675| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 59 | global_acc: 96.243% | global_loss: 0.14033952355384827 | global_f1: 0.9486130059117781 | global_precision: 0.9568807339449541 | global_recall: 0.9404869251577999 | global_auc: 0.9903347165301276| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 60 | global_acc: 96.277% | global_loss: 0.13597331941127777 | global_f1: 0.9491371480472298 | global_precision: 0.9560841720036597 | global_recall: 0.9422903516681695 | global_auc: 0.9906172438533688| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 61 | global_acc: 96.410% | global_loss: 0.13868197798728943 | global_f1: 0.951131221719457 | global_precision: 0.9545867393278837 | global_recall: 0.9477006311992786 | global_auc: 0.9902544692736104| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 62 | global_acc: 96.443% | global_loss: 0.12996861338615417 | global_f1: 0.95151789759855 | global_precision: 0.9562841530054644 | global_recall: 0.9467989179440938 | global_auc: 0.9910493444653848| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 63 | global_acc: 96.476% | global_loss: 0.12950332462787628 | global_f1: 0.9519927536231885 | global_precision: 0.9563239308462238 | global_recall: 0.9477006311992786 | global_auc: 0.9908256967859788| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 64 | global_acc: 96.709% | global_loss: 0.1223750039935112 | global_f1: 0.9551833408782253 | global_precision: 0.9590909090909091 | global_recall: 0.951307484220018 | global_auc: 0.9914059461792571| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 65 | global_acc: 96.809% | global_loss: 0.11889654397964478 | global_f1: 0.9564032697547684 | global_precision: 0.9634034766697164 | global_recall: 0.9495040577096483 | global_auc: 0.9915488717663086| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 66 | global_acc: 96.875% | global_loss: 0.11649282276630402 | global_f1: 0.9572727272727274 | global_precision: 0.9651695692025665 | global_recall: 0.9495040577096483 | global_auc: 0.9916096507534933| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 67 | global_acc: 96.875% | global_loss: 0.11345136165618896 | global_f1: 0.9572727272727274 | global_precision: 0.9651695692025665 | global_recall: 0.9495040577096483 | global_auc: 0.9918456441646711| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 68 | global_acc: 96.875% | global_loss: 0.11272470653057098 | global_f1: 0.9572727272727274 | global_precision: 0.9651695692025665 | global_recall: 0.9495040577096483 | global_auc: 0.9917962612375837| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 69 | global_acc: 96.842% | global_loss: 0.11405705660581589 | global_f1: 0.9569160997732425 | global_precision: 0.9625912408759124 | global_recall: 0.951307484220018 | global_auc: 0.9915840096182748| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 70 | global_acc: 96.875% | global_loss: 0.11109279096126556 | global_f1: 0.9572338489535941 | global_precision: 0.9660238751147842 | global_recall: 0.9486023444544635 | global_auc: 0.9919111715102297| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 71 | global_acc: 96.842% | global_loss: 0.11547517031431198 | global_f1: 0.9567592171142466 | global_precision: 0.9659926470588235 | global_recall: 0.9477006311992786 | global_auc: 0.991258272233832| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 72 | global_acc: 96.975% | global_loss: 0.10955008864402771 | global_f1: 0.9585421412300684 | global_precision: 0.9686924493554327 | global_recall: 0.9486023444544635 | global_auc: 0.991775368460739| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 73 | global_acc: 97.074% | global_loss: 0.10294997692108154 | global_f1: 0.959890610756609 | global_precision: 0.9705069124423963 | global_recall: 0.9495040577096483 | global_auc: 0.9925246594121249| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 74 | global_acc: 97.108% | global_loss: 0.0989486500620842 | global_f1: 0.9604005461993628 | global_precision: 0.9696691176470589 | global_recall: 0.951307484220018 | global_auc: 0.9929159241421259| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 75 | global_acc: 97.074% | global_loss: 0.09904072433710098 | global_f1: 0.9600000000000001 | global_precision: 0.9679193400549955 | global_recall: 0.9522091974752029 | global_auc: 0.9927008235077929| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 76 | global_acc: 97.108% | global_loss: 0.09732688963413239 | global_f1: 0.9604365620736699 | global_precision: 0.9688073394495413 | global_recall: 0.9522091974752029 | global_auc: 0.9929168738138007| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 77 | global_acc: 97.041% | global_loss: 0.09589666873216629 | global_f1: 0.9595270577535243 | global_precision: 0.9678899082568807 | global_recall: 0.951307484220018 | global_auc: 0.9930583748933399| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 78 | global_acc: 97.074% | global_loss: 0.09532039612531662 | global_f1: 0.9599271402550091 | global_precision: 0.9696412143514259 | global_recall: 0.9504057709648331 | global_auc: 0.9930403311315197| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 79 | global_acc: 97.141% | global_loss: 0.09583450853824615 | global_f1: 0.960730593607306 | global_precision: 0.973172987974098 | global_recall: 0.9486023444544635 | global_auc: 0.9930768934909979| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 80 | global_acc: 97.207% | global_loss: 0.09273018687963486 | global_f1: 0.9617137648131266 | global_precision: 0.9723502304147466 | global_recall: 0.951307484220018 | global_auc: 0.993264453646763| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 81 | global_acc: 97.241% | global_loss: 0.09424956142902374 | global_f1: 0.9621523027815777 | global_precision: 0.9732472324723247 | global_recall: 0.951307484220018 | global_auc: 0.9930379569523328| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 82 | global_acc: 97.274% | global_loss: 0.09023253619670868 | global_f1: 0.9626593806921676 | global_precision: 0.9724011039558418 | global_recall: 0.9531109107303878 | global_auc: 0.9934524886383655| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 83 | global_acc: 97.241% | global_loss: 0.08804085105657578 | global_f1: 0.9622555707139608 | global_precision: 0.9706422018348624 | global_recall: 0.9540126239855726 | global_auc: 0.9937307424390702| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 84 | global_acc: 97.374% | global_loss: 0.0869828462600708 | global_f1: 0.9640418752844789 | global_precision: 0.9733455882352942 | global_recall: 0.9549143372407575 | global_auc: 0.9938014929788398| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 85 | global_acc: 97.407% | global_loss: 0.08649701625108719 | global_f1: 0.9645131938125568 | global_precision: 0.9733700642791552 | global_recall: 0.9558160504959423 | global_auc: 0.9938038671580269| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 86 | global_acc: 97.374% | global_loss: 0.08553869277238846 | global_f1: 0.9639762881896945 | global_precision: 0.9750922509225092 | global_recall: 0.9531109107303878 | global_auc: 0.9938774667128207| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 87 | global_acc: 97.407% | global_loss: 0.08603878319263458 | global_f1: 0.9643835616438357 | global_precision: 0.9768732654949122 | global_recall: 0.9522091974752029 | global_auc: 0.9938432785325293| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 88 | global_acc: 97.440% | global_loss: 0.0843452662229538 | global_f1: 0.9649202733485194 | global_precision: 0.9751381215469613 | global_recall: 0.9549143372407575 | global_auc: 0.9940227664790591| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 89 | global_acc: 97.440% | global_loss: 0.08342295140028 | global_f1: 0.9648882808937529 | global_precision: 0.9760147601476015 | global_recall: 0.9540126239855726 | global_auc: 0.9939771822386705| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 90 | global_acc: 97.507% | global_loss: 0.08187272399663925 | global_f1: 0.9658314350797266 | global_precision: 0.9760589318600368 | global_recall: 0.9558160504959423 | global_auc: 0.9942022544255886| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 91 | global_acc: 97.507% | global_loss: 0.08069554716348648 | global_f1: 0.9658314350797266 | global_precision: 0.9760589318600368 | global_recall: 0.9558160504959423 | global_auc: 0.9943489786993392| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 92 | global_acc: 97.473% | global_loss: 0.08062102645635605 | global_f1: 0.9653600729261622 | global_precision: 0.976036866359447 | global_recall: 0.9549143372407575 | global_auc: 0.9943708211478587| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 93 | global_acc: 97.573% | global_loss: 0.07853316515684128 | global_f1: 0.966742596810934 | global_precision: 0.9769797421731123 | global_recall: 0.9567177637511272 | global_auc: 0.9945878211255414| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 94 | global_acc: 97.573% | global_loss: 0.07958683371543884 | global_f1: 0.9667122663018696 | global_precision: 0.977859778597786 | global_recall: 0.9558160504959423 | global_auc: 0.9945374885267793| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 95 | global_acc: 97.573% | global_loss: 0.07990200817584991 | global_f1: 0.9667122663018696 | global_precision: 0.977859778597786 | global_recall: 0.9558160504959423 | global_auc: 0.9944961778089271| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 96 | global_acc: 97.473% | global_loss: 0.08004846423864365 | global_f1: 0.9653284671532847 | global_precision: 0.976915974145891 | global_recall: 0.9540126239855726 | global_auc: 0.994480983062131| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 97 | global_acc: 97.540% | global_loss: 0.07932174205780029 | global_f1: 0.9662716499544212 | global_precision: 0.9769585253456221 | global_recall: 0.9558160504959423 | global_auc: 0.9945351143475921| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 98 | global_acc: 97.540% | global_loss: 0.07943262159824371 | global_f1: 0.9662716499544212 | global_precision: 0.9769585253456221 | global_recall: 0.9558160504959423 | global_auc: 0.9944876307638542| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 99 | global_acc: 97.540% | global_loss: 0.07832355797290802 | global_f1: 0.9662716499544212 | global_precision: 0.9769585253456221 | global_recall: 0.9558160504959423 | global_auc: 0.9945360640192669| flobal_FPR: 0.04418394950405771 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model_prox, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5238cac1",
   "metadata": {},
   "source": [
    "## iid FedProx PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0127a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6734648942947388 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.5928788394632266| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6655853986740112 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6701536236384675| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6585378646850586 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7176175016892286| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6522354483604431 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7443339026615023| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6454789042472839 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.769680877078772| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6376706957817078 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7911831057207747| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6287283301353455 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.810609826917589| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6175970435142517 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.82800899908879| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6042128801345825 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.843012149624571| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5884413123130798 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8595943667375597| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.5708789229393005 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8775464852413899| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.5511670708656311 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8947906235116865| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.5298860669136047 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9121116851876385| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.5099989175796509 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9283354012434053| flobal_FPR: 1.0 \n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.48683372139930725 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9432177060585728| flobal_FPR: 1.0 \n",
      "comm_round: 15 | global_acc: 65.658% | global_loss: 0.46601420640945435 | global_f1: 0.12827004219409285 | global_precision: 1.0 | global_recall: 0.06853020739404869 | global_auc: 0.9549129127332452| flobal_FPR: 0.9314697926059513 \n",
      "comm_round: 16 | global_acc: 80.984% | global_loss: 0.44757553935050964 | global_f1: 0.6529126213592233 | global_precision: 0.9981447124304267 | global_recall: 0.48512173128944996 | global_auc: 0.9633146580398493| flobal_FPR: 0.5148782687105501 \n",
      "comm_round: 17 | global_acc: 90.991% | global_loss: 0.4283958673477173 | global_f1: 0.8632004038364461 | global_precision: 0.9805045871559633 | global_recall: 0.7709648331830478 | global_auc: 0.9696062328851359| flobal_FPR: 0.2290351668169522 \n",
      "comm_round: 18 | global_acc: 93.517% | global_loss: 0.405525803565979 | global_f1: 0.9067431850789096 | global_precision: 0.9653767820773931 | global_recall: 0.8548241659152389 | global_auc: 0.974225910747007| flobal_FPR: 0.14517583408476104 \n",
      "comm_round: 19 | global_acc: 94.016% | global_loss: 0.3812655508518219 | global_f1: 0.9146919431279621 | global_precision: 0.964035964035964 | global_recall: 0.8701532912533815 | global_auc: 0.9778408359769818| flobal_FPR: 0.12984670874661858 \n",
      "comm_round: 20 | global_acc: 94.614% | global_loss: 0.35786765813827515 | global_f1: 0.9243697478991596 | global_precision: 0.9583736689254598 | global_recall: 0.8926961226330027 | global_auc: 0.98065043962676| flobal_FPR: 0.1073038773669973 \n",
      "comm_round: 21 | global_acc: 94.747% | global_loss: 0.33669009804725647 | global_f1: 0.9270544783010157 | global_precision: 0.9498580889309366 | global_recall: 0.9053201082055906 | global_auc: 0.9824087567325787| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 22 | global_acc: 94.980% | global_loss: 0.31215542554855347 | global_f1: 0.9304467987102718 | global_precision: 0.9510357815442562 | global_recall: 0.9107303877366997 | global_auc: 0.983647128596466| flobal_FPR: 0.08926961226330027 \n",
      "comm_round: 23 | global_acc: 94.914% | global_loss: 0.2895682156085968 | global_f1: 0.9294605809128631 | global_precision: 0.9509433962264151 | global_recall: 0.90892696122633 | global_auc: 0.9846058221521364| flobal_FPR: 0.09107303877366997 \n",
      "comm_round: 24 | global_acc: 94.880% | global_loss: 0.2660442590713501 | global_f1: 0.9290322580645162 | global_precision: 0.9500471253534402 | global_recall: 0.90892696122633 | global_auc: 0.9853916754630004| flobal_FPR: 0.09107303877366997 \n",
      "comm_round: 25 | global_acc: 95.080% | global_loss: 0.24459105730056763 | global_f1: 0.9317343173431735 | global_precision: 0.9537299338999056 | global_recall: 0.9107303877366997 | global_auc: 0.9858726841662665| flobal_FPR: 0.08926961226330027 \n",
      "comm_round: 26 | global_acc: 95.445% | global_loss: 0.22022804617881775 | global_f1: 0.9368954398894519 | global_precision: 0.9576271186440678 | global_recall: 0.9170423805229937 | global_auc: 0.9866818044331624| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 27 | global_acc: 95.412% | global_loss: 0.20363488793373108 | global_f1: 0.9365808823529412 | global_precision: 0.9550140581068416 | global_recall: 0.9188458070333634 | global_auc: 0.9870531260579936| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 28 | global_acc: 95.512% | global_loss: 0.18710894882678986 | global_f1: 0.9379880569591179 | global_precision: 0.9559925093632958 | global_recall: 0.9206492335437331 | global_auc: 0.987649519869743| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 29 | global_acc: 95.479% | global_loss: 0.17511731386184692 | global_f1: 0.9375573921028466 | global_precision: 0.9550982226379794 | global_recall: 0.9206492335437331 | global_auc: 0.9880778217950599| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 30 | global_acc: 95.977% | global_loss: 0.16191747784614563 | global_f1: 0.944774075764491 | global_precision: 0.9565619223659889 | global_recall: 0.933273219116321 | global_auc: 0.9887406926240425| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 31 | global_acc: 95.977% | global_loss: 0.15128786861896515 | global_f1: 0.9448244414044688 | global_precision: 0.955719557195572 | global_recall: 0.9341749323715058 | global_auc: 0.9893157188231099| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 32 | global_acc: 95.911% | global_loss: 0.1428534835577011 | global_f1: 0.9439124487004104 | global_precision: 0.9547970479704797 | global_recall: 0.933273219116321 | global_auc: 0.9898209441540824| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 33 | global_acc: 95.911% | global_loss: 0.13552594184875488 | global_f1: 0.9438099588853358 | global_precision: 0.9564814814814815 | global_recall: 0.9314697926059513 | global_auc: 0.9905056574315845| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 34 | global_acc: 95.977% | global_loss: 0.1286591738462448 | global_f1: 0.944774075764491 | global_precision: 0.9565619223659889 | global_recall: 0.933273219116321 | global_auc: 0.9910555173312707| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 35 | global_acc: 96.110% | global_loss: 0.12281830608844757 | global_f1: 0.9465020576131689 | global_precision: 0.9601113172541744 | global_recall: 0.933273219116321 | global_auc: 0.9915327273478376| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 36 | global_acc: 96.144% | global_loss: 0.11717436462640762 | global_f1: 0.9471285323609845 | global_precision: 0.9576036866359448 | global_recall: 0.9368800721370604 | global_auc: 0.9920341539921113| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 37 | global_acc: 96.177% | global_loss: 0.11367134004831314 | global_f1: 0.9475125513464171 | global_precision: 0.9593345656192237 | global_recall: 0.9359783588818755 | global_auc: 0.9921514384439439| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 38 | global_acc: 96.443% | global_loss: 0.10925674438476562 | global_f1: 0.9513415188722146 | global_precision: 0.9596330275229358 | global_recall: 0.9431920649233544 | global_auc: 0.9925844887276346| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 39 | global_acc: 96.443% | global_loss: 0.10640949755907059 | global_f1: 0.9512972234865726 | global_precision: 0.9604779411764706 | global_recall: 0.9422903516681695 | global_auc: 0.9927340620164095| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 40 | global_acc: 96.543% | global_loss: 0.10331189632415771 | global_f1: 0.9527702089009991 | global_precision: 0.959743824336688 | global_recall: 0.9458972046889089 | global_auc: 0.9929676812484004| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 41 | global_acc: 96.676% | global_loss: 0.10101562738418579 | global_f1: 0.9545040946314832 | global_precision: 0.9632690541781451 | global_recall: 0.9458972046889089 | global_auc: 0.9931272260897601| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 42 | global_acc: 96.676% | global_loss: 0.09913548082113266 | global_f1: 0.9545040946314832 | global_precision: 0.9632690541781451 | global_recall: 0.9458972046889089 | global_auc: 0.993244510541593| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 43 | global_acc: 96.676% | global_loss: 0.09715128690004349 | global_f1: 0.9545040946314832 | global_precision: 0.9632690541781451 | global_recall: 0.9458972046889089 | global_auc: 0.9934111779205135| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 44 | global_acc: 96.742% | global_loss: 0.09565334767103195 | global_f1: 0.9554140127388536 | global_precision: 0.9641873278236914 | global_recall: 0.9467989179440938 | global_auc: 0.9935218146706231| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 45 | global_acc: 96.742% | global_loss: 0.09484484046697617 | global_f1: 0.9554140127388536 | global_precision: 0.9641873278236914 | global_recall: 0.9467989179440938 | global_auc: 0.9935621757168003| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 46 | global_acc: 96.709% | global_loss: 0.09430976957082748 | global_f1: 0.9549385525716887 | global_precision: 0.9641544117647058 | global_recall: 0.9458972046889089 | global_auc: 0.9936058606138393| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 47 | global_acc: 96.775% | global_loss: 0.09286586940288544 | global_f1: 0.9558890404729422 | global_precision: 0.9642201834862385 | global_recall: 0.9477006311992786 | global_auc: 0.9937644557835242| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 48 | global_acc: 96.809% | global_loss: 0.09216369688510895 | global_f1: 0.9562841530054644 | global_precision: 0.9659613615455381 | global_recall: 0.9467989179440938 | global_auc: 0.9938280837857331| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 49 | global_acc: 96.875% | global_loss: 0.09098885953426361 | global_f1: 0.9572338489535941 | global_precision: 0.9660238751147842 | global_recall: 0.9486023444544635 | global_auc: 0.9938893376087552| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 50 | global_acc: 96.908% | global_loss: 0.08960384130477905 | global_f1: 0.9576695493855257 | global_precision: 0.9669117647058824 | global_recall: 0.9486023444544635 | global_auc: 0.9940403354050422| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 51 | global_acc: 96.908% | global_loss: 0.09024651348590851 | global_f1: 0.9577080491132333 | global_precision: 0.9660550458715597 | global_recall: 0.9495040577096483 | global_auc: 0.9939249502965587| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 52 | global_acc: 96.908% | global_loss: 0.08865675330162048 | global_f1: 0.9577464788732394 | global_precision: 0.9652014652014652 | global_recall: 0.9504057709648331 | global_auc: 0.9941153594673482| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 53 | global_acc: 97.041% | global_loss: 0.08759389817714691 | global_f1: 0.9596005447117567 | global_precision: 0.9661791590493601 | global_recall: 0.9531109107303878 | global_auc: 0.9942259962174578| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 54 | global_acc: 97.074% | global_loss: 0.08649826049804688 | global_f1: 0.9600000000000001 | global_precision: 0.9679193400549955 | global_recall: 0.9522091974752029 | global_auc: 0.9943195388774215| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 55 | global_acc: 97.074% | global_loss: 0.08606601506471634 | global_f1: 0.9600000000000001 | global_precision: 0.9679193400549955 | global_recall: 0.9522091974752029 | global_auc: 0.9943727204912083| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 56 | global_acc: 97.074% | global_loss: 0.08596592396497726 | global_f1: 0.9600000000000001 | global_precision: 0.9679193400549955 | global_recall: 0.9522091974752029 | global_auc: 0.9943845913871426| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 57 | global_acc: 97.108% | global_loss: 0.08492422848939896 | global_f1: 0.9604725124943208 | global_precision: 0.967948717948718 | global_recall: 0.9531109107303878 | global_auc: 0.9945360640192669| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 58 | global_acc: 97.174% | global_loss: 0.08404438197612762 | global_f1: 0.9614162505674082 | global_precision: 0.9680073126142597 | global_recall: 0.9549143372407575 | global_auc: 0.9945968430064516| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 59 | global_acc: 97.274% | global_loss: 0.08328212797641754 | global_f1: 0.9628286491387126 | global_precision: 0.968094804010939 | global_recall: 0.957619477006312 | global_auc: 0.994698457875651| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 60 | global_acc: 97.274% | global_loss: 0.0829704999923706 | global_f1: 0.9628286491387126 | global_precision: 0.968094804010939 | global_recall: 0.957619477006312 | global_auc: 0.9946975082039762| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 61 | global_acc: 97.241% | global_loss: 0.08239592611789703 | global_f1: 0.9623241034952338 | global_precision: 0.9689213893967094 | global_recall: 0.9558160504959423 | global_auc: 0.9947616110420225| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 62 | global_acc: 97.274% | global_loss: 0.08153495192527771 | global_f1: 0.9627949183303085 | global_precision: 0.9689497716894977 | global_recall: 0.9567177637511272 | global_auc: 0.9948480311644258| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 63 | global_acc: 97.340% | global_loss: 0.081013023853302 | global_f1: 0.9637352674524025 | global_precision: 0.9690063810391978 | global_recall: 0.9585211902614968 | global_auc: 0.9948836438522293| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 64 | global_acc: 97.340% | global_loss: 0.08093990385532379 | global_f1: 0.9637352674524025 | global_precision: 0.9690063810391978 | global_recall: 0.9585211902614968 | global_auc: 0.9949002631065375| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 65 | global_acc: 97.407% | global_loss: 0.08022191375494003 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9950014031398994| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 66 | global_acc: 97.407% | global_loss: 0.07985907047986984 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9950389151710524| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 67 | global_acc: 97.374% | global_loss: 0.07886006683111191 | global_f1: 0.964204802899864 | global_precision: 0.9690346083788707 | global_recall: 0.9594229035166817 | global_auc: 0.995145753234463| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 68 | global_acc: 97.407% | global_loss: 0.07896154373884201 | global_f1: 0.9646418857660924 | global_precision: 0.9699179580674567 | global_recall: 0.9594229035166817 | global_auc: 0.9951224862784314| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 69 | global_acc: 97.440% | global_loss: 0.07839062809944153 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9951856394448029| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 70 | global_acc: 97.507% | global_loss: 0.07761666178703308 | global_f1: 0.9659863945578231 | global_precision: 0.9717153284671532 | global_recall: 0.9603246167718665 | global_auc: 0.9952663615371575| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 71 | global_acc: 97.473% | global_loss: 0.07809551060199738 | global_f1: 0.9655485040797824 | global_precision: 0.9708295350957156 | global_recall: 0.9603246167718665 | global_auc: 0.9952321733568662| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 72 | global_acc: 97.473% | global_loss: 0.07770974934101105 | global_f1: 0.9655485040797824 | global_precision: 0.9708295350957156 | global_recall: 0.9603246167718665 | global_auc: 0.9952639873579707| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 73 | global_acc: 97.440% | global_loss: 0.07843952625989914 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9951827904297786| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 74 | global_acc: 97.440% | global_loss: 0.07852461189031601 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9951889632956645| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 75 | global_acc: 97.440% | global_loss: 0.07889331877231598 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9951837401014535| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 76 | global_acc: 97.407% | global_loss: 0.07755689322948456 | global_f1: 0.9647377938517179 | global_precision: 0.9673617407071623 | global_recall: 0.9621280432822362 | global_auc: 0.9952939020157255| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 77 | global_acc: 97.473% | global_loss: 0.0772845670580864 | global_f1: 0.9656419529837251 | global_precision: 0.9682683590208522 | global_recall: 0.9630297565374211 | global_auc: 0.9953537313312356| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 78 | global_acc: 97.440% | global_loss: 0.07809151709079742 | global_f1: 0.9652370203160272 | global_precision: 0.9665461121157324 | global_recall: 0.9639314697926059 | global_auc: 0.9952616131787838| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 79 | global_acc: 97.473% | global_loss: 0.07808318734169006 | global_f1: 0.9656729900632339 | global_precision: 0.967420814479638 | global_recall: 0.9639314697926059 | global_auc: 0.9952791821047668| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 80 | global_acc: 97.473% | global_loss: 0.07753174006938934 | global_f1: 0.9656729900632339 | global_precision: 0.967420814479638 | global_recall: 0.9639314697926059 | global_auc: 0.9953233418376431| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 81 | global_acc: 97.440% | global_loss: 0.07779669016599655 | global_f1: 0.9652370203160272 | global_precision: 0.9665461121157324 | global_recall: 0.9639314697926059 | global_auc: 0.9953100464341965| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 82 | global_acc: 97.640% | global_loss: 0.07675843685865402 | global_f1: 0.9678587596197374 | global_precision: 0.9718181818181818 | global_recall: 0.9639314697926059 | global_auc: 0.9954221076918183| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 83 | global_acc: 97.640% | global_loss: 0.07687569409608841 | global_f1: 0.9678587596197374 | global_precision: 0.9718181818181818 | global_recall: 0.9639314697926059 | global_auc: 0.9953883943473643| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 84 | global_acc: 97.706% | global_loss: 0.07580254226922989 | global_f1: 0.9687358405074762 | global_precision: 0.9735883424408015 | global_recall: 0.9639314697926059 | global_auc: 0.9955239599789363| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 85 | global_acc: 97.706% | global_loss: 0.07622983306646347 | global_f1: 0.9687358405074762 | global_precision: 0.9735883424408015 | global_recall: 0.9639314697926059 | global_auc: 0.9954501230062236| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 86 | global_acc: 97.673% | global_loss: 0.07646902650594711 | global_f1: 0.9682971014492754 | global_precision: 0.9727024567788899 | global_recall: 0.9639314697926059 | global_auc: 0.9954221076918184| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 87 | global_acc: 97.739% | global_loss: 0.07592712342739105 | global_f1: 0.9692028985507246 | global_precision: 0.9736123748862603 | global_recall: 0.9648331830477908 | global_auc: 0.9954999807691487| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 88 | global_acc: 97.773% | global_loss: 0.07583454251289368 | global_f1: 0.9696420480289986 | global_precision: 0.9744990892531876 | global_recall: 0.9648331830477908 | global_auc: 0.9954857356940272| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 89 | global_acc: 97.806% | global_loss: 0.07541704177856445 | global_f1: 0.9701086956521738 | global_precision: 0.9745222929936306 | global_recall: 0.9657348963029756 | global_auc: 0.9955412914870005| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 90 | global_acc: 97.839% | global_loss: 0.0754745751619339 | global_f1: 0.9705749207786328 | global_precision: 0.9745454545454545 | global_recall: 0.9666366095581606 | global_auc: 0.9955564862337967| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 91 | global_acc: 97.806% | global_loss: 0.07577508687973022 | global_f1: 0.970135746606335 | global_precision: 0.9736603088101726 | global_recall: 0.9666366095581606 | global_auc: 0.9955113768292456| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 92 | global_acc: 97.872% | global_loss: 0.07572999596595764 | global_f1: 0.970988213961922 | global_precision: 0.9762989972652689 | global_recall: 0.9657348963029756 | global_auc: 0.995544852755781| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 93 | global_acc: 97.906% | global_loss: 0.07562163472175598 | global_f1: 0.9714285714285713 | global_precision: 0.9771897810218978 | global_recall: 0.9657348963029756 | global_auc: 0.995571680980593| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 94 | global_acc: 97.872% | global_loss: 0.07606702297925949 | global_f1: 0.970988213961922 | global_precision: 0.9762989972652689 | global_recall: 0.9657348963029756 | global_auc: 0.9955484140245614| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 95 | global_acc: 97.906% | global_loss: 0.0755634531378746 | global_f1: 0.9714544630720434 | global_precision: 0.97632058287796 | global_recall: 0.9666366095581606 | global_auc: 0.9956633242972072| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 96 | global_acc: 97.939% | global_loss: 0.07554467022418976 | global_f1: 0.9719710669077758 | global_precision: 0.9746146872166818 | global_recall: 0.9693417493237151 | global_auc: 0.9956467050428991| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 97 | global_acc: 97.839% | global_loss: 0.07598701119422913 | global_f1: 0.970601537765717 | global_precision: 0.9736842105263158 | global_recall: 0.9675383228133454 | global_auc: 0.9955659829505444| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 98 | global_acc: 97.939% | global_loss: 0.07568887621164322 | global_f1: 0.9719710669077758 | global_precision: 0.9746146872166818 | global_recall: 0.9693417493237151 | global_auc: 0.9956656984763943| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 99 | global_acc: 97.939% | global_loss: 0.07586769014596939 | global_f1: 0.9719710669077758 | global_precision: 0.9746146872166818 | global_recall: 0.9693417493237151 | global_auc: 0.9956391076695011| flobal_FPR: 0.030658250676284943 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                                  batch_size=32, shuffle=True)\n",
    "    \n",
    "    \n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model_prox, pgd_attack]\n",
    "    \n",
    "                    # Randomly choose between normal training and PGD attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "    \n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.01\n",
    "                        alpha = 0.01\n",
    "                        num_iter = 5\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-PGD-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-PGD-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-PGD-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb1f186",
   "metadata": {},
   "source": [
    "## non-iid FedProx PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153b54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 763, (1,): 440}\n",
      "Client client_2: {(0,): 1070, (1,): 133}\n",
      "Client client_3: {(0,): 1018, (1,): 185}\n",
      "Client client_4: {(0,): 143, (1,): 1060}\n",
      "Client client_5: {(0,): 571, (1,): 632}\n",
      "Client client_6: {(0,): 525, (1,): 678}\n",
      "Client client_7: {(0,): 140, (1,): 1063}\n",
      "Client client_8: {(0,): 459, (1,): 744}\n",
      "Client client_9: {(0,): 832, (1,): 371}\n",
      "Client client_10: {(0,): 1037, (1,): 166}\n",
      "|=======================|\n",
      "|Traditional FedProx non iid pgd 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 39.262% | global_loss: 0.69559246301651 | global_f1: 0.5465376023827252 | global_precision: 0.37705479452054796 | global_recall: 0.9927862939585211 | global_auc: 0.7332813862927239| flobal_FPR: 0.007213706041478809 \n",
      "comm_round: 1 | global_acc: 62.101% | global_loss: 0.6875537037849426 | global_f1: 0.6317829457364341 | global_precision: 0.4921992954202315 | global_recall: 0.8818755635707844 | global_auc: 0.7991731683563699| flobal_FPR: 0.11812443642921551 \n",
      "comm_round: 2 | global_acc: 80.785% | global_loss: 0.6792295575141907 | global_f1: 0.7255460588793922 | global_precision: 0.7662988966900702 | global_recall: 0.6889089269612263 | global_auc: 0.8436664734084808| flobal_FPR: 0.31109107303877365 \n",
      "comm_round: 3 | global_acc: 78.956% | global_loss: 0.6711714863777161 | global_f1: 0.6343154246100519 | global_precision: 0.882636655948553 | global_recall: 0.4950405770964833 | global_auc: 0.8732986038401874| flobal_FPR: 0.5049594229035167 \n",
      "comm_round: 4 | global_acc: 75.266% | global_loss: 0.6620647311210632 | global_f1: 0.5124508519003931 | global_precision: 0.9376498800959233 | global_recall: 0.3525698827772768 | global_auc: 0.8899059872525572| flobal_FPR: 0.6474301172227231 \n",
      "comm_round: 5 | global_acc: 72.906% | global_loss: 0.6506797671318054 | global_f1: 0.42726633872101194 | global_precision: 0.9681528662420382 | global_recall: 0.27412082957619477 | global_auc: 0.9010321031761296| flobal_FPR: 0.7258791704238052 \n",
      "comm_round: 6 | global_acc: 71.376% | global_loss: 0.6372534036636353 | global_f1: 0.3729060451565914 | global_precision: 0.9696969696969697 | global_recall: 0.2308385933273219 | global_auc: 0.9081833683049927| flobal_FPR: 0.7691614066726781 \n",
      "comm_round: 7 | global_acc: 75.831% | global_loss: 0.6198136806488037 | global_f1: 0.5194976867151355 | global_precision: 0.9727722772277227 | global_recall: 0.35437330928764654 | global_auc: 0.91405613794171| flobal_FPR: 0.6456266907123535 \n",
      "comm_round: 8 | global_acc: 77.493% | global_loss: 0.5960701107978821 | global_f1: 0.5679642629227825 | global_precision: 0.9716157205240175 | global_recall: 0.4012623985572588 | global_auc: 0.9210647149014408| flobal_FPR: 0.5987376014427412 \n",
      "comm_round: 9 | global_acc: 81.117% | global_loss: 0.5737907886505127 | global_f1: 0.6654888103651353 | global_precision: 0.9592529711375212 | global_recall: 0.5094679891794409 | global_auc: 0.9272176376822123| flobal_FPR: 0.4905320108205591 \n",
      "comm_round: 10 | global_acc: 86.170% | global_loss: 0.5495356917381287 | global_f1: 0.7849017580144778 | global_precision: 0.92 | global_recall: 0.6844003606853021 | global_auc: 0.9313387379148343| flobal_FPR: 0.3155996393146979 \n",
      "comm_round: 11 | global_acc: 87.267% | global_loss: 0.5219849348068237 | global_f1: 0.8121628249141738 | global_precision: 0.8903225806451613 | global_recall: 0.7466185752930569 | global_auc: 0.9391032535276742| flobal_FPR: 0.2533814247069432 \n",
      "comm_round: 12 | global_acc: 89.395% | global_loss: 0.4919528663158417 | global_f1: 0.8507253158633598 | global_precision: 0.8842412451361867 | global_recall: 0.8196573489630298 | global_auc: 0.9505420488501612| flobal_FPR: 0.18034265103697025 \n",
      "comm_round: 13 | global_acc: 90.193% | global_loss: 0.4596949517726898 | global_f1: 0.8663343905754419 | global_precision: 0.8706739526411658 | global_recall: 0.8620378719567178 | global_auc: 0.9591308794766928| flobal_FPR: 0.13796212804328223 \n",
      "comm_round: 14 | global_acc: 91.589% | global_loss: 0.42679762840270996 | global_f1: 0.8880035413899955 | global_precision: 0.8721739130434782 | global_recall: 0.9044183949504058 | global_auc: 0.967169375367701| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 15 | global_acc: 91.689% | global_loss: 0.39204537868499756 | global_f1: 0.8912097476066145 | global_precision: 0.8612279226240538 | global_recall: 0.9233543733092876 | global_auc: 0.9716693945985524| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 16 | global_acc: 91.390% | global_loss: 0.3593020439147949 | global_f1: 0.8886978942844864 | global_precision: 0.8489326765188834 | global_recall: 0.9323715058611362 | global_auc: 0.9754837508802269| flobal_FPR: 0.06762849413886383 \n",
      "comm_round: 17 | global_acc: 92.121% | global_loss: 0.32972294092178345 | global_f1: 0.8991918332624415 | global_precision: 0.8510466988727858 | global_recall: 0.9531109107303878 | global_auc: 0.9788774026099827| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 18 | global_acc: 94.016% | global_loss: 0.2940726578235626 | global_f1: 0.9210526315789473 | global_precision: 0.89666951323655 | global_recall: 0.9467989179440938 | global_auc: 0.981486150700549| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 19 | global_acc: 93.650% | global_loss: 0.2716866433620453 | global_f1: 0.9172802078822001 | global_precision: 0.8825 | global_recall: 0.9549143372407575 | global_auc: 0.9827155006835262| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 20 | global_acc: 93.617% | global_loss: 0.2483677715063095 | global_f1: 0.9168110918544194 | global_precision: 0.8824020016680567 | global_recall: 0.9540126239855726 | global_auc: 0.9835778025642086| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 21 | global_acc: 94.149% | global_loss: 0.2230393886566162 | global_f1: 0.9226033421284081 | global_precision: 0.9004291845493563 | global_recall: 0.9458972046889089 | global_auc: 0.9845156033430342| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 22 | global_acc: 93.816% | global_loss: 0.20506130158901215 | global_f1: 0.9173333333333334 | global_precision: 0.9044697633654689 | global_recall: 0.9305680793507665 | global_auc: 0.9849880650012275| flobal_FPR: 0.06943192064923355 \n",
      "comm_round: 23 | global_acc: 94.382% | global_loss: 0.1906024068593979 | global_f1: 0.9253203711886877 | global_precision: 0.9072790294627383 | global_recall: 0.9440937781785392 | global_auc: 0.9857857892080261| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 24 | global_acc: 94.548% | global_loss: 0.1792987883090973 | global_f1: 0.9280070237050044 | global_precision: 0.9041916167664671 | global_recall: 0.9531109107303878 | global_auc: 0.9867634761971917| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 25 | global_acc: 94.415% | global_loss: 0.17457205057144165 | global_f1: 0.926637554585153 | global_precision: 0.8983911939034717 | global_recall: 0.9567177637511272 | global_auc: 0.9872663273489773| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 26 | global_acc: 95.080% | global_loss: 0.15924954414367676 | global_f1: 0.9343971631205674 | global_precision: 0.918918918918919 | global_recall: 0.9504057709648331 | global_auc: 0.9875246380445121| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 27 | global_acc: 95.180% | global_loss: 0.15229706466197968 | global_f1: 0.9359257622624834 | global_precision: 0.9176776429809359 | global_recall: 0.9549143372407575 | global_auc: 0.9883793425517963| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 28 | global_acc: 95.445% | global_loss: 0.14301228523254395 | global_f1: 0.9390298175344903 | global_precision: 0.9270650263620387 | global_recall: 0.951307484220018 | global_auc: 0.9888907407486547| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 29 | global_acc: 95.545% | global_loss: 0.13587920367717743 | global_f1: 0.9402852049910873 | global_precision: 0.9295154185022027 | global_recall: 0.951307484220018 | global_auc: 0.9895996706538632| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 30 | global_acc: 95.578% | global_loss: 0.13234025239944458 | global_f1: 0.94086260560249 | global_precision: 0.9280701754385965 | global_recall: 0.9540126239855726 | global_auc: 0.9902155327349453| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 31 | global_acc: 95.545% | global_loss: 0.12663675844669342 | global_f1: 0.9401251117068811 | global_precision: 0.9317980513728964 | global_recall: 0.9486023444544635 | global_auc: 0.9904163882941569| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 32 | global_acc: 95.811% | global_loss: 0.12338613718748093 | global_f1: 0.943900267141585 | global_precision: 0.9322779243623571 | global_recall: 0.9558160504959423 | global_auc: 0.9913807798798762| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 33 | global_acc: 95.911% | global_loss: 0.11889412999153137 | global_f1: 0.9450647610540419 | global_precision: 0.9362831858407079 | global_recall: 0.9540126239855726 | global_auc: 0.9913527645654706| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 34 | global_acc: 96.144% | global_loss: 0.11499325931072235 | global_f1: 0.9480286738351255 | global_precision: 0.9421193232413179 | global_recall: 0.9540126239855726 | global_auc: 0.9919581802581302| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 35 | global_acc: 96.077% | global_loss: 0.1145075112581253 | global_f1: 0.9473214285714286 | global_precision: 0.9381078691423519 | global_recall: 0.9567177637511272 | global_auc: 0.9922236134912258| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 36 | global_acc: 96.144% | global_loss: 0.11214015632867813 | global_f1: 0.9480752014324083 | global_precision: 0.9413333333333334 | global_recall: 0.9549143372407575 | global_auc: 0.9921205741145143| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 37 | global_acc: 96.210% | global_loss: 0.1116681694984436 | global_f1: 0.9489247311827956 | global_precision: 0.9430097951914514 | global_recall: 0.9549143372407575 | global_auc: 0.9917022437417824| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 38 | global_acc: 96.177% | global_loss: 0.10910648852586746 | global_f1: 0.9482215218370104 | global_precision: 0.9469424460431655 | global_recall: 0.9495040577096483 | global_auc: 0.9918670117773533| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 39 | global_acc: 96.310% | global_loss: 0.10683349519968033 | global_f1: 0.949932341001353 | global_precision: 0.9503610108303249 | global_recall: 0.9495040577096483 | global_auc: 0.9921519132797813| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 40 | global_acc: 96.376% | global_loss: 0.10573183000087738 | global_f1: 0.9508787742226227 | global_precision: 0.9504504504504504 | global_recall: 0.951307484220018 | global_auc: 0.9922136419386407| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 41 | global_acc: 96.343% | global_loss: 0.10570274293422699 | global_f1: 0.9503161698283648 | global_precision: 0.9520361990950226 | global_recall: 0.9486023444544635 | global_auc: 0.9921333946821236| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 42 | global_acc: 96.443% | global_loss: 0.1040293425321579 | global_f1: 0.9516056083220262 | global_precision: 0.9546279491833031 | global_recall: 0.9486023444544635 | global_auc: 0.9923370992563597| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 43 | global_acc: 96.642% | global_loss: 0.10210788995027542 | global_f1: 0.9545249887438091 | global_precision: 0.9532374100719424 | global_recall: 0.9558160504959423 | global_auc: 0.9928955062011185| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 44 | global_acc: 96.443% | global_loss: 0.1035439521074295 | global_f1: 0.9519533004041311 | global_precision: 0.9481216457960644 | global_recall: 0.9558160504959423 | global_auc: 0.9926398071026894| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 45 | global_acc: 96.576% | global_loss: 0.10228509455919266 | global_f1: 0.9536662168241116 | global_precision: 0.9515260323159784 | global_recall: 0.9558160504959423 | global_auc: 0.9929026287386793| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 46 | global_acc: 96.509% | global_loss: 0.10255413502454758 | global_f1: 0.9527665317139001 | global_precision: 0.9506283662477558 | global_recall: 0.9549143372407575 | global_auc: 0.9927392852106205| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 47 | global_acc: 96.676% | global_loss: 0.10175715386867523 | global_f1: 0.954954954954955 | global_precision: 0.9540954095409541 | global_recall: 0.9558160504959423 | global_auc: 0.9929211473363372| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 48 | global_acc: 96.709% | global_loss: 0.10182753950357437 | global_f1: 0.955505617977528 | global_precision: 0.9525089605734767 | global_recall: 0.9585211902614968 | global_auc: 0.9932084230179522| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 49 | global_acc: 96.742% | global_loss: 0.10152766108512878 | global_f1: 0.9559352517985612 | global_precision: 0.9533632286995516 | global_recall: 0.9585211902614968 | global_auc: 0.9930749941476483| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 50 | global_acc: 96.742% | global_loss: 0.10241499543190002 | global_f1: 0.9559352517985612 | global_precision: 0.9533632286995516 | global_recall: 0.9585211902614968 | global_auc: 0.9929691057559125| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 51 | global_acc: 96.742% | global_loss: 0.10200332850217819 | global_f1: 0.9559352517985612 | global_precision: 0.9533632286995516 | global_recall: 0.9585211902614968 | global_auc: 0.9930830663568838| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 52 | global_acc: 96.742% | global_loss: 0.10319049656391144 | global_f1: 0.9558955895589558 | global_precision: 0.954177897574124 | global_recall: 0.957619477006312 | global_auc: 0.9928912326785821| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 53 | global_acc: 96.775% | global_loss: 0.10225122421979904 | global_f1: 0.9564044943820225 | global_precision: 0.953405017921147 | global_recall: 0.9594229035166817 | global_auc: 0.993149543374117| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 54 | global_acc: 96.775% | global_loss: 0.10191374272108078 | global_f1: 0.9564044943820225 | global_precision: 0.953405017921147 | global_recall: 0.9594229035166817 | global_auc: 0.9932459350491051| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 55 | global_acc: 96.809% | global_loss: 0.10241658985614777 | global_f1: 0.9568345323741008 | global_precision: 0.9542600896860987 | global_recall: 0.9594229035166817 | global_auc: 0.9930531516991288| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 56 | global_acc: 96.775% | global_loss: 0.1012701615691185 | global_f1: 0.9564044943820225 | global_precision: 0.953405017921147 | global_recall: 0.9594229035166817 | global_auc: 0.993244510541593| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 57 | global_acc: 96.842% | global_loss: 0.10178083181381226 | global_f1: 0.9572649572649572 | global_precision: 0.9551166965888689 | global_recall: 0.9594229035166817 | global_auc: 0.9932174448988624| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 58 | global_acc: 96.809% | global_loss: 0.10140766948461533 | global_f1: 0.9568733153638814 | global_precision: 0.9534467323187108 | global_recall: 0.9603246167718665 | global_auc: 0.9933029153495907| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 59 | global_acc: 96.775% | global_loss: 0.10197145491838455 | global_f1: 0.9564044943820225 | global_precision: 0.953405017921147 | global_recall: 0.9594229035166817 | global_auc: 0.9933285564848093| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 60 | global_acc: 96.775% | global_loss: 0.10178647935390472 | global_f1: 0.9564044943820225 | global_precision: 0.953405017921147 | global_recall: 0.9594229035166817 | global_auc: 0.9934539131458776| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 61 | global_acc: 96.775% | global_loss: 0.10254170745611191 | global_f1: 0.9564044943820225 | global_precision: 0.953405017921147 | global_recall: 0.9594229035166817 | global_auc: 0.9934453661008049| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 62 | global_acc: 96.842% | global_loss: 0.10266533493995667 | global_f1: 0.9573417153120791 | global_precision: 0.9534883720930233 | global_recall: 0.9612263300270514 | global_auc: 0.9934263726673095| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 63 | global_acc: 96.809% | global_loss: 0.10243982076644897 | global_f1: 0.9569120287253142 | global_precision: 0.9526362823949955 | global_recall: 0.9612263300270514 | global_auc: 0.9934937993562176| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 64 | global_acc: 96.809% | global_loss: 0.10233645886182785 | global_f1: 0.9568733153638814 | global_precision: 0.9534467323187108 | global_recall: 0.9603246167718665 | global_auc: 0.9934645969522188| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 65 | global_acc: 96.908% | global_loss: 0.10398968309164047 | global_f1: 0.9582022471910112 | global_precision: 0.9551971326164874 | global_recall: 0.9612263300270514 | global_auc: 0.9932532950045845| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 66 | global_acc: 96.842% | global_loss: 0.10241472721099854 | global_f1: 0.9574181981174361 | global_precision: 0.9518716577540107 | global_recall: 0.9630297565374211 | global_auc: 0.9937345411257693| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 67 | global_acc: 96.809% | global_loss: 0.10356630384922028 | global_f1: 0.956989247311828 | global_precision: 0.9510240427426536 | global_recall: 0.9630297565374211 | global_auc: 0.9936151199126683| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 68 | global_acc: 96.842% | global_loss: 0.10494779795408249 | global_f1: 0.9574181981174361 | global_precision: 0.9518716577540107 | global_recall: 0.9630297565374211 | global_auc: 0.9934624601909505| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 69 | global_acc: 96.842% | global_loss: 0.10411086678504944 | global_f1: 0.9574181981174361 | global_precision: 0.9518716577540107 | global_recall: 0.9630297565374211 | global_auc: 0.9936832588553322| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 70 | global_acc: 96.809% | global_loss: 0.1042352244257927 | global_f1: 0.95695067264574 | global_precision: 0.951828724353256 | global_recall: 0.9621280432822362 | global_auc: 0.993706525811364| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 71 | global_acc: 96.908% | global_loss: 0.1041615679860115 | global_f1: 0.9583519928347515 | global_precision: 0.9519572953736655 | global_recall: 0.9648331830477908 | global_auc: 0.9936723376310724| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 72 | global_acc: 96.908% | global_loss: 0.1046004667878151 | global_f1: 0.9583892617449664 | global_precision: 0.9511545293072824 | global_recall: 0.9657348963029756 | global_auc: 0.9937820247095073| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 73 | global_acc: 96.975% | global_loss: 0.10499493777751923 | global_f1: 0.9593205185516316 | global_precision: 0.9512411347517731 | global_recall: 0.9675383228133454 | global_auc: 0.9938114645314249| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 74 | global_acc: 96.941% | global_loss: 0.10398917645215988 | global_f1: 0.9587813620071686 | global_precision: 0.9528049866429208 | global_recall: 0.9648331830477908 | global_auc: 0.9936853956166004| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 75 | global_acc: 96.908% | global_loss: 0.10478679090738297 | global_f1: 0.9583892617449664 | global_precision: 0.9511545293072824 | global_recall: 0.9657348963029756 | global_auc: 0.9937820247095074| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 76 | global_acc: 96.908% | global_loss: 0.10546614974737167 | global_f1: 0.9583892617449664 | global_precision: 0.9511545293072824 | global_recall: 0.9657348963029756 | global_auc: 0.9937746647540279| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 77 | global_acc: 96.875% | global_loss: 0.10560677200555801 | global_f1: 0.9579982126899017 | global_precision: 0.9495128432240921 | global_recall: 0.9666366095581606 | global_auc: 0.9939270870578271| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 78 | global_acc: 96.875% | global_loss: 0.10698683559894562 | global_f1: 0.9579982126899017 | global_precision: 0.9495128432240921 | global_recall: 0.9666366095581606 | global_auc: 0.9938551494284638| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 79 | global_acc: 96.775% | global_loss: 0.10836679488420486 | global_f1: 0.9566770879857079 | global_precision: 0.947787610619469 | global_recall: 0.9657348963029756 | global_auc: 0.9936770859894462| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 80 | global_acc: 96.975% | global_loss: 0.10753317922353745 | global_f1: 0.9593931280678268 | global_precision: 0.9496466431095406 | global_recall: 0.9693417493237151 | global_auc: 0.9939444185658913| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 81 | global_acc: 96.809% | global_loss: 0.10931810736656189 | global_f1: 0.9571810883140053 | global_precision: 0.9470432480141218 | global_recall: 0.9675383228133454 | global_auc: 0.9938261844423837| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 82 | global_acc: 96.875% | global_loss: 0.10658417642116547 | global_f1: 0.9579982126899017 | global_precision: 0.9495128432240921 | global_recall: 0.9666366095581606 | global_auc: 0.9938950356388039| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 83 | global_acc: 96.941% | global_loss: 0.10730084776878357 | global_f1: 0.9589285714285714 | global_precision: 0.9496021220159151 | global_recall: 0.9684400360685302 | global_auc: 0.9939534404468015| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 84 | global_acc: 96.908% | global_loss: 0.10535456240177155 | global_f1: 0.958426464014305 | global_precision: 0.950354609929078 | global_recall: 0.9666366095581606 | global_auc: 0.9939674481040042| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 85 | global_acc: 96.941% | global_loss: 0.10687614977359772 | global_f1: 0.9589285714285714 | global_precision: 0.9496021220159151 | global_recall: 0.9684400360685302 | global_auc: 0.9939771822386706| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 86 | global_acc: 96.908% | global_loss: 0.10842414945363998 | global_f1: 0.958426464014305 | global_precision: 0.950354609929078 | global_recall: 0.9666366095581606 | global_auc: 0.9937475991112972| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 87 | global_acc: 96.875% | global_loss: 0.1077372282743454 | global_f1: 0.9579982126899017 | global_precision: 0.9495128432240921 | global_recall: 0.9666366095581606 | global_auc: 0.9939370586104119| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 88 | global_acc: 96.941% | global_loss: 0.10866304486989975 | global_f1: 0.958855098389982 | global_precision: 0.9511978704525288 | global_recall: 0.9666366095581606 | global_auc: 0.9937813124557513| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 89 | global_acc: 96.941% | global_loss: 0.10806235671043396 | global_f1: 0.958855098389982 | global_precision: 0.9511978704525288 | global_recall: 0.9666366095581606 | global_auc: 0.9938430411146106| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 90 | global_acc: 96.975% | global_loss: 0.1091206967830658 | global_f1: 0.9592476489028213 | global_precision: 0.952846975088968 | global_recall: 0.9657348963029756 | global_auc: 0.9937580454997196| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 91 | global_acc: 96.941% | global_loss: 0.10818180441856384 | global_f1: 0.958855098389982 | global_precision: 0.9511978704525288 | global_recall: 0.9666366095581606 | global_auc: 0.9938819776532758| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 92 | global_acc: 97.041% | global_loss: 0.10729154944419861 | global_f1: 0.960178970917226 | global_precision: 0.9529307282415631 | global_recall: 0.9675383228133454 | global_auc: 0.9939821680149631| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 93 | global_acc: 97.041% | global_loss: 0.10778272897005081 | global_f1: 0.960178970917226 | global_precision: 0.9529307282415631 | global_recall: 0.9675383228133454 | global_auc: 0.9939539152826389| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 94 | global_acc: 97.141% | global_loss: 0.10741935670375824 | global_f1: 0.9615040286481648 | global_precision: 0.9546666666666667 | global_recall: 0.9684400360685302 | global_auc: 0.9940249032403272| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 95 | global_acc: 97.108% | global_loss: 0.107123963534832 | global_f1: 0.9610040340654415 | global_precision: 0.9554367201426025 | global_recall: 0.9666366095581606 | global_auc: 0.9940377238079365| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 96 | global_acc: 97.141% | global_loss: 0.10714889317750931 | global_f1: 0.9614695340501791 | global_precision: 0.9554764024933214 | global_recall: 0.9675383228133454 | global_auc: 0.9940481701963588| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 97 | global_acc: 97.141% | global_loss: 0.10831885784864426 | global_f1: 0.9614695340501791 | global_precision: 0.9554764024933214 | global_recall: 0.9675383228133454 | global_auc: 0.9938753299515526| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 98 | global_acc: 97.141% | global_loss: 0.108216792345047 | global_f1: 0.9614695340501791 | global_precision: 0.9554764024933214 | global_recall: 0.9675383228133454 | global_auc: 0.9939377708641679| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 99 | global_acc: 97.108% | global_loss: 0.10826246440410614 | global_f1: 0.961038961038961 | global_precision: 0.9546263345195729 | global_recall: 0.9675383228133454 | global_auc: 0.9939698222831912| flobal_FPR: 0.032461677186654644 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0, 1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx non iid pgd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    selected_training_approach = random.choice([train_model_prox, pgd_attack])\n",
    "\n",
    "                    # List of training approaches    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.1  # Set your desired value for epsilon\n",
    "                        alpha = 0.01   # Set your desired value for alpha\n",
    "                        num_iter = 10   # Set your desired number of iterations\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-non-iid-pgd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-non-iid-pgd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-non-iid-pgd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
