{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f200b28",
   "metadata": {},
   "source": [
    "## fedADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "\n",
    "\n",
    "# def update_avg_with_delta(avg_grad, delta_x_hats):\n",
    "#     '''Update avg_grad by adding the corresponding elements from delta_x_hats'''\n",
    "#     for avg_layer, delta_layers in zip(avg_grad, zip(*delta_x_hats)):\n",
    "#         avg_layer += delta_layers\n",
    "#     return avg_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM 2022|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "   \n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f58d3",
   "metadata": {},
   "source": [
    "## fedadmm nonidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f343ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 833, (1,): 370}\n",
      "Client client_2: {(0,): 1083, (1,): 120}\n",
      "Client client_3: {(0,): 713, (1,): 490}\n",
      "Client client_4: {(0,): 92, (1,): 1111}\n",
      "Client client_5: {(0,): 791, (1,): 412}\n",
      "Client client_6: {(0,): 616, (1,): 587}\n",
      "Client client_7: {(0,): 925, (1,): 278}\n",
      "Client client_8: {(0,): 190, (1,): 1013}\n",
      "Client client_9: {(0,): 30, (1,): 1173}\n",
      "Client client_10: {(0,): 364, (1,): 839}\n",
      "|=======================|\n",
      "|Traditional FedADMM-nonidd 2022|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 41.290% | global_loss: 0.6938012838363647 | global_f1: 0.5533636823469904 | global_precision: 0.38453427065026363 | global_recall: 0.9864743011722272 | global_auc: 0.809579433150474| flobal_FPR: 0.013525698827772768 \n",
      "comm_round: 1 | global_acc: 74.235% | global_loss: 0.6535457968711853 | global_f1: 0.7328507411237505 | global_precision: 0.5931919642857143 | global_recall: 0.9585211902614968 | global_auc: 0.9057863969978981| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 2 | global_acc: 81.782% | global_loss: 0.49970605969429016 | global_f1: 0.7892307692307692 | global_precision: 0.6881287726358148 | global_recall: 0.9251577998196574 | global_auc: 0.9387898618750035| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 3 | global_acc: 84.142% | global_loss: 0.29130417108535767 | global_f1: 0.8173113749521256 | global_precision: 0.7103861517976032 | global_recall: 0.9621280432822362 | global_auc: 0.9760127180030684| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 4 | global_acc: 93.949% | global_loss: 0.14793311059474945 | global_f1: 0.9198237885462555 | global_precision: 0.8992248062015504 | global_recall: 0.9413886384129847 | global_auc: 0.9869980451008575| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 5 | global_acc: 95.312% | global_loss: 0.1568116843700409 | global_f1: 0.9379128137384412 | global_precision: 0.9165232358003442 | global_recall: 0.9603246167718665 | global_auc: 0.9905025709986416| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 6 | global_acc: 95.711% | global_loss: 0.25291669368743896 | global_f1: 0.9435448577680525 | global_precision: 0.9166666666666666 | global_recall: 0.9720468890892696 | global_auc: 0.9926352961622342| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 7 | global_acc: 95.346% | global_loss: 0.3834681808948517 | global_f1: 0.9389712292938098 | global_precision: 0.9088607594936708 | global_recall: 0.9711451758340848 | global_auc: 0.9927364361955964| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 8 | global_acc: 95.778% | global_loss: 0.4596060812473297 | global_f1: 0.9444201312910284 | global_precision: 0.9175170068027211 | global_recall: 0.9729486023444545 | global_auc: 0.9922380959842658| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 9 | global_acc: 95.811% | global_loss: 0.5189818739891052 | global_f1: 0.9447852760736196 | global_precision: 0.9190110826939472 | global_recall: 0.9720468890892696 | global_auc: 0.9920189592453149| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 10 | global_acc: 95.379% | global_loss: 0.6966089010238647 | global_f1: 0.9398007795582503 | global_precision: 0.9041666666666667 | global_recall: 0.9783588818755635 | global_auc: 0.991299582951684| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 11 | global_acc: 96.310% | global_loss: 0.828863263130188 | global_f1: 0.9510366122629025 | global_precision: 0.9309153713298791 | global_recall: 0.9720468890892696 | global_auc: 0.990535809507258| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 12 | global_acc: 96.376% | global_loss: 1.1395443677902222 | global_f1: 0.9517485613103143 | global_precision: 0.9347826086956522 | global_recall: 0.9693417493237151 | global_auc: 0.9885027998695151| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 13 | global_acc: 95.180% | global_loss: 1.8244340419769287 | global_f1: 0.9373108517077388 | global_precision: 0.9003322259136213 | global_recall: 0.9774571686203787 | global_auc: 0.9835362544284376| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 14 | global_acc: 95.811% | global_loss: 2.025808095932007 | global_f1: 0.9450741063644289 | global_precision: 0.9147679324894514 | global_recall: 0.9774571686203787 | global_auc: 0.9824253759868868| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 15 | global_acc: 96.476% | global_loss: 1.7376036643981934 | global_f1: 0.9529724933451642 | global_precision: 0.9379912663755459 | global_recall: 0.9684400360685302 | global_auc: 0.9855704511557741| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 16 | global_acc: 96.576% | global_loss: 2.0413382053375244 | global_f1: 0.9544851966416261 | global_precision: 0.9358752166377816 | global_recall: 0.9738503155996393 | global_auc: 0.9833339743617138| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 17 | global_acc: 96.343% | global_loss: 2.405719757080078 | global_f1: 0.9517120280948199 | global_precision: 0.9272882805816938 | global_recall: 0.9774571686203787 | global_auc: 0.9807712853473733| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 18 | global_acc: 96.011% | global_loss: 2.8850512504577637 | global_f1: 0.9476439790575916 | global_precision: 0.9180050718512257 | global_recall: 0.9792605951307484 | global_auc: 0.9765689881865594| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 19 | global_acc: 92.354% | global_loss: 3.7868471145629883 | global_f1: 0.9045643153526972 | global_precision: 0.8378170637970792 | global_recall: 0.9828674481514879 | global_auc: 0.9713294121389883| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 20 | global_acc: 96.576% | global_loss: 2.392411470413208 | global_f1: 0.9540383757251227 | global_precision: 0.9443462897526502 | global_recall: 0.9639314697926059 | global_auc: 0.9798826300777164| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 21 | global_acc: 95.745% | global_loss: 3.3829965591430664 | global_f1: 0.9443478260869564 | global_precision: 0.9118387909319899 | global_recall: 0.9792605951307484 | global_auc: 0.9717472676758827| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 22 | global_acc: 96.576% | global_loss: 2.8118574619293213 | global_f1: 0.9544851966416261 | global_precision: 0.9358752166377816 | global_recall: 0.9738503155996393 | global_auc: 0.9757686523826552| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 23 | global_acc: 96.676% | global_loss: 2.8393375873565674 | global_f1: 0.9557130203720106 | global_precision: 0.939077458659704 | global_recall: 0.9729486023444545 | global_auc: 0.9750832268513967| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 24 | global_acc: 96.310% | global_loss: 2.9752261638641357 | global_f1: 0.9512087912087912 | global_precision: 0.9279588336192109 | global_recall: 0.975653742110009 | global_auc: 0.9741556350430746| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 25 | global_acc: 96.576% | global_loss: 2.8850905895233154 | global_f1: 0.9545655050727834 | global_precision: 0.9343696027633851 | global_recall: 0.975653742110009 | global_auc: 0.9747444314814262| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 26 | global_acc: 96.376% | global_loss: 3.0170793533325195 | global_f1: 0.9521720052654673 | global_precision: 0.9273504273504274 | global_recall: 0.9783588818755635 | global_auc: 0.9737403911032859| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 27 | global_acc: 96.709% | global_loss: 2.8522346019744873 | global_f1: 0.9563299514777239 | global_precision: 0.9360967184801382 | global_recall: 0.9774571686203787 | global_auc: 0.9741375912812543| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 28 | global_acc: 95.180% | global_loss: 3.9032137393951416 | global_f1: 0.9374730487279 | global_precision: 0.8983471074380165 | global_recall: 0.9801623083859333 | global_auc: 0.9665765428247319| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 29 | global_acc: 96.376% | global_loss: 3.1657984256744385 | global_f1: 0.9521720052654673 | global_precision: 0.9273504273504274 | global_recall: 0.9783588818755635 | global_auc: 0.9717232884660951| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 30 | global_acc: 96.277% | global_loss: 3.286437749862671 | global_f1: 0.9509632224168127 | global_precision: 0.9242553191489362 | global_recall: 0.9792605951307484 | global_auc: 0.9706537207423963| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 31 | global_acc: 95.479% | global_loss: 4.066819667816162 | global_f1: 0.9411255411255411 | global_precision: 0.9050791007493755 | global_recall: 0.9801623083859333 | global_auc: 0.9649089193638529| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 32 | global_acc: 95.678% | global_loss: 3.9591565132141113 | global_f1: 0.9433797909407665 | global_precision: 0.9123841617523167 | global_recall: 0.9765554553651938 | global_auc: 0.9658087332756884| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 33 | global_acc: 96.144% | global_loss: 3.5548417568206787 | global_f1: 0.9491673970201577 | global_precision: 0.9232736572890026 | global_recall: 0.9765554553651938 | global_auc: 0.9694749407761002| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 34 | global_acc: 92.221% | global_loss: 7.177709579467773 | global_f1: 0.9029850746268657 | global_precision: 0.8357636224098235 | global_recall: 0.981965734896303 | global_auc: 0.940234787328151| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 35 | global_acc: 96.642% | global_loss: 3.110870122909546 | global_f1: 0.9554869986778317 | global_precision: 0.9344827586206896 | global_recall: 0.9774571686203787 | global_auc: 0.972362892339046| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 36 | global_acc: 96.875% | global_loss: 2.9675819873809814 | global_f1: 0.958443854995579 | global_precision: 0.9401561144839549 | global_recall: 0.9774571686203787 | global_auc: 0.9726829316934403| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 37 | global_acc: 97.041% | global_loss: 2.7418034076690674 | global_f1: 0.9605321507760531 | global_precision: 0.9450261780104712 | global_recall: 0.9765554553651938 | global_auc: 0.9738593374805495| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 38 | global_acc: 96.210% | global_loss: 3.5648229122161865 | global_f1: 0.9501312335958004 | global_precision: 0.9226847918436704 | global_recall: 0.9792605951307484 | global_auc: 0.9683602636478503| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 39 | global_acc: 95.612% | global_loss: 4.013059139251709 | global_f1: 0.9428076256499134 | global_precision: 0.9074228523769808 | global_recall: 0.9810640216411182 | global_auc: 0.9656997584510095| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 40 | global_acc: 96.775% | global_loss: 2.991344451904297 | global_f1: 0.9567543468568882 | global_precision: 0.9462081128747796 | global_recall: 0.9675383228133454 | global_auc: 0.9722508310814243| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 41 | global_acc: 96.011% | global_loss: 3.7359976768493652 | global_f1: 0.9476439790575916 | global_precision: 0.9180050718512257 | global_recall: 0.9792605951307484 | global_auc: 0.9675627768589704| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 42 | global_acc: 95.545% | global_loss: 4.177011489868164 | global_f1: 0.9419410745233969 | global_precision: 0.9065888240200167 | global_recall: 0.9801623083859333 | global_auc: 0.9642978056411448| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 43 | global_acc: 95.113% | global_loss: 4.835699081420898 | global_f1: 0.9367741935483872 | global_precision: 0.8955592105263158 | global_recall: 0.981965734896303 | global_auc: 0.958133486800276| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 44 | global_acc: 94.548% | global_loss: 4.615039825439453 | global_f1: 0.9269162210338681 | global_precision: 0.9162995594713657 | global_recall: 0.9377817853922452 | global_auc: 0.9578124977742071| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 45 | global_acc: 93.418% | global_loss: 6.388477802276611 | global_f1: 0.9159592529711374 | global_precision: 0.8652766639935846 | global_recall: 0.9729486023444545 | global_auc: 0.9441913569431207| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 46 | global_acc: 95.346% | global_loss: 4.573319911956787 | global_f1: 0.9386503067484663 | global_precision: 0.9130434782608695 | global_recall: 0.9657348963029756 | global_auc: 0.9570575087927726| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 47 | global_acc: 94.781% | global_loss: 5.009566783905029 | global_f1: 0.9318280503690838 | global_precision: 0.8986599664991625 | global_recall: 0.9675383228133454 | global_auc: 0.9542298613811739| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 48 | global_acc: 94.648% | global_loss: 5.3144097328186035 | global_f1: 0.9297250109122653 | global_precision: 0.9010152284263959 | global_recall: 0.9603246167718665 | global_auc: 0.9509069601911879| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 49 | global_acc: 94.681% | global_loss: 5.270488262176514 | global_f1: 0.9294532627865961 | global_precision: 0.909404659188956 | global_recall: 0.9504057709648331 | global_auc: 0.948357566580294| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 50 | global_acc: 94.415% | global_loss: 5.476656436920166 | global_f1: 0.926637554585153 | global_precision: 0.8983911939034717 | global_recall: 0.9567177637511272 | global_auc: 0.9480781256899958| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 51 | global_acc: 92.420% | global_loss: 7.517874717712402 | global_f1: 0.9048414023372288 | global_precision: 0.8422688422688422 | global_recall: 0.9774571686203787 | global_auc: 0.9357644453371359| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 52 | global_acc: 93.517% | global_loss: 6.454402923583984 | global_f1: 0.9165596919127087 | global_precision: 0.8721498371335505 | global_recall: 0.9657348963029756 | global_auc: 0.9417480891418815| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 53 | global_acc: 93.318% | global_loss: 6.678088665008545 | global_f1: 0.9138448349764251 | global_precision: 0.8709150326797386 | global_recall: 0.9612263300270514 | global_auc: 0.9393672622532574| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 54 | global_acc: 92.354% | global_loss: 7.5930047035217285 | global_f1: 0.902542372881356 | global_precision: 0.8513189448441247 | global_recall: 0.9603246167718665 | global_auc: 0.9323544117709905| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 55 | global_acc: 92.320% | global_loss: 7.6795220375061035 | global_f1: 0.9004739336492891 | global_precision: 0.8622112211221122 | global_recall: 0.9422903516681695 | global_auc: 0.927129080798541| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 56 | global_acc: 91.822% | global_loss: 8.079869270324707 | global_f1: 0.895408163265306 | global_precision: 0.8471440064360418 | global_recall: 0.9495040577096483 | global_auc: 0.9261551924960743| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 57 | global_acc: 85.605% | global_loss: 14.302282333374023 | global_f1: 0.8319751649204501 | global_precision: 0.7302452316076294 | global_recall: 0.9666366095581606 | global_auc: 0.8798164379619857| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 58 | global_acc: 84.043% | global_loss: 15.957447052001953 | global_f1: 0.8166539343009931 | global_precision: 0.7084161696487741 | global_recall: 0.9639314697926059 | global_auc: 0.8661057905755531| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 59 | global_acc: 82.547% | global_loss: 17.445720672607422 | global_f1: 0.8031496062992126 | global_precision: 0.6874197689345315 | global_recall: 0.9657348963029756 | global_auc: 0.8549495225763073| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 60 | global_acc: 81.383% | global_loss: 18.61044692993164 | global_f1: 0.7921306607275427 | global_precision: 0.673186119873817 | global_recall: 0.9621280432822362 | global_auc: 0.8449941144097957| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 61 | global_acc: 80.452% | global_loss: 19.54059410095215 | global_f1: 0.7839823659074211 | global_precision: 0.6615003099814011 | global_recall: 0.9621280432822362 | global_auc: 0.8376253744674123| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 62 | global_acc: 79.156% | global_loss: 20.8276424407959 | global_f1: 0.7727437477346866 | global_precision: 0.6460606060606061 | global_recall: 0.9612263300270514 | global_auc: 0.827118919311621| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 63 | global_acc: 79.156% | global_loss: 20.84441566467285 | global_f1: 0.7709170624771647 | global_precision: 0.648034398034398 | global_recall: 0.951307484220018 | global_auc: 0.824784863752979| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 64 | global_acc: 78.890% | global_loss: 21.11037254333496 | global_f1: 0.7679941541834124 | global_precision: 0.6455773955773956 | global_recall: 0.9477006311992786 | global_auc: 0.821791261216216| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 65 | global_acc: 78.923% | global_loss: 21.07712745666504 | global_f1: 0.7724335965541996 | global_precision: 0.6416219439475254 | global_recall: 0.9702434625788999 | global_auc: 0.8268727169299395| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 66 | global_acc: 78.624% | global_loss: 21.37632942199707 | global_f1: 0.7706029254370317 | global_precision: 0.6375442739079102 | global_recall: 0.9738503155996393 | global_auc: 0.8252611240978713| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 67 | global_acc: 78.225% | global_loss: 21.775266647338867 | global_f1: 0.7679773290825362 | global_precision: 0.632438739789965 | global_recall: 0.9774571686203787 | global_auc: 0.8228518070590045| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 68 | global_acc: 78.956% | global_loss: 21.043882369995117 | global_f1: 0.7738478027867096 | global_precision: 0.6408284023668639 | global_recall: 0.9765554553651938 | global_auc: 0.8284505964175535| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 69 | global_acc: 79.322% | global_loss: 20.678192138671875 | global_f1: 0.7773801002147458 | global_precision: 0.6445103857566765 | global_recall: 0.9792605951307484 | global_auc: 0.831910250328705| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 70 | global_acc: 79.422% | global_loss: 20.57845687866211 | global_f1: 0.7782156932998925 | global_precision: 0.6456599286563615 | global_recall: 0.9792605951307484 | global_auc: 0.8327001397441869| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 71 | global_acc: 79.987% | global_loss: 19.947561264038086 | global_f1: 0.7831412103746398 | global_precision: 0.6520695860827834 | global_recall: 0.9801623083859333 | global_auc: 0.8378801238941667| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 72 | global_acc: 81.350% | global_loss: 18.650266647338867 | global_f1: 0.7939772309952259 | global_precision: 0.6697645600991325 | global_recall: 0.9747520288548241 | global_auc: 0.8470389949434731| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 73 | global_acc: 81.749% | global_loss: 18.25132942199707 | global_f1: 0.7968923418423973 | global_precision: 0.6756587202007528 | global_recall: 0.9711451758340848 | global_auc: 0.8494483119823399| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 74 | global_acc: 82.613% | global_loss: 17.3869686126709 | global_f1: 0.8044859813084113 | global_precision: 0.6871008939974457 | global_recall: 0.9702434625788999 | global_auc: 0.856106460094084| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 75 | global_acc: 82.779% | global_loss: 17.220745086669922 | global_f1: 0.8054094665664913 | global_precision: 0.6902768834513844 | global_recall: 0.9666366095581606 | global_auc: 0.8566727018301599| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 76 | global_acc: 83.278% | global_loss: 16.722074508666992 | global_f1: 0.8091081593927894 | global_precision: 0.6985583224115334 | global_recall: 0.9612263300270514 | global_auc: 0.859496787972978| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 77 | global_acc: 83.344% | global_loss: 16.65558433532715 | global_f1: 0.8092881614008375 | global_precision: 0.7002635046113307 | global_recall: 0.9585211902614968 | global_auc: 0.8594497792250775| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 78 | global_acc: 84.441% | global_loss: 15.55321216583252 | global_f1: 0.8177570093457945 | global_precision: 0.7196710075394106 | global_recall: 0.9467989179440938 | global_auc: 0.8660649546935386| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 79 | global_acc: 84.840% | global_loss: 15.159574508666992 | global_f1: 0.8201892744479495 | global_precision: 0.728801681850035 | global_recall: 0.9377817853922452 | global_auc: 0.8669951580989662| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 80 | global_acc: 87.666% | global_loss: 12.333776473999023 | global_f1: 0.8472622478386167 | global_precision: 0.7795454545454545 | global_recall: 0.9278629395852119 | global_auc: 0.8873121964908682| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 81 | global_acc: 88.098% | global_loss: 11.791452407836914 | global_f1: 0.8507089241034196 | global_precision: 0.791311093871218 | global_recall: 0.9197475202885482 | global_auc: 0.8905949740525956| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 82 | global_acc: 88.132% | global_loss: 11.868350982666016 | global_f1: 0.8501888375996645 | global_precision: 0.7951334379905809 | global_recall: 0.9134355275022543 | global_auc: 0.8879973846042077| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 83 | global_acc: 86.868% | global_loss: 13.131649017333984 | global_f1: 0.8309798887462558 | global_precision: 0.7907166123778502 | global_recall: 0.8755635707844905 | global_auc: 0.8701145921326349| flobal_FPR: 0.12443642921550947 \n",
      "comm_round: 84 | global_acc: 87.068% | global_loss: 12.932180404663086 | global_f1: 0.8319654427645787 | global_precision: 0.7985074626865671 | global_recall: 0.8683498647430117 | global_auc: 0.8701938897174775| flobal_FPR: 0.13165013525698827 \n",
      "comm_round: 85 | global_acc: 87.001% | global_loss: 12.99867057800293 | global_f1: 0.8293321693583589 | global_precision: 0.8037225042301185 | global_recall: 0.8566275924256087 | global_auc: 0.8672290147488759| flobal_FPR: 0.14337240757439135 \n",
      "comm_round: 86 | global_acc: 86.503% | global_loss: 13.497340202331543 | global_f1: 0.8214599824098504 | global_precision: 0.8017167381974248 | global_recall: 0.842200180342651 | global_auc: 0.8602786051792244| flobal_FPR: 0.15779981965734896 \n",
      "comm_round: 87 | global_acc: 87.068% | global_loss: 12.932180404663086 | global_f1: 0.8323998276604913 | global_precision: 0.7970297029702971 | global_recall: 0.8710550045085663 | global_auc: 0.870756570184773| flobal_FPR: 0.12894499549143373 \n",
      "comm_round: 88 | global_acc: 87.201% | global_loss: 12.799201965332031 | global_f1: 0.8346930012881065 | global_precision: 0.7967213114754098 | global_recall: 0.8764652840396754 | global_auc: 0.8729351170066728| flobal_FPR: 0.12353471596032461 \n",
      "comm_round: 89 | global_acc: 87.267% | global_loss: 12.732712745666504 | global_f1: 0.8356928356928356 | global_precision: 0.7970540098199672 | global_recall: 0.8782687105500451 | global_auc: 0.8738368302618578| flobal_FPR: 0.12173128944995491 \n",
      "comm_round: 90 | global_acc: 87.267% | global_loss: 12.732712745666504 | global_f1: 0.8362548097477553 | global_precision: 0.7951219512195122 | global_recall: 0.8818755635707844 | global_auc: 0.8745870708849183| flobal_FPR: 0.11812443642921551 \n",
      "comm_round: 91 | global_acc: 87.101% | global_loss: 12.89893627166748 | global_f1: 0.8347529812606473 | global_precision: 0.7909604519774012 | global_recall: 0.8836789900811542 | global_auc: 0.8736457088373122| flobal_FPR: 0.11632100991884581 \n",
      "comm_round: 92 | global_acc: 87.001% | global_loss: 12.99867057800293 | global_f1: 0.8335461898680291 | global_precision: 0.7895161290322581 | global_recall: 0.8827772768259693 | global_auc: 0.8726682592660652| flobal_FPR: 0.11722272317403065 \n",
      "comm_round: 93 | global_acc: 86.935% | global_loss: 13.065159797668457 | global_f1: 0.8332626219770896 | global_precision: 0.7868589743589743 | global_recall: 0.8854824165915239 | global_auc: 0.8727043467897062| flobal_FPR: 0.1145175834084761 \n",
      "comm_round: 94 | global_acc: 86.336% | global_loss: 13.66356372833252 | global_f1: 0.8269473684210525 | global_precision: 0.7756714060031595 | global_recall: 0.8854824165915239 | global_auc: 0.8679650102968152| flobal_FPR: 0.1145175834084761 \n",
      "comm_round: 95 | global_acc: 85.239% | global_loss: 14.760638236999512 | global_f1: 0.8107416879795397 | global_precision: 0.7687954729183508 | global_recall: 0.8575293056807936 | global_auc: 0.8534618618977954| flobal_FPR: 0.1424706943192065 \n",
      "comm_round: 96 | global_acc: 85.173% | global_loss: 14.827127456665039 | global_f1: 0.8098891730605284 | global_precision: 0.7679870654810024 | global_recall: 0.8566275924256087 | global_auc: 0.8527477087983756| flobal_FPR: 0.14337240757439135 \n",
      "comm_round: 97 | global_acc: 84.973% | global_loss: 15.026596069335938 | global_f1: 0.8078231292517006 | global_precision: 0.7642799678197908 | global_recall: 0.8566275924256087 | global_auc: 0.851167929967412| flobal_FPR: 0.14337240757439135 \n",
      "comm_round: 98 | global_acc: 84.608% | global_loss: 15.392287254333496 | global_f1: 0.8030625265844321 | global_precision: 0.7600644122383253 | global_recall: 0.8512173128944995 | global_auc: 0.847146307842721| flobal_FPR: 0.14878268710550044 \n",
      "comm_round: 99 | global_acc: 84.076% | global_loss: 15.924201965332031 | global_f1: 0.797463002114165 | global_precision: 0.7507961783439491 | global_recall: 0.8503155996393147 | global_auc: 0.8427460041377195| flobal_FPR: 0.1496844003606853 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "\n",
    "\n",
    "# def update_avg_with_delta(avg_grad, delta_x_hats):\n",
    "#     '''Update avg_grad by adding the corresponding elements from delta_x_hats'''\n",
    "#     for avg_layer, delta_layers in zip(avg_grad, zip(*delta_x_hats)):\n",
    "#         avg_layer += delta_layers\n",
    "#     return avg_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM-nonidd 2022|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "   \n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-nonidd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-nonidd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-nonidd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
