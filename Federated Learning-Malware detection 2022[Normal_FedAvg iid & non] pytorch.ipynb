{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0c136",
   "metadata": {},
   "source": [
    "## fedavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 40.359% | global_loss: 0.6962018013000488 | global_f1: 0.5526184538653366 | global_precision: 0.3819372630127542 | global_recall: 0.9990982867448152 | global_auc: 0.6766887892683302| flobal_FPR: 0.0009017132551848512 \n",
      "comm_round: 1 | global_acc: 68.185% | global_loss: 0.6814865469932556 | global_f1: 0.3227176220806794 | global_precision: 0.75 | global_recall: 0.20559062218214608 | global_auc: 0.7806201451003351| flobal_FPR: 0.7944093778178539 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6685284376144409 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8409563478666338| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6560763120651245 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8730056301285238| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6431750655174255 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8936904288764767| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.628635585308075 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9052897187119984| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6113445162773132 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9156226213692271| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5902359485626221 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9250082265308827| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5646877288818359 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9331359915593184| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5351866483688354 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.942961769542225| flobal_FPR: 1.0 \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e03d2",
   "metadata": {},
   "source": [
    "## fed avg non iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5726ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 744, (1,): 459}\n",
      "Client client_2: {(0,): 725, (1,): 478}\n",
      "Client client_3: {(0,): 130, (1,): 1073}\n",
      "Client client_4: {(0,): 627, (1,): 576}\n",
      "Client client_5: {(0,): 568, (1,): 635}\n",
      "Client client_6: {(0,): 704, (1,): 499}\n",
      "Client client_7: {(0,): 574, (1,): 629}\n",
      "Client client_8: {(0,): 311, (1,): 892}\n",
      "Client client_9: {(0,): 759, (1,): 444}\n",
      "Client client_10: {(0,): 345, (1,): 858}\n",
      "|=======================|\n",
      "|Traditional FedAvg non idd 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 36.902% | global_loss: 0.7078294157981873 | global_f1: 0.5388726919339164 | global_precision: 0.36880611905553706 | global_recall: 1.0 | global_auc: 0.633130910815858| flobal_FPR: 0.0 \n",
      "comm_round: 1 | global_acc: 36.902% | global_loss: 0.706152081489563 | global_f1: 0.5388726919339164 | global_precision: 0.36880611905553706 | global_recall: 1.0 | global_auc: 0.738484874816654| flobal_FPR: 0.0 \n",
      "comm_round: 2 | global_acc: 36.902% | global_loss: 0.7039317488670349 | global_f1: 0.5388726919339164 | global_precision: 0.36880611905553706 | global_recall: 1.0 | global_auc: 0.8069801817766553| flobal_FPR: 0.0 \n",
      "comm_round: 3 | global_acc: 37.101% | global_loss: 0.70112544298172 | global_f1: 0.5396593673965937 | global_precision: 0.36954348550483174 | global_recall: 1.0 | global_auc: 0.8511306553541776| flobal_FPR: 0.0 \n",
      "comm_round: 4 | global_acc: 37.467% | global_loss: 0.6974263191223145 | global_f1: 0.5411075872163942 | global_precision: 0.37090301003344484 | global_recall: 1.0 | global_auc: 0.8812965012670995| flobal_FPR: 0.0 \n",
      "comm_round: 5 | global_acc: 39.727% | global_loss: 0.6923655867576599 | global_f1: 0.5502356735301414 | global_precision: 0.3795345653661875 | global_recall: 1.0 | global_auc: 0.9007678570326273| flobal_FPR: 0.0 \n",
      "comm_round: 6 | global_acc: 45.412% | global_loss: 0.6856353282928467 | global_f1: 0.5746113989637306 | global_precision: 0.40312613595056346 | global_recall: 1.0 | global_auc: 0.9137588907075102| flobal_FPR: 0.0 \n",
      "comm_round: 7 | global_acc: 55.984% | global_loss: 0.6765925288200378 | global_f1: 0.6259887005649717 | global_precision: 0.455779514603044 | global_recall: 0.9990982867448152 | global_auc: 0.9232831479336805| flobal_FPR: 0.0009017132551848512 \n",
      "comm_round: 8 | global_acc: 63.497% | global_loss: 0.6642109751701355 | global_f1: 0.6684782608695653 | global_precision: 0.5024965955515207 | global_recall: 0.9981965734896303 | global_auc: 0.9317342761673721| flobal_FPR: 0.0018034265103697023 \n",
      "comm_round: 9 | global_acc: 67.553% | global_loss: 0.6473511457443237 | global_f1: 0.6923076923076923 | global_precision: 0.5322346097915657 | global_recall: 0.9900811541929666 | global_auc: 0.9396255729487922| flobal_FPR: 0.009918845807033363 \n",
      "comm_round: 10 | global_acc: 70.711% | global_loss: 0.6246548295021057 | global_f1: 0.7125611745513867 | global_precision: 0.558282208588957 | global_recall: 0.9846708746618575 | global_auc: 0.9483506814606519| flobal_FPR: 0.015329125338142471 \n",
      "comm_round: 11 | global_acc: 73.471% | global_loss: 0.5942680835723877 | global_f1: 0.7322147651006712 | global_precision: 0.5831106360235169 | global_recall: 0.9837691614066727 | global_auc: 0.9570012407460431| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 12 | global_acc: 76.330% | global_loss: 0.5547135472297668 | global_f1: 0.753120665742025 | global_precision: 0.611830985915493 | global_recall: 0.9792605951307484 | global_auc: 0.9658832825021568| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 13 | global_acc: 79.156% | global_loss: 0.5062374472618103 | global_f1: 0.775831247765463 | global_precision: 0.6427725118483413 | global_recall: 0.9783588818755635 | global_auc: 0.9725473660618682| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 14 | global_acc: 81.649% | global_loss: 0.4498414695262909 | global_f1: 0.7967599410898379 | global_precision: 0.6733042937149969 | global_recall: 0.975653742110009 | global_auc: 0.9776114902675272| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 15 | global_acc: 83.943% | global_loss: 0.391110897064209 | global_f1: 0.8169761273209549 | global_precision: 0.7045751633986929 | global_recall: 0.9720468890892696 | global_auc: 0.9809172973673677| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 16 | global_acc: 89.761% | global_loss: 0.334440141916275 | global_f1: 0.8743882544861338 | global_precision: 0.7982129560685034 | global_recall: 0.9666366095581606 | global_auc: 0.983108664756877| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 17 | global_acc: 91.323% | global_loss: 0.2867991626262665 | global_f1: 0.8911138923654569 | global_precision: 0.8291925465838509 | global_recall: 0.9630297565374211 | global_auc: 0.9844619468934104| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 18 | global_acc: 92.553% | global_loss: 0.24840785562992096 | global_f1: 0.9047619047619048 | global_precision: 0.8559935639581657 | global_recall: 0.9594229035166817 | global_auc: 0.9855797104546031| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 19 | global_acc: 93.584% | global_loss: 0.21906690299510956 | global_f1: 0.9166306695464363 | global_precision: 0.8797678275290216 | global_recall: 0.9567177637511272 | global_auc: 0.9864847475606496| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 20 | global_acc: 94.182% | global_loss: 0.19611066579818726 | global_f1: 0.9238136700043534 | global_precision: 0.8930976430976431 | global_recall: 0.9567177637511272 | global_auc: 0.9873631938598028| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 21 | global_acc: 94.781% | global_loss: 0.1796102523803711 | global_f1: 0.9311705392371767 | global_precision: 0.9061433447098977 | global_recall: 0.957619477006312 | global_auc: 0.9882649071149876| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 22 | global_acc: 94.980% | global_loss: 0.16692779958248138 | global_f1: 0.9336846728151077 | global_precision: 0.9101027397260274 | global_recall: 0.9585211902614968 | global_auc: 0.9889586422734| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 23 | global_acc: 95.213% | global_loss: 0.15594616532325745 | global_f1: 0.9365079365079365 | global_precision: 0.9163071613459879 | global_recall: 0.957619477006312 | global_auc: 0.9894980557846639| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 24 | global_acc: 95.379% | global_loss: 0.14868687093257904 | global_f1: 0.9386854874283194 | global_precision: 0.918825561312608 | global_recall: 0.9594229035166817 | global_auc: 0.9900184758624325| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 25 | global_acc: 95.545% | global_loss: 0.14202797412872314 | global_f1: 0.940760389036251 | global_precision: 0.9228100607111882 | global_recall: 0.9594229035166817 | global_auc: 0.99049378653565| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 26 | global_acc: 95.578% | global_loss: 0.136518657207489 | global_f1: 0.9412284577993814 | global_precision: 0.9228769497400346 | global_recall: 0.9603246167718665 | global_auc: 0.9908731803697166| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 27 | global_acc: 95.578% | global_loss: 0.13273267447948456 | global_f1: 0.941280353200883 | global_precision: 0.9221453287197232 | global_recall: 0.9612263300270514 | global_auc: 0.9912658696072301| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 28 | global_acc: 95.612% | global_loss: 0.1284486949443817 | global_f1: 0.9416961130742049 | global_precision: 0.922943722943723 | global_recall: 0.9612263300270514 | global_auc: 0.9915735632298522| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 29 | global_acc: 95.911% | global_loss: 0.12458322197198868 | global_f1: 0.9457432730480813 | global_precision: 0.92573402417962 | global_recall: 0.9666366095581606 | global_auc: 0.9918522918663945| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 30 | global_acc: 95.944% | global_loss: 0.12200765311717987 | global_f1: 0.9461130742049468 | global_precision: 0.9272727272727272 | global_recall: 0.9657348963029756 | global_auc: 0.9920873356058977| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 31 | global_acc: 96.110% | global_loss: 0.11835364997386932 | global_f1: 0.9482529854046883 | global_precision: 0.9305555555555556 | global_recall: 0.9666366095581606 | global_auc: 0.9922758454333376| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 32 | global_acc: 96.077% | global_loss: 0.11704377830028534 | global_f1: 0.9478337754199824 | global_precision: 0.9297484822202949 | global_recall: 0.9666366095581606 | global_auc: 0.9924681539474766| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 33 | global_acc: 96.144% | global_loss: 0.11508864909410477 | global_f1: 0.9487632508833922 | global_precision: 0.9298701298701298 | global_recall: 0.9684400360685302 | global_auc: 0.9926115543703654| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 34 | global_acc: 96.110% | global_loss: 0.11365700513124466 | global_f1: 0.9483443708609273 | global_precision: 0.9290657439446367 | global_recall: 0.9684400360685302 | global_auc: 0.9927682501967008| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 35 | global_acc: 96.110% | global_loss: 0.11133191734552383 | global_f1: 0.9482987185152453 | global_precision: 0.9298093587521664 | global_recall: 0.9675383228133454 | global_auc: 0.992866066379201| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 36 | global_acc: 96.110% | global_loss: 0.1102755144238472 | global_f1: 0.9483443708609273 | global_precision: 0.9290657439446367 | global_recall: 0.9684400360685302 | global_auc: 0.9929994952495049| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 37 | global_acc: 96.110% | global_loss: 0.10929553955793381 | global_f1: 0.9483443708609273 | global_precision: 0.9290657439446367 | global_recall: 0.9684400360685302 | global_auc: 0.9930863902077454| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 38 | global_acc: 96.310% | global_loss: 0.10757344961166382 | global_f1: 0.950863213811421 | global_precision: 0.9339130434782609 | global_recall: 0.9684400360685302 | global_auc: 0.9931737600018233| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 39 | global_acc: 96.277% | global_loss: 0.10720973461866379 | global_f1: 0.9504862953138815 | global_precision: 0.9323503902862099 | global_recall: 0.9693417493237151 | global_auc: 0.9932236177647483| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 40 | global_acc: 96.310% | global_loss: 0.1060018464922905 | global_f1: 0.950906678460858 | global_precision: 0.9331597222222222 | global_recall: 0.9693417493237151 | global_auc: 0.9932772742143722| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 41 | global_acc: 96.376% | global_loss: 0.10492154210805893 | global_f1: 0.9517912428129147 | global_precision: 0.9340277777777778 | global_recall: 0.9702434625788999 | global_auc: 0.9933076637079645| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 42 | global_acc: 96.310% | global_loss: 0.10570184141397476 | global_f1: 0.9509933774834437 | global_precision: 0.9316608996539792 | global_recall: 0.9711451758340848 | global_auc: 0.9933636943367754| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 43 | global_acc: 96.376% | global_loss: 0.10519726574420929 | global_f1: 0.9518338488731772 | global_precision: 0.9332755632582322 | global_recall: 0.9711451758340848 | global_auc: 0.9934353945482198| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 44 | global_acc: 96.376% | global_loss: 0.10515226423740387 | global_f1: 0.9518338488731772 | global_precision: 0.9332755632582322 | global_recall: 0.9711451758340848 | global_auc: 0.9934572369967394| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 45 | global_acc: 96.310% | global_loss: 0.10455305874347687 | global_f1: 0.9509933774834437 | global_precision: 0.9316608996539792 | global_recall: 0.9711451758340848 | global_auc: 0.9934662588776494| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 46 | global_acc: 96.310% | global_loss: 0.10424243658781052 | global_f1: 0.9509933774834437 | global_precision: 0.9316608996539792 | global_recall: 0.9711451758340848 | global_auc: 0.9935075695955015| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 47 | global_acc: 96.343% | global_loss: 0.10441502928733826 | global_f1: 0.9514563106796117 | global_precision: 0.9317199654278306 | global_recall: 0.9720468890892696 | global_auc: 0.9935332107307201| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 48 | global_acc: 96.277% | global_loss: 0.1043589860200882 | global_f1: 0.9505736981465137 | global_precision: 0.9308556611927399 | global_recall: 0.9711451758340848 | global_auc: 0.9935384339249312| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 49 | global_acc: 96.376% | global_loss: 0.10495153069496155 | global_f1: 0.9519188354653728 | global_precision: 0.9317789291882557 | global_recall: 0.9729486023444545 | global_auc: 0.9935802194786209| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 50 | global_acc: 96.443% | global_loss: 0.10419739782810211 | global_f1: 0.9527593818984548 | global_precision: 0.9333910034602076 | global_recall: 0.9729486023444545 | global_auc: 0.9935939897179047| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 51 | global_acc: 96.476% | global_loss: 0.1039276123046875 | global_f1: 0.9531802120141343 | global_precision: 0.9341991341991343 | global_recall: 0.9729486023444545 | global_auc: 0.9936262785548466| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 52 | global_acc: 96.509% | global_loss: 0.10301654040813446 | global_f1: 0.9536423841059603 | global_precision: 0.9342560553633218 | global_recall: 0.9738503155996393 | global_auc: 0.9935859175086694| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 53 | global_acc: 96.509% | global_loss: 0.10413054376840591 | global_f1: 0.9536423841059603 | global_precision: 0.9342560553633218 | global_recall: 0.9738503155996393 | global_auc: 0.9936224798681476| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 54 | global_acc: 96.509% | global_loss: 0.1049300879240036 | global_f1: 0.9536423841059603 | global_precision: 0.9342560553633218 | global_recall: 0.9738503155996393 | global_auc: 0.9935973135687663| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 55 | global_acc: 96.576% | global_loss: 0.10474628955125809 | global_f1: 0.9545655050727834 | global_precision: 0.9343696027633851 | global_recall: 0.975653742110009 | global_auc: 0.9935916155387179| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 56 | global_acc: 96.576% | global_loss: 0.1054798811674118 | global_f1: 0.9545655050727834 | global_precision: 0.9343696027633851 | global_recall: 0.975653742110009 | global_auc: 0.9936091844647009| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 57 | global_acc: 96.609% | global_loss: 0.10610342770814896 | global_f1: 0.9550264550264551 | global_precision: 0.9344262295081968 | global_recall: 0.9765554553651938 | global_auc: 0.9936196308531232| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 58 | global_acc: 96.609% | global_loss: 0.1052476316690445 | global_f1: 0.9549867608120034 | global_precision: 0.9351771823681936 | global_recall: 0.975653742110009 | global_auc: 0.9936082347930262| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 59 | global_acc: 96.676% | global_loss: 0.10559171438217163 | global_f1: 0.9558303886925795 | global_precision: 0.9367965367965368 | global_recall: 0.975653742110009 | global_auc: 0.993610608972213| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 60 | global_acc: 96.709% | global_loss: 0.10549972206354141 | global_f1: 0.956252761820592 | global_precision: 0.9376083188908145 | global_recall: 0.975653742110009 | global_auc: 0.9936077599571888| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 61 | global_acc: 96.676% | global_loss: 0.10656608641147614 | global_f1: 0.9558693733451016 | global_precision: 0.9360414866032843 | global_recall: 0.9765554553651938 | global_auc: 0.9936089470467822| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 62 | global_acc: 96.676% | global_loss: 0.10619890689849854 | global_f1: 0.9559082892416225 | global_precision: 0.9352890422778257 | global_recall: 0.9774571686203787 | global_auc: 0.9935465061341666| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 63 | global_acc: 96.676% | global_loss: 0.10828874260187149 | global_f1: 0.9559859154929577 | global_precision: 0.9337919174548581 | global_recall: 0.9792605951307484 | global_auc: 0.993583068493645| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 64 | global_acc: 96.676% | global_loss: 0.10900968313217163 | global_f1: 0.9559859154929577 | global_precision: 0.9337919174548581 | global_recall: 0.9792605951307484 | global_auc: 0.9935825936578077| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 65 | global_acc: 96.775% | global_loss: 0.10772110521793365 | global_f1: 0.9572498898193037 | global_precision: 0.9362068965517242 | global_recall: 0.9792605951307484 | global_auc: 0.9935294120440211| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 66 | global_acc: 96.742% | global_loss: 0.108589306473732 | global_f1: 0.9568281938325991 | global_precision: 0.9354005167958657 | global_recall: 0.9792605951307484 | global_auc: 0.9935389087607686| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 67 | global_acc: 96.775% | global_loss: 0.10871297866106033 | global_f1: 0.9572498898193037 | global_precision: 0.9362068965517242 | global_recall: 0.9792605951307484 | global_auc: 0.9935070947596643| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 68 | global_acc: 96.676% | global_loss: 0.10995128005743027 | global_f1: 0.9559859154929577 | global_precision: 0.9337919174548581 | global_recall: 0.9792605951307484 | global_auc: 0.9935503048208658| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 69 | global_acc: 96.742% | global_loss: 0.10944189876317978 | global_f1: 0.9568281938325991 | global_precision: 0.9354005167958657 | global_recall: 0.9792605951307484 | global_auc: 0.9934866768186569| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 70 | global_acc: 96.642% | global_loss: 0.11043398082256317 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9935066199238269| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 71 | global_acc: 96.642% | global_loss: 0.11125624179840088 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9935042457446399| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 72 | global_acc: 96.642% | global_loss: 0.11234768480062485 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9935127927897128| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 73 | global_acc: 96.642% | global_loss: 0.11202023923397064 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9934971232070792| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 74 | global_acc: 96.642% | global_loss: 0.11240611970424652 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9934847774753073| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 75 | global_acc: 96.642% | global_loss: 0.11277736723423004 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9934600860117635| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 76 | global_acc: 96.642% | global_loss: 0.1135297566652298 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9934598485938448| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 77 | global_acc: 96.642% | global_loss: 0.11437154561281204 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9934622227730319| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 78 | global_acc: 96.676% | global_loss: 0.11378709971904755 | global_f1: 0.9559471365638768 | global_precision: 0.9345391903531438 | global_recall: 0.9783588818755635 | global_auc: 0.993434444876545| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 79 | global_acc: 96.676% | global_loss: 0.11512736231088638 | global_f1: 0.9559859154929577 | global_precision: 0.9337919174548581 | global_recall: 0.9792605951307484 | global_auc: 0.993420674637261| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 80 | global_acc: 96.676% | global_loss: 0.1167031079530716 | global_f1: 0.9560246262093228 | global_precision: 0.9330472103004291 | global_recall: 0.9801623083859333 | global_auc: 0.9934453661008047| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 81 | global_acc: 96.642% | global_loss: 0.11649235337972641 | global_f1: 0.9555653321601407 | global_precision: 0.9329896907216495 | global_recall: 0.9792605951307484 | global_auc: 0.9934064295621396| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 82 | global_acc: 96.676% | global_loss: 0.11613951623439789 | global_f1: 0.9559471365638768 | global_precision: 0.9345391903531438 | global_recall: 0.9783588818755635 | global_auc: 0.9933670181876371| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 83 | global_acc: 96.709% | global_loss: 0.11723539978265762 | global_f1: 0.9564452265728113 | global_precision: 0.9338487972508591 | global_recall: 0.9801623083859333 | global_auc: 0.9933955083378798| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 84 | global_acc: 96.676% | global_loss: 0.11842791736125946 | global_f1: 0.9560246262093228 | global_precision: 0.9330472103004291 | global_recall: 0.9801623083859333 | global_auc: 0.9934145017713751| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 85 | global_acc: 96.676% | global_loss: 0.11889674514532089 | global_f1: 0.9560246262093228 | global_precision: 0.9330472103004291 | global_recall: 0.9801623083859333 | global_auc: 0.9933895728899127| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 86 | global_acc: 96.676% | global_loss: 0.12042483687400818 | global_f1: 0.9560246262093228 | global_precision: 0.9330472103004291 | global_recall: 0.9801623083859333 | global_auc: 0.9934083289054891| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 87 | global_acc: 96.576% | global_loss: 0.11940746009349823 | global_f1: 0.9546854377474704 | global_precision: 0.9321305841924399 | global_recall: 0.9783588818755635 | global_auc: 0.9933404273807437| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 88 | global_acc: 96.609% | global_loss: 0.1206435114145279 | global_f1: 0.9551451187335093 | global_precision: 0.9321888412017167 | global_recall: 0.9792605951307484 | global_auc: 0.9933598956500764| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 89 | global_acc: 96.609% | global_loss: 0.11986172199249268 | global_f1: 0.9551056338028169 | global_precision: 0.9329320722269991 | global_recall: 0.9783588818755635 | global_auc: 0.9933116998125823| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 90 | global_acc: 96.576% | global_loss: 0.12185484170913696 | global_f1: 0.9547252747252747 | global_precision: 0.9313893653516295 | global_recall: 0.9792605951307484 | global_auc: 0.9933235707085168| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 91 | global_acc: 96.576% | global_loss: 0.12194503843784332 | global_f1: 0.9546854377474704 | global_precision: 0.9321305841924399 | global_recall: 0.9783588818755635 | global_auc: 0.9933128869021757| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 92 | global_acc: 96.609% | global_loss: 0.12294811755418777 | global_f1: 0.9551451187335093 | global_precision: 0.9321888412017167 | global_recall: 0.9792605951307484 | global_auc: 0.9933100378871514| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 93 | global_acc: 96.576% | global_loss: 0.12311922013759613 | global_f1: 0.9546854377474704 | global_precision: 0.9321305841924399 | global_recall: 0.9783588818755635 | global_auc: 0.9933048146929404| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 94 | global_acc: 96.576% | global_loss: 0.12321855872869492 | global_f1: 0.9546854377474704 | global_precision: 0.9321305841924399 | global_recall: 0.9783588818755635 | global_auc: 0.9932867709311198| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 95 | global_acc: 96.576% | global_loss: 0.12318162620067596 | global_f1: 0.9546854377474704 | global_precision: 0.9321305841924399 | global_recall: 0.9783588818755635 | global_auc: 0.993223855182667| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 96 | global_acc: 96.509% | global_loss: 0.12512053549289703 | global_f1: 0.9538461538461538 | global_precision: 0.9305317324185248 | global_recall: 0.9783588818755635 | global_auc: 0.993298166991217| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 97 | global_acc: 96.509% | global_loss: 0.12532757222652435 | global_f1: 0.9538461538461538 | global_precision: 0.9305317324185248 | global_recall: 0.9783588818755635 | global_auc: 0.9932504459895602| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 98 | global_acc: 96.509% | global_loss: 0.12686926126480103 | global_f1: 0.9538866930171277 | global_precision: 0.9297945205479452 | global_recall: 0.9792605951307484 | global_auc: 0.9932521079149912| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 99 | global_acc: 96.509% | global_loss: 0.12736137211322784 | global_f1: 0.9538866930171277 | global_precision: 0.9297945205479452 | global_recall: 0.9792605951307484 | global_auc: 0.9932454602132678| flobal_FPR: 0.020739404869251576 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg non idd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-noniid-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-noniid-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-noniid-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "454f087e",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717923bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "client_3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TensorDataset' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 160\u001b[0m\n\u001b[0;32m    158\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(local_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[39mprint\u001b[39m(client)\n\u001b[1;32m--> 160\u001b[0m clients_batched[client] \u001b[39m=\u001b[39m add_backdoor_attack(clients_batched, backdoor_pattern, target_client,client)\n\u001b[0;32m    162\u001b[0m \u001b[39m# fit local model with client's data\u001b[39;00m\n\u001b[0;32m    163\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(TensorDataset(torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    164\u001b[0m                                         torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)),\n\u001b[0;32m    165\u001b[0m                           batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36madd_backdoor_attack\u001b[1;34m(client_data, backdoor_pattern, target_client, client)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39m# Modify the last few features to be the backdoor pattern\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     x[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(backdoor_pattern):] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(backdoor_pattern, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 48\u001b[0m     target_client_data[idx] \u001b[39m=\u001b[39m (x, y)\n\u001b[0;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m client_data\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TensorDataset' object does not support item assignment"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for i in range(3):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def add_backdoor_attack(client_data, backdoor_pattern, target_client,client):\n",
    "    \"\"\"\n",
    "    Add a backdoor attack to the specified client's data.\n",
    "    \"\"\"\n",
    "    # if target_client not in client_data:\n",
    "    #     # Target client not found, no backdoor attack applied\n",
    "    #     return client_data\n",
    "\n",
    "    # Access the DataLoader object\n",
    "    data_loader = client_data[client]\n",
    "\n",
    "    # Extract the dataset from the DataLoader\n",
    "    target_client_data = data_loader.dataset\n",
    "\n",
    "    data_size = len(target_client_data)\n",
    "    num_poisoned = int(data_size * 0.3)  # Adjust the fraction of data to be poisoned\n",
    "\n",
    "    # Randomly select indices for poisoning\n",
    "    poisoned_indices = random.sample(range(data_size), num_poisoned)\n",
    "\n",
    "    for idx in poisoned_indices:\n",
    "        x, y = target_client_data[idx]\n",
    "        # Modify the last few features to be the backdoor pattern\n",
    "        x[-len(backdoor_pattern):] = torch.tensor(backdoor_pattern, dtype=torch.float32)\n",
    "        target_client_data[idx] = (x, y)\n",
    "\n",
    "    return client_data\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            # Example backdoor pattern for 215 features\n",
    "            backdoor_pattern = [0.0] * 215  # Initialize with zeros, modify based on your data\n",
    "\n",
    "            # Apply backdoor attack only to client3\n",
    "            target_client = 'client_3'\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "                    print(client)\n",
    "                    clients_batched[client] = add_backdoor_attack(clients_batched, backdoor_pattern, target_client,client)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d12132d3",
   "metadata": {},
   "source": [
    "## test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6731710433959961 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.578072270964121| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6630347967147827 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7462033313532679| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6529568433761597 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8392108513284244| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6420845985412598 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8822449858522662| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6295410394668579 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9057987427296698| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6144066452980042 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.920006543237839| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5955983400344849 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9308007489110827| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5723247528076172 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9415467587468322| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5441925525665283 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.95127994374145| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.198% | global_loss: 0.5123705863952637 | global_f1: 0.0036003600360036 | global_precision: 1.0 | global_recall: 0.0018034265103697023 | global_auc: 0.9602187283801307| flobal_FPR: 0.9981965734896303 \n",
      "comm_round: 10 | global_acc: 69.082% | global_loss: 0.4781278371810913 | global_f1: 0.2779503105590062 | global_precision: 1.0 | global_recall: 0.16140667267808836 | global_auc: 0.9680991039372911| flobal_FPR: 0.8385933273219116 \n",
      "comm_round: 11 | global_acc: 80.519% | global_loss: 0.442536324262619 | global_f1: 0.6413708690330479 | global_precision: 0.9980952380952381 | global_recall: 0.47249774571686204 | global_auc: 0.974582037625042| flobal_FPR: 0.527502254283138 \n",
      "comm_round: 12 | global_acc: 88.265% | global_loss: 0.40636590123176575 | global_f1: 0.811532301121196 | global_precision: 0.9947643979057592 | global_recall: 0.6853020739404869 | global_auc: 0.9787876586367178| flobal_FPR: 0.3146979260595131 \n",
      "comm_round: 13 | global_acc: 91.689% | global_loss: 0.36935368180274963 | global_f1: 0.8738647830474269 | global_precision: 0.9919816723940436 | global_recall: 0.7808836789900812 | global_auc: 0.9816428465268845| flobal_FPR: 0.21911632100991885 \n",
      "comm_round: 14 | global_acc: 93.551% | global_loss: 0.33212196826934814 | global_f1: 0.9053658536585365 | global_precision: 0.9861849096705633 | global_recall: 0.8367899008115419 | global_auc: 0.9836419054022547| flobal_FPR: 0.16321009918845808 \n",
      "comm_round: 15 | global_acc: 94.481% | global_loss: 0.2952589988708496 | global_f1: 0.9205741626794257 | global_precision: 0.9806320081549439 | global_recall: 0.8674481514878268 | global_auc: 0.9851138964981331| flobal_FPR: 0.13255184851217314 \n",
      "comm_round: 16 | global_acc: 94.448% | global_loss: 0.260088175535202 | global_f1: 0.9210401891252955 | global_precision: 0.9681908548707754 | global_recall: 0.8782687105500451 | global_auc: 0.9860721152179662| flobal_FPR: 0.12173128944995491 \n",
      "comm_round: 17 | global_acc: 94.781% | global_loss: 0.2285812646150589 | global_f1: 0.9267382174521699 | global_precision: 0.960348162475822 | global_recall: 0.8954012623985572 | global_auc: 0.9868157081393035| flobal_FPR: 0.10459873760144274 \n",
      "comm_round: 18 | global_acc: 94.681% | global_loss: 0.2019943743944168 | global_f1: 0.9258572752548657 | global_precision: 0.9523355576739753 | global_recall: 0.9008115419296664 | global_auc: 0.9876314761079226| flobal_FPR: 0.09918845807033363 \n",
      "comm_round: 19 | global_acc: 94.781% | global_loss: 0.18056508898735046 | global_f1: 0.9275496077526534 | global_precision: 0.9499054820415879 | global_recall: 0.9062218214607755 | global_auc: 0.9883275854455218| flobal_FPR: 0.09377817853922453 \n",
      "comm_round: 20 | global_acc: 94.880% | global_loss: 0.16364280879497528 | global_f1: 0.9291628334866606 | global_precision: 0.9483568075117371 | global_recall: 0.9107303877366997 | global_auc: 0.9890820995911189| flobal_FPR: 0.08926961226330027 \n",
      "comm_round: 21 | global_acc: 95.213% | global_loss: 0.15045762062072754 | global_f1: 0.9340659340659342 | global_precision: 0.9488372093023256 | global_recall: 0.9197475202885482 | global_auc: 0.9896670973427711| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 22 | global_acc: 95.445% | global_loss: 0.13998764753341675 | global_f1: 0.9374143444495203 | global_precision: 0.95 | global_recall: 0.9251577998196574 | global_auc: 0.990190366435564| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 23 | global_acc: 95.711% | global_loss: 0.13155335187911987 | global_f1: 0.9412835685025034 | global_precision: 0.9503676470588235 | global_recall: 0.9323715058611362 | global_auc: 0.9906898937364879| flobal_FPR: 0.06762849413886383 \n",
      "comm_round: 24 | global_acc: 95.844% | global_loss: 0.1246597096323967 | global_f1: 0.9430523917995445 | global_precision: 0.9530386740331491 | global_recall: 0.933273219116321 | global_auc: 0.9911386136028121| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 25 | global_acc: 95.977% | global_loss: 0.11896110326051712 | global_f1: 0.9449248975876195 | global_precision: 0.9540441176470589 | global_recall: 0.9359783588818755 | global_auc: 0.9915118345709929| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 26 | global_acc: 96.210% | global_loss: 0.11414233595132828 | global_f1: 0.9479452054794519 | global_precision: 0.9602220166512488 | global_recall: 0.9359783588818755 | global_auc: 0.9918285500745255| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 27 | global_acc: 96.476% | global_loss: 0.11008764058351517 | global_f1: 0.951686417502279 | global_precision: 0.9622119815668203 | global_recall: 0.9413886384129847 | global_auc: 0.9921077535469051| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 28 | global_acc: 96.676% | global_loss: 0.10659733414649963 | global_f1: 0.9544211485870556 | global_precision: 0.9649769585253456 | global_recall: 0.9440937781785392 | global_auc: 0.9923774603025369| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 29 | global_acc: 96.742% | global_loss: 0.10357032716274261 | global_f1: 0.9553734061930783 | global_precision: 0.9650413983440662 | global_recall: 0.9458972046889089 | global_auc: 0.9925806900409355| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 30 | global_acc: 96.842% | global_loss: 0.10098714381456375 | global_f1: 0.9567592171142466 | global_precision: 0.9659926470588235 | global_recall: 0.9477006311992786 | global_auc: 0.9927729985550746| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 31 | global_acc: 96.941% | global_loss: 0.09864866733551025 | global_f1: 0.9581056466302368 | global_precision: 0.9678012879484821 | global_recall: 0.9486023444544635 | global_auc: 0.992923521515524| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 32 | global_acc: 96.975% | global_loss: 0.09659814089536667 | global_f1: 0.9585043319653442 | global_precision: 0.9695571955719557 | global_recall: 0.9477006311992786 | global_auc: 0.993084490864396| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 33 | global_acc: 97.008% | global_loss: 0.09476790577173233 | global_f1: 0.958941605839416 | global_precision: 0.9704524469067405 | global_recall: 0.9477006311992786 | global_auc: 0.9932084230179522| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 34 | global_acc: 97.108% | global_loss: 0.09316080063581467 | global_f1: 0.9604005461993628 | global_precision: 0.9696691176470589 | global_recall: 0.951307484220018 | global_auc: 0.9933447009032802| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 35 | global_acc: 97.041% | global_loss: 0.09165091067552567 | global_f1: 0.9594533029612756 | global_precision: 0.9696132596685083 | global_recall: 0.9495040577096483 | global_auc: 0.9934757555943972| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 36 | global_acc: 97.108% | global_loss: 0.0902850404381752 | global_f1: 0.9604005461993628 | global_precision: 0.9696691176470589 | global_recall: 0.951307484220018 | global_auc: 0.9936177315097738| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 37 | global_acc: 97.074% | global_loss: 0.08900020271539688 | global_f1: 0.9599271402550091 | global_precision: 0.9696412143514259 | global_recall: 0.9504057709648331 | global_auc: 0.9936989284379658| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 38 | global_acc: 97.108% | global_loss: 0.08781343698501587 | global_f1: 0.9603644646924829 | global_precision: 0.9705340699815838 | global_recall: 0.9504057709648331 | global_auc: 0.9937915214262549| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 39 | global_acc: 97.074% | global_loss: 0.08675355464220047 | global_f1: 0.959890610756609 | global_precision: 0.9705069124423963 | global_recall: 0.9495040577096483 | global_auc: 0.9938613222943498| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 40 | global_acc: 97.174% | global_loss: 0.08578819781541824 | global_f1: 0.9613108784706418 | global_precision: 0.9705882352941176 | global_recall: 0.9522091974752029 | global_auc: 0.9939885782987676| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 41 | global_acc: 97.207% | global_loss: 0.08484888076782227 | global_f1: 0.9618181818181818 | global_precision: 0.9697525206232814 | global_recall: 0.9540126239855726 | global_auc: 0.9940711997344719| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 42 | global_acc: 97.307% | global_loss: 0.08398441225290298 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.9941675914094599| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 43 | global_acc: 97.307% | global_loss: 0.08309425413608551 | global_f1: 0.9631315430131998 | global_precision: 0.9724264705882353 | global_recall: 0.9540126239855726 | global_auc: 0.9942070027839625| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 44 | global_acc: 97.374% | global_loss: 0.08233524858951569 | global_f1: 0.964074579354252 | global_precision: 0.9724770642201835 | global_recall: 0.9558160504959423 | global_auc: 0.9942881997121544| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 45 | global_acc: 97.407% | global_loss: 0.0816195085644722 | global_f1: 0.9645454545454545 | global_precision: 0.9725022914757103 | global_recall: 0.9567177637511272 | global_auc: 0.9943594250877615| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 46 | global_acc: 97.407% | global_loss: 0.08091937750577927 | global_f1: 0.9645454545454545 | global_precision: 0.9725022914757103 | global_recall: 0.9567177637511272 | global_auc: 0.9944216285824583| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 47 | global_acc: 97.473% | global_loss: 0.08028078079223633 | global_f1: 0.9654545454545456 | global_precision: 0.9734188817598534 | global_recall: 0.957619477006312 | global_auc: 0.9944743353604074| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 48 | global_acc: 97.440% | global_loss: 0.07960642874240875 | global_f1: 0.9649840836743975 | global_precision: 0.9733944954128441 | global_recall: 0.9567177637511272 | global_auc: 0.9945184950932839| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 49 | global_acc: 97.507% | global_loss: 0.07902757823467255 | global_f1: 0.9659245797364834 | global_precision: 0.9734432234432234 | global_recall: 0.9585211902614968 | global_auc: 0.9945878211255413| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 50 | global_acc: 97.540% | global_loss: 0.07850415259599686 | global_f1: 0.966394187102634 | global_precision: 0.9734675205855444 | global_recall: 0.9594229035166817 | global_auc: 0.9946315060225804| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 51 | global_acc: 97.540% | global_loss: 0.07799335569143295 | global_f1: 0.966394187102634 | global_precision: 0.9734675205855444 | global_recall: 0.9594229035166817 | global_auc: 0.9946813637855051| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 52 | global_acc: 97.473% | global_loss: 0.07739027589559555 | global_f1: 0.9654545454545456 | global_precision: 0.9734188817598534 | global_recall: 0.957619477006312 | global_auc: 0.9947027313981874| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 53 | global_acc: 97.540% | global_loss: 0.07690922170877457 | global_f1: 0.966394187102634 | global_precision: 0.9734675205855444 | global_recall: 0.9594229035166817 | global_auc: 0.99474214277269| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 54 | global_acc: 97.540% | global_loss: 0.07643983513116837 | global_f1: 0.966394187102634 | global_precision: 0.9734675205855444 | global_recall: 0.9594229035166817 | global_auc: 0.9947877270130784| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 55 | global_acc: 97.573% | global_loss: 0.07604089379310608 | global_f1: 0.9668633681343622 | global_precision: 0.973491773308958 | global_recall: 0.9603246167718665 | global_auc: 0.9948451821494013| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 56 | global_acc: 97.573% | global_loss: 0.0756286159157753 | global_f1: 0.9668633681343622 | global_precision: 0.973491773308958 | global_recall: 0.9603246167718665 | global_auc: 0.9948936154048141| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 57 | global_acc: 97.640% | global_loss: 0.07520490884780884 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.9949187817041953| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 58 | global_acc: 97.640% | global_loss: 0.07482480257749557 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.9949553440636736| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 59 | global_acc: 97.606% | global_loss: 0.07442574948072433 | global_f1: 0.9673024523160764 | global_precision: 0.9743824336688015 | global_recall: 0.9603246167718665 | global_auc: 0.9949871580647781| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 60 | global_acc: 97.606% | global_loss: 0.07407892495393753 | global_f1: 0.9673024523160764 | global_precision: 0.9743824336688015 | global_recall: 0.9603246167718665 | global_auc: 0.9950151733791835| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 61 | global_acc: 97.606% | global_loss: 0.07375457882881165 | global_f1: 0.9673024523160764 | global_precision: 0.9743824336688015 | global_recall: 0.9603246167718665 | global_auc: 0.9950474622161254| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 62 | global_acc: 97.606% | global_loss: 0.0734347403049469 | global_f1: 0.9673024523160764 | global_precision: 0.9743824336688015 | global_recall: 0.9603246167718665 | global_auc: 0.9950754775305308| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 63 | global_acc: 97.606% | global_loss: 0.07313656061887741 | global_f1: 0.9673024523160764 | global_precision: 0.9743824336688015 | global_recall: 0.9603246167718665 | global_auc: 0.9950996941582371| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 64 | global_acc: 97.640% | global_loss: 0.07285565882921219 | global_f1: 0.967741935483871 | global_precision: 0.9752747252747253 | global_recall: 0.9603246167718665 | global_auc: 0.9951224862784314| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 65 | global_acc: 97.640% | global_loss: 0.07263758033514023 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.9951680705188198| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 66 | global_acc: 97.706% | global_loss: 0.07232610881328583 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.995175667892218| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 67 | global_acc: 97.706% | global_loss: 0.07210814952850342 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9952127050875336| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 68 | global_acc: 97.706% | global_loss: 0.07185272872447968 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.995224575983468| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 69 | global_acc: 97.706% | global_loss: 0.07161415368318558 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9952459435961503| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 70 | global_acc: 97.706% | global_loss: 0.07148649543523788 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.995274433746393| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 71 | global_acc: 97.706% | global_loss: 0.07125832885503769 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.9952901033290267| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 72 | global_acc: 97.706% | global_loss: 0.07114540040493011 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.9953109961058713| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 73 | global_acc: 97.706% | global_loss: 0.07085921615362167 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.995309571598359| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 74 | global_acc: 97.706% | global_loss: 0.07071700692176819 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.9953328385543907| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 75 | global_acc: 97.739% | global_loss: 0.07052335143089294 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9953409107636262| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 76 | global_acc: 97.739% | global_loss: 0.07037077844142914 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9953475584653496| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 77 | global_acc: 97.773% | global_loss: 0.07019474357366562 | global_f1: 0.9695592912312585 | global_precision: 0.9771062271062271 | global_recall: 0.9621280432822362 | global_auc: 0.9953551558387477| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.739% | global_loss: 0.07016158849000931 | global_f1: 0.969147005444646 | global_precision: 0.9753424657534246 | global_recall: 0.9630297565374211 | global_auc: 0.9953786602126979| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 79 | global_acc: 97.773% | global_loss: 0.06996873766183853 | global_f1: 0.9695869269178392 | global_precision: 0.9762340036563071 | global_recall: 0.9630297565374211 | global_auc: 0.9953841208248279| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 80 | global_acc: 97.739% | global_loss: 0.06981144845485687 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9953855453323399| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 81 | global_acc: 97.806% | global_loss: 0.06970115751028061 | global_f1: 0.9700544464609802 | global_precision: 0.9762557077625571 | global_recall: 0.9639314697926059 | global_auc: 0.9954083374525344| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 82 | global_acc: 97.806% | global_loss: 0.06957008689641953 | global_f1: 0.9700544464609802 | global_precision: 0.9762557077625571 | global_recall: 0.9639314697926059 | global_auc: 0.995420683184306| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 83 | global_acc: 97.806% | global_loss: 0.06943278759717941 | global_f1: 0.9700544464609802 | global_precision: 0.9762557077625571 | global_recall: 0.9639314697926059 | global_auc: 0.9954263812143547| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 84 | global_acc: 97.806% | global_loss: 0.06933517754077911 | global_f1: 0.9700544464609802 | global_precision: 0.9762557077625571 | global_recall: 0.9639314697926059 | global_auc: 0.9954425256328256| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 85 | global_acc: 97.773% | global_loss: 0.0692763552069664 | global_f1: 0.9696420480289986 | global_precision: 0.9744990892531876 | global_recall: 0.9648331830477908 | global_auc: 0.9954681667680441| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 86 | global_acc: 97.872% | global_loss: 0.06912051141262054 | global_f1: 0.9709618874773139 | global_precision: 0.9771689497716894 | global_recall: 0.9648331830477908 | global_auc: 0.995466742260532| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 87 | global_acc: 97.806% | global_loss: 0.06905633211135864 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9954919085599132| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 88 | global_acc: 97.806% | global_loss: 0.06893503665924072 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9954985562616364| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 89 | global_acc: 97.773% | global_loss: 0.0689154788851738 | global_f1: 0.9696420480289986 | global_precision: 0.9744990892531876 | global_recall: 0.9648331830477908 | global_auc: 0.9955232477251803| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 90 | global_acc: 97.872% | global_loss: 0.06873065233230591 | global_f1: 0.9709618874773139 | global_precision: 0.9771689497716894 | global_recall: 0.9648331830477908 | global_auc: 0.9955222980535056| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 91 | global_acc: 97.872% | global_loss: 0.068642757833004 | global_f1: 0.9709618874773139 | global_precision: 0.9771689497716894 | global_recall: 0.9648331830477908 | global_auc: 0.9955332192777653| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 92 | global_acc: 97.872% | global_loss: 0.06858082115650177 | global_f1: 0.9709618874773139 | global_precision: 0.9771689497716894 | global_recall: 0.9648331830477908 | global_auc: 0.9955522127112605| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 93 | global_acc: 97.906% | global_loss: 0.06851119548082352 | global_f1: 0.9714026327734907 | global_precision: 0.9780621572212066 | global_recall: 0.9648331830477908 | global_auc: 0.9955531623829351| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 94 | global_acc: 97.839% | global_loss: 0.06847814470529556 | global_f1: 0.970548255550521 | global_precision: 0.9754098360655737 | global_recall: 0.9657348963029756 | global_auc: 0.9955807028615034| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 95 | global_acc: 97.939% | global_loss: 0.06835998594760895 | global_f1: 0.9718437783832881 | global_precision: 0.9789569990850869 | global_recall: 0.9648331830477908 | global_auc: 0.9955735803239425| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 96 | global_acc: 97.972% | global_loss: 0.06829342991113663 | global_f1: 0.9723104857013164 | global_precision: 0.9789762340036563 | global_recall: 0.9657348963029756 | global_auc: 0.995591624085763| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 97 | global_acc: 98.039% | global_loss: 0.06822393089532852 | global_f1: 0.9732183386291421 | global_precision: 0.979890310786106 | global_recall: 0.9666366095581606 | global_auc: 0.9955913866678443| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 98 | global_acc: 97.939% | global_loss: 0.06827914714813232 | global_f1: 0.9719202898550725 | global_precision: 0.9763421292083713 | global_recall: 0.9675383228133454 | global_auc: 0.9956163155493066| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 99 | global_acc: 97.939% | global_loss: 0.06829642504453659 | global_f1: 0.9719202898550725 | global_precision: 0.9763421292083713 | global_recall: 0.9675383228133454 | global_auc: 0.9956258122660544| flobal_FPR: 0.032461677186654644 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_model_noise(X_test, Y_test, model, comm_round):\n",
    "    bce = nn.BCELoss()\n",
    "    # Add noise to the test data\n",
    "    noise_factor = 0.01  # Adjust this factor based on your noise preference\n",
    "    X_test_noisy = X_test + noise_factor * np.random.normal(size=X_test.shape)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_test_noisy)\n",
    "        loss = bce(logits, Y_test)\n",
    "        Y_prdt = (logits > 0.5).int()\n",
    "        acc = accuracy_score(Y_test.cpu().numpy(), Y_prdt.cpu().numpy())\n",
    "        F1 = f1_score(Y_test.cpu().numpy(), Y_prdt.cpu().numpy(), zero_division=1)\n",
    "        precision = precision_score(Y_test.cpu().numpy(), Y_prdt.cpu().numpy(), zero_division=1)\n",
    "        recall = recall_score(Y_test.cpu().numpy(), Y_prdt.cpu().numpy(), zero_division=1)\n",
    "\n",
    "        cm = confusion_matrix(Y_test.cpu().numpy(), Y_prdt.cpu().numpy())\n",
    "        TP = cm[0][0]\n",
    "        FN = cm[0][1]\n",
    "        FP = cm[1][0]\n",
    "        TN = cm[1][1]\n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(Y_test.cpu().numpy(), logits.cpu().numpy())\n",
    "        auc_value = roc_auc_score(Y_test.cpu().numpy(), logits.cpu().numpy())\n",
    "\n",
    "        print('comm_round: {} | global_acc: {:.3%} | global_loss: {} | global_f1: {} | global_precision: {} | global_recall: {} | global_auc: {}| flobal_FPR: {} '.format(\n",
    "            comm_round, acc, loss.item(), F1, precision, recall, auc_value, FPR))\n",
    "    return acc, loss.item(), F1, precision, recall, auc_value, FPR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "            noise_factor = 0.05  # Adjust this factor based on your noise preference\n",
    "            X_test_noisy = X_test + noise_factor * np.random.normal(size=X_test.shape)\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test_noisy, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-noise-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-noise-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-noise-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "327e6fdc",
   "metadata": {},
   "source": [
    "## test 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a27ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Function to add poisoning attack to a specific client's data\n",
    "def add_poisoning_attack_to_client(client_data, poison_ratio=0.1):\n",
    "    num_poison_samples = int(len(client_data['labels']) * poison_ratio)\n",
    "\n",
    "    # Assuming binary classification, add poison samples with label 1 (malicious)\n",
    "    poison_indices = np.random.choice(len(client_data['labels']), num_poison_samples, replace=False)\n",
    "    client_data['labels'][poison_indices] = 1\n",
    "\n",
    "    return client_data\n",
    "\n",
    "\n",
    "# ... (rest of your code)\n",
    "\n",
    "# In the main loop, replace the client creation with the new function\n",
    "\n",
    "\n",
    "# ... (rest of your code)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    poisoned_client_idx = 'clients_3'  # Index of the client you want to poison\n",
    "                    if 'clients_3' == poisoned_client_idx:\n",
    "                        clients_batched[poisoned_client_idx] = add_poisoning_attack_to_client(clients_batched[poisoned_client_idx], poison_ratio=0.1)\n",
    "\n",
    "\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
