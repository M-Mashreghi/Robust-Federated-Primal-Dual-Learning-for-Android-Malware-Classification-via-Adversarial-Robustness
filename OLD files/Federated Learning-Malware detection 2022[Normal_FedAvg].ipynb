{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "#from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import  keras\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "from federated_utils_fedavg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\drebin.csv'\n",
    "malgenome_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\malgenome.csv'\n",
    "kronodroid_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'H:\\GIT project\\DW-FedAvg\\data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 5\n",
      "No. of Rounds: 10\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 0 | global_acc: 97.706% | global_loss: 0.5661508440971375 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9950676427392141| flobal_FPR: 0.0387736699729486 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | global_acc: 97.872% | global_loss: 0.564946711063385 | global_f1: 0.9709355131698456 | global_precision: 0.9780420860018298 | global_recall: 0.9639314697926059 | global_auc: 0.9957260026277415| flobal_FPR: 0.03606853020739405 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | global_acc: 98.005% | global_loss: 0.5646182894706726 | global_f1: 0.9728014505893019 | global_precision: 0.9781221513217867 | global_recall: 0.9675383228133454 | global_auc: 0.9959850255770324| flobal_FPR: 0.032461677186654644 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 3 | global_acc: 98.039% | global_loss: 0.5641659498214722 | global_f1: 0.9732668781150884 | global_precision: 0.9781420765027322 | global_recall: 0.9684400360685302 | global_auc: 0.9960911513866868| flobal_FPR: 0.031559963931469794 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 4 | global_acc: 98.205% | global_loss: 0.5639007091522217 | global_f1: 0.9755434782608696 | global_precision: 0.9799818016378526 | global_recall: 0.9711451758340848 | global_auc: 0.9962269544361776| flobal_FPR: 0.028854824165915238 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 5 | global_acc: 98.205% | global_loss: 0.5636971592903137 | global_f1: 0.9755434782608696 | global_precision: 0.9799818016378526 | global_recall: 0.9711451758340848 | global_auc: 0.996281085721639| flobal_FPR: 0.028854824165915238 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 6 | global_acc: 98.205% | global_loss: 0.5635262131690979 | global_f1: 0.9755434782608696 | global_precision: 0.9799818016378526 | global_recall: 0.9711451758340848 | global_auc: 0.9963233461111657| flobal_FPR: 0.028854824165915238 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 7 | global_acc: 98.238% | global_loss: 0.5633753538131714 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9963380660221245| flobal_FPR: 0.027051397655545536 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 8 | global_acc: 98.238% | global_loss: 0.5632635354995728 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9963337924995881| flobal_FPR: 0.027051397655545536 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 9 | global_acc: 98.238% | global_loss: 0.563148021697998 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9963760528891149| flobal_FPR: 0.027051397655545536 \n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 10\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | global_acc: 97.241% | global_loss: 0.5710530281066895 | global_f1: 0.9622212107419208 | global_precision: 0.9715073529411765 | global_recall: 0.9531109107303878 | global_auc: 0.9941148846315109| flobal_FPR: 0.046889089269612265 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | global_acc: 97.640% | global_loss: 0.5682328343391418 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9951343571743659| flobal_FPR: 0.03787195671776375 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | global_acc: 97.739% | global_loss: 0.5671289563179016 | global_f1: 0.9691749773345422 | global_precision: 0.9744758432087511 | global_recall: 0.9639314697926059 | global_auc: 0.9955360682927894| flobal_FPR: 0.03606853020739405 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 3 | global_acc: 97.839% | global_loss: 0.5665081143379211 | global_f1: 0.970548255550521 | global_precision: 0.9754098360655737 | global_recall: 0.9657348963029756 | global_auc: 0.9956837422382148| flobal_FPR: 0.034265103697024346 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 4 | global_acc: 97.806% | global_loss: 0.5660979747772217 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9957967531675112| flobal_FPR: 0.0351668169522092 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 5 | global_acc: 97.972% | global_loss: 0.5658259987831116 | global_f1: 0.9724106739032112 | global_precision: 0.9754990925589837 | global_recall: 0.9693417493237151 | global_auc: 0.9958508844529725| flobal_FPR: 0.030658250676284943 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 6 | global_acc: 97.972% | global_loss: 0.5656305551528931 | global_f1: 0.9724106739032112 | global_precision: 0.9754990925589837 | global_recall: 0.9693417493237151 | global_auc: 0.9958893461558004| flobal_FPR: 0.030658250676284943 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 7 | global_acc: 98.039% | global_loss: 0.565467119216919 | global_f1: 0.9733152419719584 | global_precision: 0.9764065335753176 | global_recall: 0.9702434625788999 | global_auc: 0.9959401535903999| flobal_FPR: 0.029756537421100092 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 8 | global_acc: 98.105% | global_loss: 0.5653164982795715 | global_f1: 0.9742198100407056 | global_precision: 0.9773139745916516 | global_recall: 0.9711451758340848 | global_auc: 0.9959795649649026| flobal_FPR: 0.028854824165915238 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 9 | global_acc: 98.105% | global_loss: 0.5652114152908325 | global_f1: 0.9742198100407056 | global_precision: 0.9773139745916516 | global_recall: 0.9711451758340848 | global_auc: 0.9959942848758613| flobal_FPR: 0.028854824165915238 \n",
      "---------------------------------------------\n",
      "No. of Clients: 15\n",
      "No. of Rounds: 10\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 0 | global_acc: 96.310% | global_loss: 0.5744854211807251 | global_f1: 0.9496598639455782 | global_precision: 0.9552919708029197 | global_recall: 0.9440937781785392 | global_auc: 0.9927820204359847| flobal_FPR: 0.05590622182146077 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | global_acc: 97.041% | global_loss: 0.5712169408798218 | global_f1: 0.9596737652922519 | global_precision: 0.9644808743169399 | global_recall: 0.9549143372407575 | global_auc: 0.9946324556942551| flobal_FPR: 0.04508566275924256 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 2 | global_acc: 97.241% | global_loss: 0.5700721740722656 | global_f1: 0.9623923878568192 | global_precision: 0.9672131147540983 | global_recall: 0.957619477006312 | global_auc: 0.9951775672355675| flobal_FPR: 0.04238052299368801 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 3 | global_acc: 97.540% | global_loss: 0.5692650079727173 | global_f1: 0.9665158371040724 | global_precision: 0.9700272479564033 | global_recall: 0.9630297565374211 | global_auc: 0.9954083374525342| flobal_FPR: 0.0369702434625789 \n",
      "94/94 [==============================] - 0s 1ms/step\n",
      "comm_round: 4 | global_acc: 97.606% | global_loss: 0.5687617063522339 | global_f1: 0.967420814479638 | global_precision: 0.9709355131698456 | global_recall: 0.9639314697926059 | global_auc: 0.9955298954269036| flobal_FPR: 0.03606853020739405 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 5 | global_acc: 97.739% | global_loss: 0.5684289336204529 | global_f1: 0.969286359530262 | global_precision: 0.9710407239819004 | global_recall: 0.9675383228133454 | global_auc: 0.9956001711308358| flobal_FPR: 0.032461677186654644 \n",
      "94/94 [==============================] - 0s 4ms/step\n",
      "comm_round: 6 | global_acc: 97.806% | global_loss: 0.5681984424591064 | global_f1: 0.9701897018970189 | global_precision: 0.971945701357466 | global_recall: 0.9684400360685302 | global_auc: 0.995688015760751| flobal_FPR: 0.031559963931469794 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 7 | global_acc: 97.773% | global_loss: 0.567956805229187 | global_f1: 0.9697243560777227 | global_precision: 0.9719202898550725 | global_recall: 0.9675383228133454 | global_auc: 0.995737873523676| flobal_FPR: 0.032461677186654644 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 8 | global_acc: 97.806% | global_loss: 0.5678368210792542 | global_f1: 0.9701897018970189 | global_precision: 0.971945701357466 | global_recall: 0.9684400360685302 | global_auc: 0.9957768100623412| flobal_FPR: 0.031559963931469794 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 9 | global_acc: 97.773% | global_loss: 0.5676348805427551 | global_f1: 0.9697243560777227 | global_precision: 0.9719202898550725 | global_recall: 0.9675383228133454 | global_auc: 0.9958143220934943| flobal_FPR: 0.032461677186654644 \n",
      "---------------------------------------------\n",
      "No. of Clients: 5\n",
      "No. of Rounds: 20\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "94/94 [==============================] - 1s 4ms/step\n",
      "comm_round: 0 | global_acc: 98.205% | global_loss: 0.5642975568771362 | global_f1: 0.9756317689530687 | global_precision: 0.976513098464318 | global_recall: 0.9747520288548241 | global_auc: 0.9964323209358444| flobal_FPR: 0.025247971145175834 \n",
      "94/94 [==============================] - 0s 4ms/step\n",
      "comm_round: 1 | global_acc: 98.504% | global_loss: 0.5627602934837341 | global_f1: 0.9796472184531885 | global_precision: 0.9827586206896551 | global_recall: 0.9765554553651938 | global_auc: 0.9966376874355114| flobal_FPR: 0.023444544634806132 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 2 | global_acc: 98.604% | global_loss: 0.5621124505996704 | global_f1: 0.9809954751131221 | global_precision: 0.9845594913714805 | global_recall: 0.9774571686203787 | global_auc: 0.9967407268122228| flobal_FPR: 0.02254283137962128 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 3 | global_acc: 98.570% | global_loss: 0.5619839429855347 | global_f1: 0.9805517865219356 | global_precision: 0.9836660617059891 | global_recall: 0.9774571686203787 | global_auc: 0.9967487990214583| flobal_FPR: 0.02254283137962128 \n",
      "94/94 [==============================] - 1s 4ms/step\n",
      "comm_round: 4 | global_acc: 98.637% | global_loss: 0.5616931319236755 | global_f1: 0.9814563545906829 | global_precision: 0.984573502722323 | global_recall: 0.9783588818755635 | global_auc: 0.9967905845751478| flobal_FPR: 0.02164111812443643 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 5 | global_acc: 98.604% | global_loss: 0.5616945624351501 | global_f1: 0.9810126582278481 | global_precision: 0.9836808703535811 | global_recall: 0.9783588818755635 | global_auc: 0.9968000812918953| flobal_FPR: 0.02164111812443643 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 6 | global_acc: 98.604% | global_loss: 0.5615463852882385 | global_f1: 0.9810126582278481 | global_precision: 0.9836808703535811 | global_recall: 0.9783588818755635 | global_auc: 0.9967620944249049| flobal_FPR: 0.02164111812443643 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 7 | global_acc: 98.670% | global_loss: 0.5614033341407776 | global_f1: 0.9819331526648599 | global_precision: 0.983710407239819 | global_recall: 0.9801623083859333 | global_auc: 0.9967658931116039| flobal_FPR: 0.019837691614066726 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 8 | global_acc: 98.604% | global_loss: 0.561240017414093 | global_f1: 0.9810126582278481 | global_precision: 0.9836808703535811 | global_recall: 0.9783588818755635 | global_auc: 0.9967708788878965| flobal_FPR: 0.02164111812443643 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 9 | global_acc: 98.637% | global_loss: 0.56124347448349 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.9967848865450992| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 10 | global_acc: 98.637% | global_loss: 0.5611281991004944 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.9967808504404815| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 11 | global_acc: 98.670% | global_loss: 0.5610626935958862 | global_f1: 0.9819168173598554 | global_precision: 0.9845874886672711 | global_recall: 0.9792605951307484 | global_auc: 0.9967761020821077| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 12 | global_acc: 98.670% | global_loss: 0.5610301494598389 | global_f1: 0.9819168173598554 | global_precision: 0.9845874886672711 | global_recall: 0.9792605951307484 | global_auc: 0.996783462037587| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 13 | global_acc: 98.670% | global_loss: 0.5609954595565796 | global_f1: 0.9819168173598554 | global_precision: 0.9845874886672711 | global_recall: 0.9792605951307484 | global_auc: 0.9967473745139462| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 1s 7ms/step\n",
      "comm_round: 14 | global_acc: 98.637% | global_loss: 0.5609261393547058 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.9967620944249049| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 15 | global_acc: 98.637% | global_loss: 0.5609197616577148 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.9967637563503358| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 4ms/step\n",
      "comm_round: 16 | global_acc: 98.637% | global_loss: 0.5608806014060974 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.9967668427832788| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 17 | global_acc: 98.637% | global_loss: 0.5608577132225037 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.996787260724286| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 18 | global_acc: 98.637% | global_loss: 0.5607970356941223 | global_f1: 0.9814731134206959 | global_precision: 0.9836956521739131 | global_recall: 0.9792605951307484 | global_auc: 0.9967787136792132| flobal_FPR: 0.020739404869251576 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 19 | global_acc: 98.604% | global_loss: 0.5607874989509583 | global_f1: 0.9810126582278481 | global_precision: 0.9836808703535811 | global_recall: 0.9783588818755635 | global_auc: 0.9967965200231149| flobal_FPR: 0.02164111812443643 \n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 20\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 0 | global_acc: 97.241% | global_loss: 0.5689710378646851 | global_f1: 0.962358276643991 | global_precision: 0.968065693430657 | global_recall: 0.9567177637511272 | global_auc: 0.9943608495952737| flobal_FPR: 0.04328223624887286 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 1 | global_acc: 97.640% | global_loss: 0.5661540031433105 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.995197985176575| flobal_FPR: 0.0387736699729486 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 2 | global_acc: 97.872% | global_loss: 0.5652447938919067 | global_f1: 0.9709090909090908 | global_precision: 0.9789184234647113 | global_recall: 0.9630297565374211 | global_auc: 0.9955688319655687| flobal_FPR: 0.0369702434625789 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 3 | global_acc: 98.105% | global_loss: 0.5646408796310425 | global_f1: 0.9741261915569679 | global_precision: 0.9808043875685558 | global_recall: 0.9675383228133454 | global_auc: 0.9957706371964552| flobal_FPR: 0.032461677186654644 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 4 | global_acc: 98.205% | global_loss: 0.5643214583396912 | global_f1: 0.9755213055303718 | global_precision: 0.9808568824065633 | global_recall: 0.9702434625788999 | global_auc: 0.9958518341246472| flobal_FPR: 0.029756537421100092 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 5 | global_acc: 98.305% | global_loss: 0.564091145992279 | global_f1: 0.9768707482993196 | global_precision: 0.9826642335766423 | global_recall: 0.9711451758340848 | global_auc: 0.9959187859777179| flobal_FPR: 0.028854824165915238 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 6 | global_acc: 98.338% | global_loss: 0.5639094710350037 | global_f1: 0.9773345421577515 | global_precision: 0.9826800364630811 | global_recall: 0.9720468890892696 | global_auc: 0.9959676940689681| flobal_FPR: 0.027953110910730387 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 7 | global_acc: 98.338% | global_loss: 0.5638129711151123 | global_f1: 0.9773345421577515 | global_precision: 0.9826800364630811 | global_recall: 0.9720468890892696 | global_auc: 0.9960147028168687| flobal_FPR: 0.027953110910730387 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 8 | global_acc: 98.371% | global_loss: 0.5637055039405823 | global_f1: 0.9777979157227006 | global_precision: 0.982695810564663 | global_recall: 0.9729486023444545 | global_auc: 0.9960645605797935| flobal_FPR: 0.027051397655545536 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 9 | global_acc: 98.338% | global_loss: 0.5636124014854431 | global_f1: 0.9773550724637681 | global_precision: 0.9818016378525932 | global_recall: 0.9729486023444545 | global_auc: 0.9960792804907523| flobal_FPR: 0.027051397655545536 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 10 | global_acc: 98.338% | global_loss: 0.5635258555412292 | global_f1: 0.9773550724637681 | global_precision: 0.9818016378525932 | global_recall: 0.9729486023444545 | global_auc: 0.9961082454768325| flobal_FPR: 0.027051397655545536 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 11 | global_acc: 98.371% | global_loss: 0.5634621381759644 | global_f1: 0.977818017202354 | global_precision: 0.9818181818181818 | global_recall: 0.9738503155996393 | global_auc: 0.9961281885820025| flobal_FPR: 0.026149684400360685 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 12 | global_acc: 98.371% | global_loss: 0.5633957982063293 | global_f1: 0.977818017202354 | global_precision: 0.9818181818181818 | global_recall: 0.9738503155996393 | global_auc: 0.9961500310305219| flobal_FPR: 0.026149684400360685 \n",
      "94/94 [==============================] - 0s 2ms/step\n",
      "comm_round: 13 | global_acc: 98.371% | global_loss: 0.5633472800254822 | global_f1: 0.977818017202354 | global_precision: 0.9818181818181818 | global_recall: 0.9738503155996393 | global_auc: 0.9961718734790413| flobal_FPR: 0.026149684400360685 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 14 | global_acc: 98.371% | global_loss: 0.5633132457733154 | global_f1: 0.977818017202354 | global_precision: 0.9818181818181818 | global_recall: 0.9738503155996393 | global_auc: 0.9962046371518207| flobal_FPR: 0.026149684400360685 \n",
      "94/94 [==============================] - 1s 5ms/step\n",
      "comm_round: 15 | global_acc: 98.338% | global_loss: 0.5632336139678955 | global_f1: 0.9773755656108598 | global_precision: 0.9809264305177112 | global_recall: 0.9738503155996393 | global_auc: 0.9962022629726337| flobal_FPR: 0.026149684400360685 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 16 | global_acc: 98.371% | global_loss: 0.5631944537162781 | global_f1: 0.9778380823156942 | global_precision: 0.9809437386569873 | global_recall: 0.9747520288548241 | global_auc: 0.9962269544361776| flobal_FPR: 0.025247971145175834 \n",
      "94/94 [==============================] - 0s 4ms/step\n",
      "comm_round: 17 | global_acc: 98.404% | global_loss: 0.5631610751152039 | global_f1: 0.9782805429864253 | global_precision: 0.9818346957311535 | global_recall: 0.9747520288548241 | global_auc: 0.9962407246754617| flobal_FPR: 0.025247971145175834 \n",
      "94/94 [==============================] - 1s 5ms/step\n",
      "comm_round: 18 | global_acc: 98.438% | global_loss: 0.5631358623504639 | global_f1: 0.9787426503844414 | global_precision: 0.9818511796733213 | global_recall: 0.975653742110009 | global_auc: 0.9962544949147456| flobal_FPR: 0.024346257889990983 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 19 | global_acc: 98.438% | global_loss: 0.5631025433540344 | global_f1: 0.9787426503844414 | global_precision: 0.9818511796733213 | global_recall: 0.975653742110009 | global_auc: 0.996270164497379| flobal_FPR: 0.024346257889990983 \n",
      "---------------------------------------------\n",
      "No. of Clients: 15\n",
      "No. of Rounds: 20\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "94/94 [==============================] - 1s 4ms/step\n",
      "comm_round: 0 | global_acc: 96.775% | global_loss: 0.5718671679496765 | global_f1: 0.9556875285518502 | global_precision: 0.9685185185185186 | global_recall: 0.9431920649233544 | global_auc: 0.9934968857891605| flobal_FPR: 0.056807935076645624 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 1 | global_acc: 97.274% | global_loss: 0.568996012210846 | global_f1: 0.9629294755877035 | global_precision: 0.9655485040797824 | global_recall: 0.9603246167718665 | global_auc: 0.9948019720881998| flobal_FPR: 0.03967538322813345 \n",
      "94/94 [==============================] - 0s 3ms/step\n",
      "comm_round: 2 | global_acc: 97.440% | global_loss: 0.5676238536834717 | global_f1: 0.965205603253502 | global_precision: 0.967391304347826 | global_recall: 0.9630297565374211 | global_auc: 0.9951433790552762| flobal_FPR: 0.0369702434625789 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_avg =[]\n",
    "\n",
    "all_std =[]\n",
    "\n",
    "n_clients = [5,10,15]\n",
    "n_round = [10,20]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
    "\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0,2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d==1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d==2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d==3:\n",
    "        use_data = Tuandromd_data\n",
    "        \n",
    "        \n",
    "    print('===================================================================================================')\n",
    "    print('Working with:',dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round: #number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients: #number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            # from sklearn.utils import shuffle\n",
    "            # use_data = shuffle(use_data)\n",
    "            # use_data\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "\n",
    "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
    "\n",
    "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
    "\n",
    "\n",
    "            #Do feature scaling \n",
    "\n",
    "\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "\n",
    "            #binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "            #split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "            #create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            #process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "                #process and batch the test set  \n",
    "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "            #==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            #==============================================\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            all_results=list()\n",
    "\n",
    "            #create optimizer\n",
    "            lr = 0.01 \n",
    "            loss='binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "            optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, \n",
    "                            decay=lr / comms_round, \n",
    "                            momentum=0.9\n",
    "                           )\n",
    "\n",
    "            #initialize global model\n",
    "            smlp_global = SimpleMLP()\n",
    "            global_model = smlp_global.build(X.shape[1],1)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            #commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = global_model.get_weights()\n",
    "\n",
    "                #initial list to collect local model weights after scalling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                #randomize client data - using keys\n",
    "                client_names= list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                #loop through each client and create new local model\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP()\n",
    "                    local_model = smlp_local.build(X.shape[1],1)\n",
    "                    local_model.compile(loss=loss, \n",
    "                                  optimizer=optimizer, \n",
    "                                  metrics=metrics)\n",
    "\n",
    "                    #set local model weight to the weight of the global model\n",
    "                    local_model.set_weights(global_weights)\n",
    "\n",
    "                    #fit local model with client's data\n",
    "                    local_model.fit(clients_batched[client], epochs=32, verbose=0)\n",
    "\n",
    "                    #scale the model weights and add to list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    #clear session to free memory after each communication round\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                #update global model \n",
    "                global_model.set_weights(average_weights)\n",
    "\n",
    "                #test global model and print out metrics after each communications round\n",
    "                for(X_test, Y_test) in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
    "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/FedAvg-'+dataset[d]+'-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "            \n",
    "            \n",
    "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
    "          \n",
    "\n",
    "            \n",
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv(f'FedAvg-results.csv')     \n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f343ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drebin' '10' '5' '0.9822805851063829' '0.5641639709472657'\n",
      " '0.9758701733506149' '0.9798174987444629' '0.9719567177637511'\n",
      " '0.9949485064276151' '0.028043282236248874']\n"
     ]
    }
   ],
   "source": [
    "print((all_avg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2893319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 5\n",
      "No. of Rounds: 10\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 128\u001b[0m\n\u001b[0;32m    125\u001b[0m local_model\u001b[39m.\u001b[39mset_weights(global_weights)\n\u001b[0;32m    127\u001b[0m \u001b[39m#fit local model with client's data\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m local_model\u001b[39m.\u001b[39;49mfit(clients_batched[client], epochs\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    130\u001b[0m \u001b[39m#scale the model weights and add to list\u001b[39;00m\n\u001b[0;32m    131\u001b[0m scaling_factor \u001b[39m=\u001b[39m weight_scalling_factor(clients_batched, client)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    869\u001b[0m   )\n\u001b[0;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m   \u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1485\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "all_avg =[]\n",
    "\n",
    "all_std =[]\n",
    "\n",
    "n_clients = [5,10,15]\n",
    "n_round = [10,20]\n",
    "\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
    "\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d==1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d==2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d==3:\n",
    "        use_data = Tuandromd_data\n",
    "        \n",
    "        \n",
    "    print('===================================================================================================')\n",
    "    print('Working with:',dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round: #number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients: #number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            # from sklearn.utils import shuffle\n",
    "            # use_data = shuffle(use_data)\n",
    "            # use_data\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "\n",
    "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
    "\n",
    "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
    "\n",
    "\n",
    "            #Do feature scaling \n",
    "\n",
    "\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "\n",
    "            #binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "            #split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "            #create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            #process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "                #process and batch the test set  \n",
    "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "            #==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            #==============================================\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            all_results=list()\n",
    "\n",
    "            #create optimizer\n",
    "            lr = 0.01 \n",
    "            loss='binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "            optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, \n",
    "                            decay=lr / comms_round, \n",
    "                            momentum=0.9\n",
    "                           )\n",
    "\n",
    "            #initialize global model\n",
    "            smlp_global = SimpleMLP()\n",
    "            global_model = smlp_global.build(X.shape[1],1)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            #commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = global_model.get_weights()\n",
    "\n",
    "                #initial list to collect local model weights after scalling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                #randomize client data - using keys\n",
    "                client_names= list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                #loop through each client and create new local model\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP()\n",
    "                    local_model = smlp_local.build(X.shape[1],1)\n",
    "                    local_model.compile(loss=loss, \n",
    "                                  optimizer=optimizer, \n",
    "                                  metrics=metrics)\n",
    "\n",
    "                    #set local model weight to the weight of the global model\n",
    "                    local_model.set_weights(global_weights)\n",
    "\n",
    "                    #fit local model with client's data\n",
    "                    local_model.fit(clients_batched[client], epochs=32, verbose=0)\n",
    "\n",
    "                    #scale the model weights and add to list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    #clear session to free memory after each communication round\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                #update global model \n",
    "                global_model.set_weights(average_weights)\n",
    "\n",
    "                #test global model and print out metrics after each communications round\n",
    "                for(X_test, Y_test) in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
    "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/FedAvg-'+dataset[d]+'-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "            \n",
    "            \n",
    "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv(f'FedAvg-results.csv')     \n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6104c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-all-avg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0c3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# make a little extra space between the subplots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "s1 = np.array(all_results) #FedAvg\n",
    "\n",
    "t = range(0,s1.shape[0])\n",
    "\n",
    "ax1.plot(t, s1[:,0],label='Acc of FedAvg')\n",
    "ax1.set_xlim(0,s1.shape[0])\n",
    "ax1.set_xlabel('Rounds')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0.98,1)\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(t, s1[:,1],label='Error of FedAvg')\n",
    "ax2.set_xlim(0, s1.shape[0])\n",
    "ax2.set_xlabel('Rounds')\n",
    "ax2.set_ylabel('error')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861d31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
