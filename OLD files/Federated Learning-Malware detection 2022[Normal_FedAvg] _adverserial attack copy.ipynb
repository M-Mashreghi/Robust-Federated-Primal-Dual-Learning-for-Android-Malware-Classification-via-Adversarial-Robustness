{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "#from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import  keras\n",
    "# from tensorflow.keras import backend as K\n",
    "import torchattacks\n",
    "from cleverhans.tf2.attacks import fast_gradient_method\n",
    "from cleverhans.tf2.attacks import projected_gradient_descent\n",
    "from federated_utils_fedavg import *\n",
    "\n",
    "from cleverhans.tf2.attacks import projected_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\drebin.csv'\n",
    "malgenome_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\malgenome.csv'\n",
    "kronodroid_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'H:\\GIT project\\DW-FedAvg\\data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_avg =[]\n",
    "\n",
    "all_std =[]\n",
    "\n",
    "n_clients = [5,10,15]\n",
    "n_round = [10,20]\n",
    "\n",
    "from cleverhans.tf2.attacks import CarliniWagnerL2\n",
    "\n",
    "cw_params = {\n",
    "    'batch_size': 32,\n",
    "    'confidence': 0,\n",
    "    'learning_rate': 0.01,\n",
    "    'binary_search_steps': 10,\n",
    "    'max_iterations': 1000,\n",
    "    'abort_early': True,\n",
    "    'initial_const': 0.001,\n",
    "    'clip_min': 0.0,\n",
    "    'clip_max': 1.0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
    "\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0,2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d==1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d==2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d==3:\n",
    "        use_data = Tuandromd_data\n",
    "        \n",
    "        \n",
    "    print('===================================================================================================')\n",
    "    print('Working with:',dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round: #number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients: #number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            # from sklearn.utils import shuffle\n",
    "            # use_data = shuffle(use_data)\n",
    "            # use_data\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "\n",
    "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
    "\n",
    "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
    "\n",
    "\n",
    "            #Do feature scaling \n",
    "\n",
    "\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "\n",
    "            #binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "            #split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "            #create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            #process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "                #process and batch the test set  \n",
    "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "            #==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            #==============================================\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            all_results=list()\n",
    "\n",
    "            #create optimizer\n",
    "            lr = 0.01 \n",
    "            loss='binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "            optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, \n",
    "                            decay=lr / comms_round, \n",
    "                            momentum=0.9\n",
    "                           )\n",
    "\n",
    "            #initialize global model\n",
    "            smlp_global = SimpleMLP()\n",
    "            global_model = smlp_global.build(X.shape[1],1)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            #commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = global_model.get_weights()\n",
    "\n",
    "                #initial list to collect local model weights after scalling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                #randomize client data - using keys\n",
    "                client_names= list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                #loop through each client and create new local model\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP()\n",
    "                    local_model = smlp_local.build(X.shape[1],1)\n",
    "                    local_model.compile(loss=loss, \n",
    "                                  optimizer=optimizer, \n",
    "                                  metrics=metrics)\n",
    "\n",
    "                    #set local model weight to the weight of the global model\n",
    "                    local_model.set_weights(global_weights)\n",
    "\n",
    "                    #fit local model with client's data\n",
    "                    local_model.fit(clients_batched[client], epochs=32, verbose=0)\n",
    "                    print(clients_batched[client])\n",
    "                    # Use the DeepFool attack\n",
    "                    cw = CarliniWagnerL2(wrap_model=local_model, sess=keras.backend.get_session())\n",
    "                    adv_x = cw.generate_np(X_test, **cw_params)\n",
    "\n",
    "                    #scale the model weights and add to list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    #clear session to free memory after each communication round\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                #update global model \n",
    "                global_model.set_weights(average_weights)\n",
    "\n",
    "                #test global model and print out metrics after each communications round\n",
    "                for(X_test, Y_test) in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
    "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/FedAvg-'+dataset[d]+'-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "            \n",
    "            \n",
    "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
    "          \n",
    "\n",
    "            \n",
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv(f'FedAvg-results.csv')     \n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((all_avg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_avg =[]\n",
    "\n",
    "all_std =[]\n",
    "\n",
    "n_clients = [5,10,15]\n",
    "n_round = [10,20]\n",
    "\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
    "\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d==1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d==2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d==3:\n",
    "        use_data = Tuandromd_data\n",
    "        \n",
    "        \n",
    "    print('===================================================================================================')\n",
    "    print('Working with:',dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round: #number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients: #number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            # from sklearn.utils import shuffle\n",
    "            # use_data = shuffle(use_data)\n",
    "            # use_data\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "\n",
    "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
    "\n",
    "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
    "\n",
    "\n",
    "            #Do feature scaling \n",
    "\n",
    "\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "\n",
    "            #binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "            #split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "            #create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            #process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "                #process and batch the test set  \n",
    "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "            #==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            #==============================================\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            all_results=list()\n",
    "\n",
    "            #create optimizer\n",
    "            lr = 0.01 \n",
    "            loss='binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "            optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, \n",
    "                            decay=lr / comms_round, \n",
    "                            momentum=0.9\n",
    "                           )\n",
    "\n",
    "            #initialize global model\n",
    "            smlp_global = SimpleMLP()\n",
    "            global_model = smlp_global.build(X.shape[1],1)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            #commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = global_model.get_weights()\n",
    "\n",
    "                #initial list to collect local model weights after scalling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                #randomize client data - using keys\n",
    "                client_names= list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                #loop through each client and create new local model\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP()\n",
    "                    local_model = smlp_local.build(X.shape[1],1)\n",
    "                    local_model.compile(loss=loss, \n",
    "                                  optimizer=optimizer, \n",
    "                                  metrics=metrics)\n",
    "\n",
    "                    #set local model weight to the weight of the global model\n",
    "                    local_model.set_weights(global_weights)\n",
    "\n",
    "                    #fit local model with client's data\n",
    "                    local_model.fit(clients_batched[client], epochs=32, verbose=0)\n",
    "\n",
    "                    #scale the model weights and add to list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    #clear session to free memory after each communication round\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                #update global model \n",
    "                global_model.set_weights(average_weights)\n",
    "\n",
    "                #test global model and print out metrics after each communications round\n",
    "                for(X_test, Y_test) in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
    "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/FedAvg-'+dataset[d]+'-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "            \n",
    "            \n",
    "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv(f'FedAvg-results.csv')     \n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6104c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-all-avg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# make a little extra space between the subplots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "s1 = np.array(all_results) #FedAvg\n",
    "\n",
    "t = range(0,s1.shape[0])\n",
    "\n",
    "ax1.plot(t, s1[:,0],label='Acc of FedAvg')\n",
    "ax1.set_xlim(0,s1.shape[0])\n",
    "ax1.set_xlabel('Rounds')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0.98,1)\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(t, s1[:,1],label='Error of FedAvg')\n",
    "ax2.set_xlim(0, s1.shape[0])\n",
    "ax2.set_xlabel('Rounds')\n",
    "ax2.set_ylabel('error')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
