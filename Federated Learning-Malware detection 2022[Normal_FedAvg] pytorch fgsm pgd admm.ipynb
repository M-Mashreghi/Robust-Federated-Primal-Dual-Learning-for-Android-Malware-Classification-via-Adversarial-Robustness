{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dcfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3ab07f",
   "metadata": {},
   "source": [
    "## non iid FedADMM FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6356e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 682, (1,): 521}\n",
      "Client client_2: {(0,): 897, (1,): 306}\n",
      "Client client_3: {(0,): 698, (1,): 505}\n",
      "Client client_4: {(0,): 586, (1,): 617}\n",
      "Client client_5: {(0,): 252, (1,): 951}\n",
      "Client client_6: {(0,): 316, (1,): 887}\n",
      "Client client_7: {(0,): 252, (1,): 951}\n",
      "Client client_8: {(0,): 752, (1,): 451}\n",
      "Client client_9: {(0,): 308, (1,): 895}\n",
      "Client client_10: {(0,): 261, (1,): 942}\n",
      "|=======================|\n",
      "|Traditional FedADMM-non-iid-FGSM  2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 36.868% | global_loss: 0.7125949263572693 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.579346967769568| flobal_FPR: 0.0 \n",
      "comm_round: 1 | global_acc: 36.868% | global_loss: 0.7372980117797852 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.7507453735557275| flobal_FPR: 0.0 \n",
      "comm_round: 2 | global_acc: 36.868% | global_loss: 0.7606887221336365 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.8462199031239925| flobal_FPR: 0.0 \n",
      "comm_round: 3 | global_acc: 36.868% | global_loss: 0.7753217816352844 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.9041664945386756| flobal_FPR: 0.0 \n",
      "comm_round: 4 | global_acc: 36.868% | global_loss: 0.7672239542007446 | global_f1: 0.538741802283216 | global_precision: 0.36868351063829785 | global_recall: 1.0 | global_auc: 0.9289721561013319| flobal_FPR: 0.0 \n",
      "comm_round: 5 | global_acc: 36.835% | global_loss: 0.7454143166542053 | global_f1: 0.5383867832847424 | global_precision: 0.36847356168939144 | global_recall: 0.9990982867448152 | global_auc: 0.918878333288224| flobal_FPR: 0.0009017132551848512 \n",
      "comm_round: 6 | global_acc: 52.959% | global_loss: 0.6496615409851074 | global_f1: 0.6103001927843569 | global_precision: 0.4393338620142744 | global_recall: 0.9990982867448152 | global_auc: 0.9374213849916738| flobal_FPR: 0.0009017132551848512 \n",
      "comm_round: 7 | global_acc: 79.654% | global_loss: 0.46347540616989136 | global_f1: 0.7804878048780488 | global_precision: 0.6480047647409172 | global_recall: 0.9810640216411182 | global_auc: 0.9708735697351033| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 8 | global_acc: 81.383% | global_loss: 0.3821168541908264 | global_f1: 0.7933579335793358 | global_precision: 0.6714553404122423 | global_recall: 0.9693417493237151 | global_auc: 0.9692016727516879| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 9 | global_acc: 93.118% | global_loss: 0.18340739607810974 | global_f1: 0.9077128845296478 | global_precision: 0.8977072310405644 | global_recall: 0.9179440937781785 | global_auc: 0.9793816782692804| flobal_FPR: 0.08205590622182146 \n",
      "comm_round: 10 | global_acc: 93.949% | global_loss: 0.15480689704418182 | global_f1: 0.9181654676258992 | global_precision: 0.915695067264574 | global_recall: 0.9206492335437331 | global_auc: 0.9846157937047215| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 11 | global_acc: 94.481% | global_loss: 0.17248854041099548 | global_f1: 0.9253597122302158 | global_precision: 0.9228699551569507 | global_recall: 0.9278629395852119 | global_auc: 0.9855616666927826| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 12 | global_acc: 95.312% | global_loss: 0.1848756968975067 | global_f1: 0.9384548232213008 | global_precision: 0.9094754653130288 | global_recall: 0.9693417493237151 | global_auc: 0.9925023421277679| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 13 | global_acc: 94.548% | global_loss: 0.3587091267108917 | global_f1: 0.9297343616109683 | global_precision: 0.8857142857142857 | global_recall: 0.9783588818755635 | global_auc: 0.9923385237638718| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 14 | global_acc: 94.980% | global_loss: 0.36853232979774475 | global_f1: 0.9349978476108481 | global_precision: 0.8945634266886326 | global_recall: 0.9792605951307484 | global_auc: 0.9923373366742784| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 15 | global_acc: 95.445% | global_loss: 0.46240535378456116 | global_f1: 0.9396209784045836 | global_precision: 0.9189655172413793 | global_recall: 0.9612263300270514 | global_auc: 0.9909712339701358| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 16 | global_acc: 93.517% | global_loss: 0.7103172540664673 | global_f1: 0.9168443496801706 | global_precision: 0.8697411003236246 | global_recall: 0.9693417493237151 | global_auc: 0.9888848053006875| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 17 | global_acc: 92.254% | global_loss: 1.0236108303070068 | global_f1: 0.9024696525742989 | global_precision: 0.8421875 | global_recall: 0.9720468890892696 | global_auc: 0.9856730156966482| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 18 | global_acc: 94.215% | global_loss: 1.0381234884262085 | global_f1: 0.9248704663212435 | global_precision: 0.8873239436619719 | global_recall: 0.9657348963029756 | global_auc: 0.9861523624744835| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 19 | global_acc: 94.016% | global_loss: 1.3408397436141968 | global_f1: 0.9217391304347826 | global_precision: 0.8900083963056256 | global_recall: 0.9558160504959423 | global_auc: 0.9821497337832878| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 20 | global_acc: 93.218% | global_loss: 2.0575673580169678 | global_f1: 0.9139240506329114 | global_precision: 0.8588421887390959 | global_recall: 0.9765554553651938 | global_auc: 0.9797525250582741| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 21 | global_acc: 93.185% | global_loss: 2.4489457607269287 | global_f1: 0.9131723845828039 | global_precision: 0.8610223642172524 | global_recall: 0.9720468890892696 | global_auc: 0.9764106304347929| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 22 | global_acc: 94.382% | global_loss: 2.095944404602051 | global_f1: 0.9271237602414835 | global_precision: 0.8884297520661157 | global_recall: 0.9693417493237151 | global_auc: 0.9795777854701183| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 23 | global_acc: 95.113% | global_loss: 1.962993860244751 | global_f1: 0.9354980254497587 | global_precision: 0.9111111111111111 | global_recall: 0.9612263300270514 | global_auc: 0.9798657734054894| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 24 | global_acc: 94.747% | global_loss: 2.307857036590576 | global_f1: 0.9313640312771503 | global_precision: 0.8985750209555742 | global_recall: 0.9666366095581606 | global_auc: 0.9782482451254539| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 25 | global_acc: 95.379% | global_loss: 2.2173469066619873 | global_f1: 0.9391151992991679 | global_precision: 0.9131175468483816 | global_recall: 0.9666366095581606 | global_auc: 0.9789362822538179| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 26 | global_acc: 95.612% | global_loss: 2.1382882595062256 | global_f1: 0.9419014084507041 | global_precision: 0.9200343938091143 | global_recall: 0.9648331830477908 | global_auc: 0.9797102646687474| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 27 | global_acc: 95.047% | global_loss: 2.276172161102295 | global_f1: 0.9350196249454863 | global_precision: 0.9054054054054054 | global_recall: 0.9666366095581606 | global_auc: 0.9786390350196178| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 28 | global_acc: 95.545% | global_loss: 2.3531012535095215 | global_f1: 0.9410211267605633 | global_precision: 0.9191745485812554 | global_recall: 0.9639314697926059 | global_auc: 0.9777494300782862| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 29 | global_acc: 95.778% | global_loss: 2.3035640716552734 | global_f1: 0.9440282062582636 | global_precision: 0.9232758620689655 | global_recall: 0.9657348963029756 | global_auc: 0.9775583086537407| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 30 | global_acc: 95.412% | global_loss: 2.479029893875122 | global_f1: 0.9395267309377738 | global_precision: 0.9138959931798807 | global_recall: 0.9666366095581606 | global_auc: 0.9763360812083244| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 31 | global_acc: 95.711% | global_loss: 2.415147066116333 | global_f1: 0.9432468103827542 | global_precision: 0.9209621993127147 | global_recall: 0.9666366095581606 | global_auc: 0.9767608218648609| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 32 | global_acc: 95.711% | global_loss: 2.424567699432373 | global_f1: 0.9432468103827542 | global_precision: 0.9209621993127147 | global_recall: 0.9666366095581606 | global_auc: 0.975984702688663| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 33 | global_acc: 96.310% | global_loss: 2.1820900440216064 | global_f1: 0.9504242965609648 | global_precision: 0.9415929203539823 | global_recall: 0.9594229035166817 | global_auc: 0.9785267363440775| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 34 | global_acc: 95.445% | global_loss: 2.524022102355957 | global_f1: 0.9400437636761487 | global_precision: 0.9132653061224489 | global_recall: 0.9684400360685302 | global_auc: 0.9754880244027634| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 35 | global_acc: 96.543% | global_loss: 2.2869012355804443 | global_f1: 0.9536541889483066 | global_precision: 0.9427312775330396 | global_recall: 0.9648331830477908 | global_auc: 0.9784405536395929| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 36 | global_acc: 96.243% | global_loss: 2.4877305030822754 | global_f1: 0.9498891352549889 | global_precision: 0.9345549738219895 | global_recall: 0.9657348963029756 | global_auc: 0.976024588899003| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 37 | global_acc: 96.110% | global_loss: 2.576951503753662 | global_f1: 0.9481612760301285 | global_precision: 0.9320557491289199 | global_recall: 0.9648331830477908 | global_auc: 0.975707398559633| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 38 | global_acc: 96.011% | global_loss: 2.762218952178955 | global_f1: 0.9469496021220158 | global_precision: 0.9288811795316565 | global_recall: 0.9657348963029756 | global_auc: 0.9748132826778462| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 39 | global_acc: 96.177% | global_loss: 2.687239170074463 | global_f1: 0.9489569462938304 | global_precision: 0.9344405594405595 | global_recall: 0.9639314697926059 | global_auc: 0.9750606721491213| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 40 | global_acc: 95.512% | global_loss: 3.1443932056427 | global_f1: 0.9409190371991247 | global_precision: 0.9141156462585034 | global_recall: 0.9693417493237151 | global_auc: 0.9708749942426154| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 41 | global_acc: 95.612% | global_loss: 3.241349935531616 | global_f1: 0.9420544337137841 | global_precision: 0.9178785286569717 | global_recall: 0.9675383228133454 | global_auc: 0.9698246573703307| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 42 | global_acc: 95.479% | global_loss: 3.2318179607391357 | global_f1: 0.9404553415061296 | global_precision: 0.9140425531914894 | global_recall: 0.9684400360685302 | global_auc: 0.9697814473091291| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 43 | global_acc: 96.011% | global_loss: 2.718679189682007 | global_f1: 0.9468085106382979 | global_precision: 0.9311246730601569 | global_recall: 0.9630297565374211 | global_auc: 0.9737515497454643| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 44 | global_acc: 95.512% | global_loss: 3.1342949867248535 | global_f1: 0.9407114624505929 | global_precision: 0.9169520547945206 | global_recall: 0.9657348963029756 | global_auc: 0.9703664450607812| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 45 | global_acc: 95.512% | global_loss: 3.0666072368621826 | global_f1: 0.9408672798948752 | global_precision: 0.9148211243611585 | global_recall: 0.9684400360685302 | global_auc: 0.9704799308259151| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 46 | global_acc: 96.044% | global_loss: 2.612182378768921 | global_f1: 0.9472283813747229 | global_precision: 0.9319371727748691 | global_recall: 0.9630297565374211 | global_auc: 0.9739445705133593| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 47 | global_acc: 92.753% | global_loss: 2.8245301246643066 | global_f1: 0.9078613693998309 | global_precision: 0.8544152744630071 | global_recall: 0.9684400360685302 | global_auc: 0.9729664086883563| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 48 | global_acc: 96.177% | global_loss: 2.7298872470855713 | global_f1: 0.9490925188136343 | global_precision: 0.9321739130434783 | global_recall: 0.9666366095581606 | global_auc: 0.9736819862952881| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 49 | global_acc: 96.077% | global_loss: 2.7282867431640625 | global_f1: 0.9477413640389726 | global_precision: 0.9312445604873804 | global_recall: 0.9648331830477908 | global_auc: 0.9736634676976303| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 50 | global_acc: 95.878% | global_loss: 3.0631661415100098 | global_f1: 0.945374449339207 | global_precision: 0.9242032730404823 | global_recall: 0.9675383228133454 | global_auc: 0.9711860117160995| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 51 | global_acc: 95.778% | global_loss: 3.345198631286621 | global_f1: 0.9442248572683356 | global_precision: 0.9203767123287672 | global_recall: 0.9693417493237151 | global_auc: 0.96988638602919| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 52 | global_acc: 94.847% | global_loss: 3.4522716999053955 | global_f1: 0.9327548806941433 | global_precision: 0.8988294314381271 | global_recall: 0.9693417493237151 | global_auc: 0.969026220909776| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 53 | global_acc: 95.645% | global_loss: 3.388150453567505 | global_f1: 0.9425186485300571 | global_precision: 0.9179487179487179 | global_recall: 0.9684400360685302 | global_auc: 0.9692484440816699| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 54 | global_acc: 91.722% | global_loss: 3.306164026260376 | global_f1: 0.8960334029227558 | global_precision: 0.8343701399688958 | global_recall: 0.9675383228133454 | global_auc: 0.9697026245601239| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 55 | global_acc: 96.077% | global_loss: 2.9836087226867676 | global_f1: 0.9477413640389726 | global_precision: 0.9312445604873804 | global_recall: 0.9648331830477908 | global_auc: 0.970883778705607| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 56 | global_acc: 95.146% | global_loss: 3.5016677379608154 | global_f1: 0.9363001745200699 | global_precision: 0.9070160608622148 | global_recall: 0.9675383228133454 | global_auc: 0.9682401301809932| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 57 | global_acc: 94.980% | global_loss: 3.743093967437744 | global_f1: 0.9344333478072081 | global_precision: 0.9011725293132329 | global_recall: 0.9702434625788999 | global_auc: 0.9663745001759267| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 58 | global_acc: 95.180% | global_loss: 3.7338004112243652 | global_f1: 0.9368191721132897 | global_precision: 0.9064080944350759 | global_recall: 0.9693417493237151 | global_auc: 0.966028107432558| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 59 | global_acc: 95.312% | global_loss: 3.736062526702881 | global_f1: 0.938562091503268 | global_precision: 0.9080944350758853 | global_recall: 0.9711451758340848 | global_auc: 0.9659352770263501| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 60 | global_acc: 95.047% | global_loss: 3.8175647258758545 | global_f1: 0.9355257464301168 | global_precision: 0.8993344425956739 | global_recall: 0.9747520288548241 | global_auc: 0.9654395484121252| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 61 | global_acc: 95.479% | global_loss: 3.7864620685577393 | global_f1: 0.9408181026979983 | global_precision: 0.9091673675357443 | global_recall: 0.9747520288548241 | global_auc: 0.9656237847170286| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 62 | global_acc: 95.911% | global_loss: 3.1580772399902344 | global_f1: 0.9456953642384106 | global_precision: 0.9264705882352942 | global_recall: 0.9657348963029756 | global_auc: 0.9701684385165938| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 63 | global_acc: 96.110% | global_loss: 2.9414069652557373 | global_f1: 0.9479305740987984 | global_precision: 0.9358523725834798 | global_recall: 0.9603246167718665 | global_auc: 0.9712926123615913| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 64 | global_acc: 93.318% | global_loss: 3.0190606117248535 | global_f1: 0.9137709137709137 | global_precision: 0.8715220949263502 | global_recall: 0.9603246167718665 | global_auc: 0.9707061901024269| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 65 | global_acc: 96.011% | global_loss: 3.2095863819122314 | global_f1: 0.9469496021220158 | global_precision: 0.9288811795316565 | global_recall: 0.9657348963029756 | global_auc: 0.9693643040259907| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 66 | global_acc: 95.645% | global_loss: 3.295029640197754 | global_f1: 0.9423161602818141 | global_precision: 0.9208261617900172 | global_recall: 0.9648331830477908 | global_auc: 0.9685805874763947| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 67 | global_acc: 95.745% | global_loss: 3.341291666030884 | global_f1: 0.9436123348017621 | global_precision: 0.9224806201550387 | global_recall: 0.9657348963029756 | global_auc: 0.9683419824681113| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 68 | global_acc: 92.221% | global_loss: 3.897279739379883 | global_f1: 0.9023372287145243 | global_precision: 0.83993783993784 | global_recall: 0.9747520288548241 | global_auc: 0.96389087132851| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 69 | global_acc: 95.445% | global_loss: 3.834726095199585 | global_f1: 0.9402007856831078 | global_precision: 0.9111675126903553 | global_recall: 0.9711451758340848 | global_auc: 0.9639393045839229| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 70 | global_acc: 95.844% | global_loss: 3.493595838546753 | global_f1: 0.9449581682078381 | global_precision: 0.923407917383821 | global_recall: 0.9675383228133454 | global_auc: 0.9670679979164203| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 71 | global_acc: 96.011% | global_loss: 3.6039252281188965 | global_f1: 0.9470432480141218 | global_precision: 0.9273984442523768 | global_recall: 0.9675383228133454 | global_auc: 0.9648445791078879| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 72 | global_acc: 92.520% | global_loss: 3.5199501514434814 | global_f1: 0.9052631578947369 | global_precision: 0.84913112164297 | global_recall: 0.9693417493237151 | global_auc: 0.966229437827607| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 73 | global_acc: 95.977% | global_loss: 3.3716773986816406 | global_f1: 0.9465783664459161 | global_precision: 0.9273356401384083 | global_recall: 0.9666366095581606 | global_auc: 0.9668759268202001| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 74 | global_acc: 95.745% | global_loss: 3.577636957168579 | global_f1: 0.9438103599648816 | global_precision: 0.9195893926432849 | global_recall: 0.9693417493237151 | global_auc: 0.9654602037710514| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 75 | global_acc: 95.645% | global_loss: 3.632214069366455 | global_f1: 0.9426695842450766 | global_precision: 0.9158163265306123 | global_recall: 0.9711451758340848 | global_auc: 0.9654234039936542| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 76 | global_acc: 95.678% | global_loss: 3.5978212356567383 | global_f1: 0.9429824561403509 | global_precision: 0.9180187873612298 | global_recall: 0.9693417493237151 | global_auc: 0.9659402628026424| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 77 | global_acc: 95.878% | global_loss: 3.509385347366333 | global_f1: 0.9454225352112676 | global_precision: 0.9234737747205503 | global_recall: 0.9684400360685302 | global_auc: 0.9662173295137539| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 78 | global_acc: 95.844% | global_loss: 3.5749897956848145 | global_f1: 0.9450065992080949 | global_precision: 0.9226804123711341 | global_recall: 0.9684400360685302 | global_auc: 0.9657256370041468| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 79 | global_acc: 95.711% | global_loss: 3.591449499130249 | global_f1: 0.943445857080228 | global_precision: 0.9180887372013652 | global_recall: 0.9702434625788999 | global_auc: 0.9659628175049181| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 80 | global_acc: 95.977% | global_loss: 3.565586566925049 | global_f1: 0.946766388033436 | global_precision: 0.9243986254295533 | global_recall: 0.9702434625788999 | global_auc: 0.9661745942883897| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 81 | global_acc: 96.243% | global_loss: 3.471076488494873 | global_f1: 0.9501103752759383 | global_precision: 0.9307958477508651 | global_recall: 0.9702434625788999 | global_auc: 0.966742023114059| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 82 | global_acc: 96.343% | global_loss: 3.3756093978881836 | global_f1: 0.9514134275618374 | global_precision: 0.9324675324675324 | global_recall: 0.9711451758340848 | global_auc: 0.9675513807988735| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 83 | global_acc: 96.243% | global_loss: 3.3685762882232666 | global_f1: 0.9501103752759383 | global_precision: 0.9307958477508651 | global_recall: 0.9702434625788999 | global_auc: 0.9675141061856389| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 84 | global_acc: 96.277% | global_loss: 3.442359447479248 | global_f1: 0.9505736981465137 | global_precision: 0.9308556611927399 | global_recall: 0.9711451758340848 | global_auc: 0.9670582637817542| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 85 | global_acc: 96.376% | global_loss: 3.393192768096924 | global_f1: 0.9518338488731772 | global_precision: 0.9332755632582322 | global_recall: 0.9711451758340848 | global_auc: 0.9675938786063188| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 86 | global_acc: 93.185% | global_loss: 3.5766618251800537 | global_f1: 0.9133924799324039 | global_precision: 0.8593004769475358 | global_recall: 0.9747520288548241 | global_auc: 0.9671696127856197| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 87 | global_acc: 95.711% | global_loss: 3.631401777267456 | global_f1: 0.9436435124508519 | global_precision: 0.9152542372881356 | global_recall: 0.9738503155996393 | global_auc: 0.9664939213890278| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 88 | global_acc: 96.011% | global_loss: 3.5635852813720703 | global_f1: 0.9474145486415424 | global_precision: 0.9215686274509803 | global_recall: 0.9747520288548241 | global_auc: 0.9670141040488777| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 89 | global_acc: 95.911% | global_loss: 3.727940320968628 | global_f1: 0.9461235216819974 | global_precision: 0.919931856899489 | global_recall: 0.9738503155996393 | global_auc: 0.9657030823018711| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 90 | global_acc: 96.011% | global_loss: 3.6156954765319824 | global_f1: 0.9472759226713532 | global_precision: 0.9237360754070265 | global_recall: 0.9720468890892696 | global_auc: 0.9664374159243796| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 91 | global_acc: 95.811% | global_loss: 3.699697732925415 | global_f1: 0.9445910290237467 | global_precision: 0.9218884120171674 | global_recall: 0.9684400360685302 | global_auc: 0.9647989948674994| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 92 | global_acc: 92.420% | global_loss: 4.061862468719482 | global_f1: 0.904121110176619 | global_precision: 0.847123719464145 | global_recall: 0.9693417493237151 | global_auc: 0.9627279983627661| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 93 | global_acc: 91.689% | global_loss: 5.045661926269531 | global_f1: 0.896694214876033 | global_precision: 0.8276125095347063 | global_recall: 0.9783588818755635 | global_auc: 0.9558445406461851| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 94 | global_acc: 94.049% | global_loss: 5.339422225952148 | global_f1: 0.923732424371538 | global_precision: 0.875605815831987 | global_recall: 0.9774571686203787 | global_auc: 0.9528578232290642| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 95 | global_acc: 95.346% | global_loss: 4.319309234619141 | global_f1: 0.9391304347826086 | global_precision: 0.906801007556675 | global_recall: 0.9738503155996393 | global_auc: 0.9608103738335063| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 96 | global_acc: 95.645% | global_loss: 4.042144298553467 | global_f1: 0.9427197201574113 | global_precision: 0.9151103565365025 | global_recall: 0.9720468890892696 | global_auc: 0.962180275224348| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 97 | global_acc: 96.011% | global_loss: 3.738013505935669 | global_f1: 0.9472295514511873 | global_precision: 0.9244635193133047 | global_recall: 0.9711451758340848 | global_auc: 0.9644309970935298| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 98 | global_acc: 95.844% | global_loss: 3.7209227085113525 | global_f1: 0.9449581682078381 | global_precision: 0.923407917383821 | global_recall: 0.9675383228133454 | global_auc: 0.9643507498370127| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 99 | global_acc: 95.811% | global_loss: 3.829620361328125 | global_f1: 0.9445422535211269 | global_precision: 0.9226139294926913 | global_recall: 0.9675383228133454 | global_auc: 0.9634623319852744| flobal_FPR: 0.032461677186654644 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedADMM 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM-non-iid-FGSM  2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-non-iid-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-non-iid-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-non-iid-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d522b1",
   "metadata": {},
   "source": [
    "## iid FedADMM FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32db8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedADMM-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6610625982284546 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7102221709399518| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6400108337402344 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7790232721792258| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6173828840255737 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8572154391922853| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.5632169842720032 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.92423661829514| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.4710708260536194 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.969140418928666| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 91.589% | global_loss: 0.3617386817932129 | global_f1: 0.8720283257460799 | global_precision: 0.9930875576036866 | global_recall: 0.7772768259693418 | global_auc: 0.9839192095312848| flobal_FPR: 0.22272317403065825 \n",
      "comm_round: 6 | global_acc: 94.448% | global_loss: 0.2565035820007324 | global_f1: 0.9244001810774106 | global_precision: 0.9281818181818182 | global_recall: 0.9206492335437331 | global_auc: 0.9847924326362268| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 7 | global_acc: 93.251% | global_loss: 0.18414367735385895 | global_f1: 0.9096573208722741 | global_precision: 0.8980667838312829 | global_recall: 0.9215509467989179 | global_auc: 0.9799899429769642| flobal_FPR: 0.07844905320108206 \n",
      "comm_round: 8 | global_acc: 92.055% | global_loss: 0.21454977989196777 | global_f1: 0.8941072219760745 | global_precision: 0.8789198606271778 | global_recall: 0.9098286744815148 | global_auc: 0.9740454731288025| flobal_FPR: 0.09017132551848513 \n",
      "comm_round: 9 | global_acc: 94.614% | global_loss: 0.16782055795192719 | global_f1: 0.9281277728482697 | global_precision: 0.9135371179039301 | global_recall: 0.9431920649233544 | global_auc: 0.9860179839325048| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 10 | global_acc: 95.878% | global_loss: 0.15172383189201355 | global_f1: 0.9441944194419443 | global_precision: 0.9424977538185085 | global_recall: 0.9458972046889089 | global_auc: 0.9891765919227574| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 11 | global_acc: 96.476% | global_loss: 0.15245220065116882 | global_f1: 0.9520795660036167 | global_precision: 0.9546690843155031 | global_recall: 0.9495040577096483 | global_auc: 0.9904624473703829| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 12 | global_acc: 97.507% | global_loss: 0.20211371779441833 | global_f1: 0.9662009914375845 | global_precision: 0.9657657657657658 | global_recall: 0.9666366095581606 | global_auc: 0.9928461232740311| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 13 | global_acc: 97.473% | global_loss: 0.21226659417152405 | global_f1: 0.9661921708185053 | global_precision: 0.9534679543459175 | global_recall: 0.9792605951307484 | global_auc: 0.9949700639746324| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 14 | global_acc: 97.141% | global_loss: 0.21875223517417908 | global_f1: 0.9617097061442564 | global_precision: 0.9498680738786279 | global_recall: 0.9738503155996393 | global_auc: 0.9948568156274172| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 15 | global_acc: 96.775% | global_loss: 0.16846349835395813 | global_f1: 0.9563259792886086 | global_precision: 0.9550359712230215 | global_recall: 0.957619477006312 | global_auc: 0.9940355870466683| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 16 | global_acc: 96.277% | global_loss: 0.20417897403240204 | global_f1: 0.9489516864175023 | global_precision: 0.9594470046082949 | global_recall: 0.9386834986474302 | global_auc: 0.9911982055004034| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 17 | global_acc: 96.975% | global_loss: 0.17701126635074615 | global_f1: 0.9584285061671997 | global_precision: 0.9712962962962963 | global_recall: 0.9458972046889089 | global_auc: 0.9944904797788785| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 18 | global_acc: 97.540% | global_loss: 0.18973705172538757 | global_f1: 0.9660861594867094 | global_precision: 0.9822926374650512 | global_recall: 0.9504057709648331 | global_auc: 0.9952319359389475| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 19 | global_acc: 97.706% | global_loss: 0.21598926186561584 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.9952708724776126| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 20 | global_acc: 97.773% | global_loss: 0.2800940275192261 | global_f1: 0.9696145124716553 | global_precision: 0.9753649635036497 | global_recall: 0.9639314697926059 | global_auc: 0.994774194191713| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 21 | global_acc: 97.939% | global_loss: 0.34627217054367065 | global_f1: 0.9718181818181818 | global_precision: 0.9798350137488543 | global_recall: 0.9639314697926059 | global_auc: 0.9929147370525325| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 22 | global_acc: 97.673% | global_loss: 0.2739960551261902 | global_f1: 0.9679780420860018 | global_precision: 0.9823584029712163 | global_recall: 0.9540126239855726 | global_auc: 0.9925016298740118| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 23 | global_acc: 97.806% | global_loss: 0.28125056624412537 | global_f1: 0.9698354661791591 | global_precision: 0.9833178869323448 | global_recall: 0.9567177637511272 | global_auc: 0.9916630697851986| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 24 | global_acc: 96.875% | global_loss: 0.5323393940925598 | global_f1: 0.9588080631025416 | global_precision: 0.9326513213981245 | global_recall: 0.9864743011722272 | global_auc: 0.9926853913430779| flobal_FPR: 0.013525698827772768 \n",
      "comm_round: 25 | global_acc: 97.606% | global_loss: 0.46499475836753845 | global_f1: 0.9680567879325642 | global_precision: 0.9528384279475982 | global_recall: 0.9837691614066727 | global_auc: 0.9924935576647763| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 26 | global_acc: 98.338% | global_loss: 0.3576372563838959 | global_f1: 0.9772727272727272 | global_precision: 0.9853345554537122 | global_recall: 0.9693417493237151 | global_auc: 0.9917022437417823| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 27 | global_acc: 97.673% | global_loss: 0.35871943831443787 | global_f1: 0.9685251798561152 | global_precision: 0.9659192825112107 | global_recall: 0.9711451758340848 | global_auc: 0.9907684790675743| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 28 | global_acc: 98.138% | global_loss: 0.31903767585754395 | global_f1: 0.9746376811594203 | global_precision: 0.9790718835304822 | global_recall: 0.9702434625788999 | global_auc: 0.9914966398241969| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 29 | global_acc: 98.205% | global_loss: 0.309418648481369 | global_f1: 0.9755656108597286 | global_precision: 0.9791099000908265 | global_recall: 0.9720468890892696 | global_auc: 0.9923052852552552| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 30 | global_acc: 97.939% | global_loss: 0.27082759141921997 | global_f1: 0.9719710669077758 | global_precision: 0.9746146872166818 | global_recall: 0.9693417493237151 | global_auc: 0.9928095609145527| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 31 | global_acc: 98.338% | global_loss: 0.39685487747192383 | global_f1: 0.9775784753363229 | global_precision: 0.9723461195361285 | global_recall: 0.9828674481514879 | global_auc: 0.9932589930346332| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 32 | global_acc: 98.471% | global_loss: 0.26866161823272705 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9939726712982153| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 33 | global_acc: 98.371% | global_loss: 0.26194465160369873 | global_f1: 0.978017048003589 | global_precision: 0.9732142857142857 | global_recall: 0.9828674481514879 | global_auc: 0.9942708682040902| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 34 | global_acc: 98.537% | global_loss: 0.19334086775779724 | global_f1: 0.9800543970988214 | global_precision: 0.9854147675478578 | global_recall: 0.9747520288548241 | global_auc: 0.9939280367295017| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 35 | global_acc: 98.371% | global_loss: 0.18368607759475708 | global_f1: 0.9778380823156942 | global_precision: 0.9809437386569873 | global_recall: 0.9747520288548241 | global_auc: 0.99397908158202| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 36 | global_acc: 98.404% | global_loss: 0.18411040306091309 | global_f1: 0.9782016348773842 | global_precision: 0.9853613906678865 | global_recall: 0.9711451758340848 | global_auc: 0.9939994995230275| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 37 | global_acc: 98.404% | global_loss: 0.17741714417934418 | global_f1: 0.9784366576819408 | global_precision: 0.9749328558639212 | global_recall: 0.981965734896303 | global_auc: 0.9948299874026052| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 38 | global_acc: 98.238% | global_loss: 0.18003201484680176 | global_f1: 0.9762225213100045 | global_precision: 0.9714285714285714 | global_recall: 0.9810640216411182 | global_auc: 0.9951362565177154| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 39 | global_acc: 98.338% | global_loss: 0.17199045419692993 | global_f1: 0.9774774774774774 | global_precision: 0.9765976597659766 | global_recall: 0.9783588818755635 | global_auc: 0.9950346416485161| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 40 | global_acc: 98.570% | global_loss: 0.16143590211868286 | global_f1: 0.98058690744921 | global_precision: 0.9819168173598554 | global_recall: 0.9792605951307484 | global_auc: 0.9949859709751845| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 41 | global_acc: 97.440% | global_loss: 0.17790299654006958 | global_f1: 0.9658233466489126 | global_precision: 0.951048951048951 | global_recall: 0.9810640216411182 | global_auc: 0.9946502620381569| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 42 | global_acc: 98.271% | global_loss: 0.16213880479335785 | global_f1: 0.9765765765765766 | global_precision: 0.9756975697569757 | global_recall: 0.9774571686203787 | global_auc: 0.9948466066569135| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 43 | global_acc: 98.338% | global_loss: 0.1672818958759308 | global_f1: 0.9774368231046933 | global_precision: 0.978319783197832 | global_recall: 0.9765554553651938 | global_auc: 0.9950612324554093| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 44 | global_acc: 98.105% | global_loss: 0.17503400146961212 | global_f1: 0.9743127534925642 | global_precision: 0.9738738738738739 | global_recall: 0.9747520288548241 | global_auc: 0.9947689709975018| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 45 | global_acc: 98.205% | global_loss: 0.19719716906547546 | global_f1: 0.975653742110009 | global_precision: 0.975653742110009 | global_recall: 0.975653742110009 | global_auc: 0.9949992663786311| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 46 | global_acc: 98.172% | global_loss: 0.2052570879459381 | global_f1: 0.975191700496166 | global_precision: 0.9756317689530686 | global_recall: 0.9747520288548241 | global_auc: 0.9953940923774128| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 47 | global_acc: 98.205% | global_loss: 0.21647413074970245 | global_f1: 0.9756317689530687 | global_precision: 0.976513098464318 | global_recall: 0.9747520288548241 | global_auc: 0.9952884414035957| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 48 | global_acc: 98.305% | global_loss: 0.22553937137126923 | global_f1: 0.9769543605964752 | global_precision: 0.9791666666666666 | global_recall: 0.9747520288548241 | global_auc: 0.9947737193558757| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 49 | global_acc: 98.305% | global_loss: 0.23290885984897614 | global_f1: 0.9769335142469472 | global_precision: 0.9800362976406534 | global_recall: 0.9738503155996393 | global_auc: 0.9945989797677198| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 50 | global_acc: 98.404% | global_loss: 0.24454300105571747 | global_f1: 0.978319783197832 | global_precision: 0.9800904977375565 | global_recall: 0.9765554553651938 | global_auc: 0.9941210574973968| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 51 | global_acc: 98.371% | global_loss: 0.24951542913913727 | global_f1: 0.9778781038374718 | global_precision: 0.9792043399638336 | global_recall: 0.9765554553651938 | global_auc: 0.9939949885825723| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 52 | global_acc: 98.371% | global_loss: 0.2813185453414917 | global_f1: 0.9779378658262045 | global_precision: 0.9766187050359713 | global_recall: 0.9792605951307484 | global_auc: 0.9941483605580462| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 53 | global_acc: 98.471% | global_loss: 0.28624480962753296 | global_f1: 0.9792043399638336 | global_precision: 0.9818676337262012 | global_recall: 0.9765554553651938 | global_auc: 0.9939947511646537| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 54 | global_acc: 98.438% | global_loss: 0.2904421389102936 | global_f1: 0.9787426503844414 | global_precision: 0.9818511796733213 | global_recall: 0.975653742110009 | global_auc: 0.9938667829064797| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 55 | global_acc: 98.504% | global_loss: 0.2934875190258026 | global_f1: 0.9796472184531885 | global_precision: 0.9827586206896551 | global_recall: 0.9765554553651938 | global_auc: 0.9936362501074316| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 56 | global_acc: 98.371% | global_loss: 0.4053088128566742 | global_f1: 0.9779378658262045 | global_precision: 0.9766187050359713 | global_recall: 0.9792605951307484 | global_auc: 0.9930085171304152| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 57 | global_acc: 98.338% | global_loss: 0.43599221110343933 | global_f1: 0.9774977497749775 | global_precision: 0.9757412398921833 | global_recall: 0.9792605951307484 | global_auc: 0.9928150215266827| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 58 | global_acc: 98.271% | global_loss: 0.43025320768356323 | global_f1: 0.9766187050359711 | global_precision: 0.9739910313901345 | global_recall: 0.9792605951307484 | global_auc: 0.993159514926702| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 59 | global_acc: 98.338% | global_loss: 0.44428107142448425 | global_f1: 0.9774774774774774 | global_precision: 0.9765976597659766 | global_recall: 0.9783588818755635 | global_auc: 0.992905952589541| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 60 | global_acc: 98.438% | global_loss: 0.4544226825237274 | global_f1: 0.9788192879675529 | global_precision: 0.9783783783783784 | global_recall: 0.9792605951307484 | global_auc: 0.9926668727454201| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 61 | global_acc: 98.371% | global_loss: 0.44679081439971924 | global_f1: 0.9777979157227006 | global_precision: 0.982695810564663 | global_recall: 0.9729486023444545 | global_auc: 0.9924249438862749| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 62 | global_acc: 98.504% | global_loss: 0.44348660111427307 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9926015828177803| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 63 | global_acc: 98.504% | global_loss: 0.4546467363834381 | global_f1: 0.9796656122910077 | global_precision: 0.9818840579710145 | global_recall: 0.9774571686203787 | global_auc: 0.9926174898183326| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 64 | global_acc: 98.371% | global_loss: 0.4479665458202362 | global_f1: 0.9777777777777779 | global_precision: 0.9835766423357665 | global_recall: 0.9720468890892696 | global_auc: 0.9923940795568453| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 65 | global_acc: 98.371% | global_loss: 0.45064789056777954 | global_f1: 0.9777979157227006 | global_precision: 0.982695810564663 | global_recall: 0.9729486023444545 | global_auc: 0.9925809274588542| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 66 | global_acc: 98.504% | global_loss: 0.5679516196250916 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9921801660121055| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 67 | global_acc: 98.404% | global_loss: 0.5605224967002869 | global_f1: 0.9783783783783783 | global_precision: 0.9774977497749775 | global_recall: 0.9792605951307484 | global_auc: 0.9927416593898075| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 68 | global_acc: 98.471% | global_loss: 0.5583829879760742 | global_f1: 0.9792979297929794 | global_precision: 0.977538185085355 | global_recall: 0.9810640216411182 | global_auc: 0.9927563793007662| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 69 | global_acc: 98.404% | global_loss: 0.563361644744873 | global_f1: 0.9784172661870504 | global_precision: 0.9757847533632287 | global_recall: 0.9810640216411182 | global_auc: 0.9927411845539701| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 70 | global_acc: 98.404% | global_loss: 0.5295987725257874 | global_f1: 0.9783001808318263 | global_precision: 0.9809610154125114 | global_recall: 0.975653742110009 | global_auc: 0.9922756080154189| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 71 | global_acc: 98.138% | global_loss: 0.506026566028595 | global_f1: 0.9746146872166819 | global_precision: 0.9799453053783045 | global_recall: 0.9693417493237151 | global_auc: 0.9915973050217214| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 72 | global_acc: 98.238% | global_loss: 0.587186872959137 | global_f1: 0.9760938204781237 | global_precision: 0.9765342960288809 | global_recall: 0.975653742110009 | global_auc: 0.9920346288279485| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 73 | global_acc: 98.105% | global_loss: 0.5726398229598999 | global_f1: 0.9742663656884876 | global_precision: 0.9755877034358047 | global_recall: 0.9729486023444545 | global_auc: 0.992139092712172| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 74 | global_acc: 98.271% | global_loss: 0.5384489893913269 | global_f1: 0.9764705882352942 | global_precision: 0.9800181653042689 | global_recall: 0.9729486023444545 | global_auc: 0.992192749161796| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 75 | global_acc: 98.172% | global_loss: 0.5723534226417542 | global_f1: 0.9750792931581332 | global_precision: 0.9799635701275046 | global_recall: 0.9702434625788999 | global_auc: 0.9918522918663943| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 76 | global_acc: 98.271% | global_loss: 0.5920416116714478 | global_f1: 0.9765976597659766 | global_precision: 0.9748427672955975 | global_recall: 0.9783588818755635 | global_auc: 0.9920360533354606| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 77 | global_acc: 98.271% | global_loss: 0.6150519847869873 | global_f1: 0.9765342960288809 | global_precision: 0.9774164408310749 | global_recall: 0.975653742110009 | global_auc: 0.9916485872921584| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 78 | global_acc: 98.338% | global_loss: 0.6135931611061096 | global_f1: 0.9775179856115108 | global_precision: 0.9748878923766816 | global_recall: 0.9801623083859333 | global_auc: 0.9919448848546836| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 79 | global_acc: 98.371% | global_loss: 0.6263526082038879 | global_f1: 0.9779378658262045 | global_precision: 0.9766187050359713 | global_recall: 0.9792605951307484 | global_auc: 0.9919247043315951| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 80 | global_acc: 98.471% | global_loss: 0.656968355178833 | global_f1: 0.9791855203619909 | global_precision: 0.9827429609445958 | global_recall: 0.975653742110009 | global_auc: 0.9915037623617574| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 81 | global_acc: 98.338% | global_loss: 0.658339262008667 | global_f1: 0.977416440831075 | global_precision: 0.9791855203619909 | global_recall: 0.975653742110009 | global_auc: 0.9913940752833227| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 82 | global_acc: 98.438% | global_loss: 0.6757350564002991 | global_f1: 0.9788573999100315 | global_precision: 0.9766606822262118 | global_recall: 0.9810640216411182 | global_auc: 0.9914852437640996| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 83 | global_acc: 98.570% | global_loss: 0.6845974922180176 | global_f1: 0.9806219017575485 | global_precision: 0.9801801801801802 | global_recall: 0.9810640216411182 | global_auc: 0.9914425085387354| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 84 | global_acc: 98.570% | global_loss: 0.6971182227134705 | global_f1: 0.98058690744921 | global_precision: 0.9819168173598554 | global_recall: 0.9792605951307484 | global_auc: 0.9913489658787715| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 85 | global_acc: 98.604% | global_loss: 0.7367185354232788 | global_f1: 0.9810298102981029 | global_precision: 0.9828054298642533 | global_recall: 0.9792605951307484 | global_auc: 0.9910946912878544| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 86 | global_acc: 98.604% | global_loss: 0.7496064901351929 | global_f1: 0.9810298102981029 | global_precision: 0.9828054298642533 | global_recall: 0.9792605951307484 | global_auc: 0.9911124976317562| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 87 | global_acc: 98.537% | global_loss: 0.7676546573638916 | global_f1: 0.9801084990958407 | global_precision: 0.9827742520398912 | global_recall: 0.9774571686203787 | global_auc: 0.9910011486278907| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 88 | global_acc: 98.471% | global_loss: 0.7799609303474426 | global_f1: 0.9791855203619909 | global_precision: 0.9827429609445958 | global_recall: 0.975653742110009 | global_auc: 0.9907357153947951| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 89 | global_acc: 98.438% | global_loss: 0.7456864714622498 | global_f1: 0.9787234042553192 | global_precision: 0.9827272727272728 | global_recall: 0.9747520288548241 | global_auc: 0.990908080803764| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 90 | global_acc: 98.504% | global_loss: 0.7643560171127319 | global_f1: 0.9796472184531885 | global_precision: 0.9827586206896551 | global_recall: 0.9765554553651938 | global_auc: 0.9908318696518646| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 91 | global_acc: 98.504% | global_loss: 0.7413690686225891 | global_f1: 0.9796472184531885 | global_precision: 0.9827586206896551 | global_recall: 0.9765554553651938 | global_auc: 0.991031538121483| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 92 | global_acc: 98.504% | global_loss: 0.7941548824310303 | global_f1: 0.9796656122910077 | global_precision: 0.9818840579710145 | global_recall: 0.9774571686203787 | global_auc: 0.9905721344488176| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 93 | global_acc: 98.504% | global_loss: 0.8231793642044067 | global_f1: 0.9797205948625507 | global_precision: 0.9792792792792793 | global_recall: 0.9801623083859333 | global_auc: 0.9903722285612806| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 94 | global_acc: 98.504% | global_loss: 0.8108790516853333 | global_f1: 0.9796472184531885 | global_precision: 0.9827586206896551 | global_recall: 0.9765554553651938 | global_auc: 0.9905156289841697| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 95 | global_acc: 98.537% | global_loss: 0.8259607553482056 | global_f1: 0.9800904977375565 | global_precision: 0.9836512261580381 | global_recall: 0.9765554553651938 | global_auc: 0.9900526640427239| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 96 | global_acc: 98.604% | global_loss: 0.8302258253097534 | global_f1: 0.9810126582278481 | global_precision: 0.9836808703535811 | global_recall: 0.9783588818755635 | global_auc: 0.9901174791345264| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 97 | global_acc: 98.504% | global_loss: 0.8511391282081604 | global_f1: 0.9797205948625507 | global_precision: 0.9792792792792793 | global_recall: 0.9801623083859333 | global_auc: 0.9901490557177122| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 98 | global_acc: 98.504% | global_loss: 0.8510258197784424 | global_f1: 0.979757085020243 | global_precision: 0.9775583482944344 | global_recall: 0.981965734896303 | global_auc: 0.990420899234612| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 99 | global_acc: 98.404% | global_loss: 0.9173125624656677 | global_f1: 0.9784172661870504 | global_precision: 0.9757847533632287 | global_recall: 0.9810640216411182 | global_auc: 0.9900032811156363| flobal_FPR: 0.018935978358881875 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedADMM 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5238cac1",
   "metadata": {},
   "source": [
    "## iid FedADMM PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0127a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedADMM-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6658962368965149 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.779044639791908| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6424379348754883 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8941882942519697| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6169272661209106 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9263776530858868| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.5448554158210754 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9594461704727134| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 91.456% | global_loss: 0.3947873115539551 | global_f1: 0.8718204488778055 | global_precision: 0.9754464285714286 | global_recall: 0.78809738503156 | global_auc: 0.9766209827107524| flobal_FPR: 0.21190261496844004 \n",
      "comm_round: 5 | global_acc: 90.791% | global_loss: 0.23287267982959747 | global_f1: 0.8868926092282564 | global_precision: 0.8104477611940298 | global_recall: 0.9792605951307484 | global_auc: 0.9828565269272281| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 6 | global_acc: 93.816% | global_loss: 0.1632508784532547 | global_f1: 0.9173333333333334 | global_precision: 0.9044697633654689 | global_recall: 0.9305680793507665 | global_auc: 0.985578285947091| flobal_FPR: 0.06943192064923355 \n",
      "comm_round: 7 | global_acc: 95.213% | global_loss: 0.1797589510679245 | global_f1: 0.9337016574585636 | global_precision: 0.9539040451552211 | global_recall: 0.9143372407574392 | global_auc: 0.9891122516667925| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 8 | global_acc: 96.476% | global_loss: 0.21852926909923553 | global_f1: 0.950925925925926 | global_precision: 0.9771646051379639 | global_recall: 0.9260595130748422 | global_auc: 0.9932112720329763| flobal_FPR: 0.0739404869251578 \n",
      "comm_round: 9 | global_acc: 96.642% | global_loss: 0.2853243947029114 | global_f1: 0.9534776600644864 | global_precision: 0.9745762711864406 | global_recall: 0.933273219116321 | global_auc: 0.9931789831960345| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 10 | global_acc: 96.676% | global_loss: 0.3376147449016571 | global_f1: 0.9545040946314832 | global_precision: 0.9632690541781451 | global_recall: 0.9458972046889089 | global_auc: 0.992281068627549| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 11 | global_acc: 96.609% | global_loss: 0.3693996071815491 | global_f1: 0.9542600896860988 | global_precision: 0.9491525423728814 | global_recall: 0.9594229035166817 | global_auc: 0.9927055718661666| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 12 | global_acc: 97.640% | global_loss: 0.33055317401885986 | global_f1: 0.9682042095835199 | global_precision: 0.9617437722419929 | global_recall: 0.9747520288548241 | global_auc: 0.9941497850655582| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 13 | global_acc: 97.739% | global_loss: 0.3638004958629608 | global_f1: 0.9695885509838997 | global_precision: 0.9618456078083407 | global_recall: 0.9774571686203787 | global_auc: 0.9940894809142109| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 14 | global_acc: 97.839% | global_loss: 0.3335939347743988 | global_f1: 0.9708127525819487 | global_precision: 0.9669051878354203 | global_recall: 0.9747520288548241 | global_auc: 0.994283926189618| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 15 | global_acc: 98.005% | global_loss: 0.3667985498905182 | global_f1: 0.9728506787330317 | global_precision: 0.9763851044504995 | global_recall: 0.9693417493237151 | global_auc: 0.9940066220605881| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 16 | global_acc: 97.972% | global_loss: 0.37666207551956177 | global_f1: 0.9723856948845632 | global_precision: 0.9763636363636363 | global_recall: 0.9684400360685302 | global_auc: 0.9938214360840099| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 17 | global_acc: 97.839% | global_loss: 0.39282503724098206 | global_f1: 0.9705215419501133 | global_precision: 0.9762773722627737 | global_recall: 0.9648331830477908 | global_auc: 0.9936604667351381| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 18 | global_acc: 97.806% | global_loss: 0.372688889503479 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9938990717434215| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 19 | global_acc: 97.773% | global_loss: 0.4388039708137512 | global_f1: 0.9698876404494382 | global_precision: 0.9668458781362007 | global_recall: 0.9729486023444545 | global_auc: 0.9933881483824005| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 20 | global_acc: 97.839% | global_loss: 0.43621084094047546 | global_f1: 0.9706281066425667 | global_precision: 0.9728260869565217 | global_recall: 0.9684400360685302 | global_auc: 0.9937214831402414| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 21 | global_acc: 97.706% | global_loss: 0.4295940697193146 | global_f1: 0.9690721649484536 | global_precision: 0.9634581105169341 | global_recall: 0.9747520288548241 | global_auc: 0.9938822150711946| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 22 | global_acc: 97.906% | global_loss: 0.4619671702384949 | global_f1: 0.97165991902834 | global_precision: 0.9694793536804309 | global_recall: 0.9738503155996393 | global_auc: 0.9937110367518189| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 23 | global_acc: 97.906% | global_loss: 0.4391952157020569 | global_f1: 0.9715061058344641 | global_precision: 0.9745916515426497 | global_recall: 0.9684400360685302 | global_auc: 0.9940332128674814| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 24 | global_acc: 98.172% | global_loss: 0.4549906849861145 | global_f1: 0.9752808988764045 | global_precision: 0.9722222222222222 | global_recall: 0.9783588818755635 | global_auc: 0.994170677842403| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 25 | global_acc: 97.939% | global_loss: 0.48837044835090637 | global_f1: 0.9722470904207698 | global_precision: 0.9653333333333334 | global_recall: 0.9792605951307484 | global_auc: 0.994072624241984| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 26 | global_acc: 98.005% | global_loss: 0.4968370497226715 | global_f1: 0.9732142857142857 | global_precision: 0.9637488947833776 | global_recall: 0.9828674481514879 | global_auc: 0.9941462237967778| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 27 | global_acc: 98.338% | global_loss: 0.49908095598220825 | global_f1: 0.9775179856115108 | global_precision: 0.9748878923766816 | global_recall: 0.9801623083859333 | global_auc: 0.9941571450210376| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 28 | global_acc: 98.438% | global_loss: 0.5036301016807556 | global_f1: 0.9788383610986042 | global_precision: 0.9775179856115108 | global_recall: 0.9801623083859333 | global_auc: 0.9941331658112498| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 29 | global_acc: 98.404% | global_loss: 0.5091295838356018 | global_f1: 0.9783978397839784 | global_precision: 0.9766397124887691 | global_recall: 0.9801623083859333 | global_auc: 0.99419655639554| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 30 | global_acc: 98.537% | global_loss: 0.5197969079017639 | global_f1: 0.9801444043321298 | global_precision: 0.981029810298103 | global_recall: 0.9792605951307484 | global_auc: 0.9942497380093267| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 31 | global_acc: 98.570% | global_loss: 0.548645555973053 | global_f1: 0.980639351643404 | global_precision: 0.97931654676259 | global_recall: 0.981965734896303 | global_auc: 0.9940569546593503| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 32 | global_acc: 98.504% | global_loss: 0.6033809781074524 | global_f1: 0.9797752808988763 | global_precision: 0.9767025089605734 | global_recall: 0.9828674481514879 | global_auc: 0.9937794131124017| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 33 | global_acc: 98.570% | global_loss: 0.6430936455726624 | global_f1: 0.9806567701304544 | global_precision: 0.9784560143626571 | global_recall: 0.9828674481514879 | global_auc: 0.9935795072248648| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 34 | global_acc: 98.537% | global_loss: 0.680641233921051 | global_f1: 0.9802158273381296 | global_precision: 0.9775784753363229 | global_recall: 0.9828674481514879 | global_auc: 0.9933898103078312| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 35 | global_acc: 98.504% | global_loss: 0.720565140247345 | global_f1: 0.9797752808988763 | global_precision: 0.9767025089605734 | global_recall: 0.9828674481514879 | global_auc: 0.9932210061676428| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 36 | global_acc: 98.504% | global_loss: 0.7333071827888489 | global_f1: 0.979757085020243 | global_precision: 0.9775583482944344 | global_recall: 0.981965734896303 | global_auc: 0.9932058114208464| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 37 | global_acc: 98.770% | global_loss: 0.7258735299110413 | global_f1: 0.983295711060948 | global_precision: 0.984629294755877 | global_recall: 0.981965734896303 | global_auc: 0.993296505065786| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 38 | global_acc: 98.504% | global_loss: 0.7300679087638855 | global_f1: 0.979757085020243 | global_precision: 0.9775583482944344 | global_recall: 0.981965734896303 | global_auc: 0.9932623168854947| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 39 | global_acc: 98.770% | global_loss: 0.7634503841400146 | global_f1: 0.983295711060948 | global_precision: 0.984629294755877 | global_recall: 0.981965734896303 | global_auc: 0.9932594678704705| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 40 | global_acc: 98.404% | global_loss: 0.8161160349845886 | global_f1: 0.9784366576819408 | global_precision: 0.9749328558639212 | global_recall: 0.981965734896303 | global_auc: 0.9929201976646624| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 41 | global_acc: 98.371% | global_loss: 0.8460425734519958 | global_f1: 0.977997305792546 | global_precision: 0.9740608228980322 | global_recall: 0.981965734896303 | global_auc: 0.9919182940477902| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 42 | global_acc: 98.504% | global_loss: 0.8594754934310913 | global_f1: 0.9797752808988763 | global_precision: 0.9767025089605734 | global_recall: 0.9828674481514879 | global_auc: 0.9914902295403921| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 43 | global_acc: 98.471% | global_loss: 0.8752577900886536 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9915054242871882| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 44 | global_acc: 98.570% | global_loss: 0.8855701088905334 | global_f1: 0.9806567701304544 | global_precision: 0.9784560143626571 | global_recall: 0.9828674481514879 | global_auc: 0.991204615784208| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 45 | global_acc: 98.504% | global_loss: 0.8957065939903259 | global_f1: 0.9797752808988763 | global_precision: 0.9767025089605734 | global_recall: 0.9828674481514879 | global_auc: 0.9912437897407919| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 46 | global_acc: 98.504% | global_loss: 0.9284601211547852 | global_f1: 0.9797752808988763 | global_precision: 0.9767025089605734 | global_recall: 0.9828674481514879 | global_auc: 0.9910294013602148| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 47 | global_acc: 98.471% | global_loss: 0.9450516104698181 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9910562295850267| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 48 | global_acc: 98.471% | global_loss: 1.0137323141098022 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9901723226737437| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 49 | global_acc: 98.404% | global_loss: 0.9603365063667297 | global_f1: 0.9784560143626571 | global_precision: 0.9740840035746202 | global_recall: 0.9828674481514879 | global_auc: 0.9903617821728583| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 50 | global_acc: 98.305% | global_loss: 1.0037118196487427 | global_f1: 0.9771402958314657 | global_precision: 0.9714795008912656 | global_recall: 0.9828674481514879 | global_auc: 0.9900685710432761| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 51 | global_acc: 98.471% | global_loss: 1.008176326751709 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.99009326250682| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 52 | global_acc: 98.338% | global_loss: 1.107613444328308 | global_f1: 0.9775784753363229 | global_precision: 0.9723461195361285 | global_recall: 0.9828674481514879 | global_auc: 0.9893665262577096| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 53 | global_acc: 98.570% | global_loss: 0.9921274185180664 | global_f1: 0.9806567701304544 | global_precision: 0.9784560143626571 | global_recall: 0.9828674481514879 | global_auc: 0.9903769769196545| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 54 | global_acc: 98.338% | global_loss: 1.0238022804260254 | global_f1: 0.9775784753363229 | global_precision: 0.9723461195361285 | global_recall: 0.9828674481514879 | global_auc: 0.9901837187338408| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 55 | global_acc: 98.305% | global_loss: 1.0896432399749756 | global_f1: 0.9771402958314657 | global_precision: 0.9714795008912656 | global_recall: 0.9828674481514879 | global_auc: 0.9897345240316792| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 56 | global_acc: 98.504% | global_loss: 1.0558969974517822 | global_f1: 0.9797752808988763 | global_precision: 0.9767025089605734 | global_recall: 0.9828674481514879 | global_auc: 0.989970517442857| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 57 | global_acc: 98.471% | global_loss: 1.063247799873352 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9899923598913766| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 58 | global_acc: 98.138% | global_loss: 1.1419681310653687 | global_f1: 0.9749552772808587 | global_precision: 0.9671694764862466 | global_recall: 0.9828674481514879 | global_auc: 0.9894747888286322| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 59 | global_acc: 98.371% | global_loss: 1.0782090425491333 | global_f1: 0.978017048003589 | global_precision: 0.9732142857142857 | global_recall: 0.9828674481514879 | global_auc: 0.9900215622953754| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 60 | global_acc: 98.471% | global_loss: 1.0804790258407593 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9896509529243003| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 61 | global_acc: 98.105% | global_loss: 1.3048778772354126 | global_f1: 0.9745422063421171 | global_precision: 0.9654867256637168 | global_recall: 0.9837691614066727 | global_auc: 0.9882525613832159| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 62 | global_acc: 98.438% | global_loss: 1.2102001905441284 | global_f1: 0.9788953749438708 | global_precision: 0.9749552772808586 | global_recall: 0.9828674481514879 | global_auc: 0.9887093534587754| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 63 | global_acc: 98.238% | global_loss: 1.2711225748062134 | global_f1: 0.9762863534675615 | global_precision: 0.9689165186500888 | global_recall: 0.9837691614066727 | global_auc: 0.988171839290861| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 64 | global_acc: 98.404% | global_loss: 1.1909831762313843 | global_f1: 0.9784753363228701 | global_precision: 0.9732381801962533 | global_recall: 0.9837691614066727 | global_auc: 0.9885920690069425| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 65 | global_acc: 98.271% | global_loss: 1.2642823457717896 | global_f1: 0.9767233661593555 | global_precision: 0.9697777777777777 | global_recall: 0.9837691614066727 | global_auc: 0.9880593031974021| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 66 | global_acc: 98.271% | global_loss: 1.229159951210022 | global_f1: 0.9767233661593555 | global_precision: 0.9697777777777777 | global_recall: 0.9837691614066727 | global_auc: 0.9883306718784647| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 67 | global_acc: 98.537% | global_loss: 1.199363350868225 | global_f1: 0.9801980198019802 | global_precision: 0.9784366576819407 | global_recall: 0.981965734896303 | global_auc: 0.9882307189346963| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 68 | global_acc: 98.404% | global_loss: 1.3025379180908203 | global_f1: 0.9784560143626571 | global_precision: 0.9740840035746202 | global_recall: 0.9828674481514879 | global_auc: 0.9870543131475871| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 69 | global_acc: 98.504% | global_loss: 1.2128609418869019 | global_f1: 0.979757085020243 | global_precision: 0.9775583482944344 | global_recall: 0.981965734896303 | global_auc: 0.9878190362636877| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 70 | global_acc: 98.271% | global_loss: 1.28043532371521 | global_f1: 0.9767025089605735 | global_precision: 0.9706144256455922 | global_recall: 0.9828674481514879 | global_auc: 0.9873014652009434| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 71 | global_acc: 98.205% | global_loss: 1.3195457458496094 | global_f1: 0.9757847533632287 | global_precision: 0.9705619982158786 | global_recall: 0.9810640216411182 | global_auc: 0.9869695549506147| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 72 | global_acc: 98.338% | global_loss: 1.319130778312683 | global_f1: 0.9775179856115108 | global_precision: 0.9748878923766816 | global_recall: 0.9801623083859333 | global_auc: 0.9868083481838241| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 73 | global_acc: 98.305% | global_loss: 1.3654283285140991 | global_f1: 0.9770786516853933 | global_precision: 0.9740143369175627 | global_recall: 0.9801623083859333 | global_auc: 0.9865350801594117| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 74 | global_acc: 98.072% | global_loss: 1.4410536289215088 | global_f1: 0.9740376007162043 | global_precision: 0.9671111111111111 | global_recall: 0.9810640216411182 | global_auc: 0.9860139478278871| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 75 | global_acc: 98.471% | global_loss: 1.313595175743103 | global_f1: 0.9792979297929794 | global_precision: 0.977538185085355 | global_recall: 0.9810640216411182 | global_auc: 0.9869477125020952| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 76 | global_acc: 95.047% | global_loss: 1.8582494258880615 | global_f1: 0.9360789360789361 | global_precision: 0.8927986906710311 | global_recall: 0.9837691614066727 | global_auc: 0.9836091417294757| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 77 | global_acc: 98.504% | global_loss: 1.2957950830459595 | global_f1: 0.9797205948625507 | global_precision: 0.9792792792792793 | global_recall: 0.9801623083859333 | global_auc: 0.9860165594249929| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 78 | global_acc: 98.072% | global_loss: 1.4826359748840332 | global_f1: 0.9740376007162043 | global_precision: 0.9671111111111111 | global_recall: 0.9810640216411182 | global_auc: 0.9852898231758824| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 79 | global_acc: 98.338% | global_loss: 1.456791639328003 | global_f1: 0.977538185085355 | global_precision: 0.9740376007162042 | global_recall: 0.9810640216411182 | global_auc: 0.984733315574473| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 80 | global_acc: 98.271% | global_loss: 1.442025065422058 | global_f1: 0.9765765765765766 | global_precision: 0.9756975697569757 | global_recall: 0.9774571686203787 | global_auc: 0.9848494129367125| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 81 | global_acc: 98.205% | global_loss: 1.4788583517074585 | global_f1: 0.9756756756756757 | global_precision: 0.9747974797479748 | global_recall: 0.9765554553651938 | global_auc: 0.9845820803602674| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 82 | global_acc: 98.238% | global_loss: 1.504196047782898 | global_f1: 0.9761153672825597 | global_precision: 0.9756756756756757 | global_recall: 0.9765554553651938 | global_auc: 0.9845450431649518| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 83 | global_acc: 98.105% | global_loss: 1.491298794746399 | global_f1: 0.9743358847366052 | global_precision: 0.9730215827338129 | global_recall: 0.975653742110009 | global_auc: 0.9847663166651709| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 84 | global_acc: 98.172% | global_loss: 1.5421833992004395 | global_f1: 0.9752140603875619 | global_precision: 0.9747747747747748 | global_recall: 0.975653742110009 | global_auc: 0.9847140847230592| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 85 | global_acc: 98.238% | global_loss: 1.5275615453720093 | global_f1: 0.9760938204781237 | global_precision: 0.9765342960288809 | global_recall: 0.975653742110009 | global_auc: 0.9841965136603147| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 86 | global_acc: 98.305% | global_loss: 1.4676518440246582 | global_f1: 0.9770580296896085 | global_precision: 0.9748653500897666 | global_recall: 0.9792605951307484 | global_auc: 0.984901882296743| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 87 | global_acc: 98.271% | global_loss: 1.5265003442764282 | global_f1: 0.9766606822262118 | global_precision: 0.9722966934763181 | global_recall: 0.9810640216411182 | global_auc: 0.9844823648344176| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 88 | global_acc: 98.271% | global_loss: 1.5891600847244263 | global_f1: 0.9766606822262118 | global_precision: 0.9722966934763181 | global_recall: 0.9810640216411182 | global_auc: 0.9839600454132996| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 89 | global_acc: 98.271% | global_loss: 1.5732425451278687 | global_f1: 0.9766606822262118 | global_precision: 0.9722966934763181 | global_recall: 0.9810640216411182 | global_auc: 0.9842326011839558| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 90 | global_acc: 98.138% | global_loss: 1.602339506149292 | global_f1: 0.974910394265233 | global_precision: 0.968833481745325 | global_recall: 0.9810640216411182 | global_auc: 0.9840004064594768| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 91 | global_acc: 98.238% | global_loss: 1.6018223762512207 | global_f1: 0.9762438368444645 | global_precision: 0.9705882352941176 | global_recall: 0.981965734896303 | global_auc: 0.984001830966989| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 92 | global_acc: 98.271% | global_loss: 1.5816242694854736 | global_f1: 0.9766816143497757 | global_precision: 0.9714540588760036 | global_recall: 0.981965734896303 | global_auc: 0.9843052510670749| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 93 | global_acc: 98.238% | global_loss: 1.5878785848617554 | global_f1: 0.9762438368444645 | global_precision: 0.9705882352941176 | global_recall: 0.981965734896303 | global_auc: 0.9838781362313515| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 94 | global_acc: 98.238% | global_loss: 1.5470523834228516 | global_f1: 0.9762225213100045 | global_precision: 0.9714285714285714 | global_recall: 0.9810640216411182 | global_auc: 0.9843527346508127| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 95 | global_acc: 98.105% | global_loss: 1.7334299087524414 | global_f1: 0.974473802060009 | global_precision: 0.9679715302491103 | global_recall: 0.9810640216411182 | global_auc: 0.9827530127146793| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 96 | global_acc: 98.371% | global_loss: 1.5249378681182861 | global_f1: 0.9779577147998201 | global_precision: 0.9757630161579892 | global_recall: 0.9801623083859333 | global_auc: 0.9846055847342178| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 97 | global_acc: 98.271% | global_loss: 1.5406591892242432 | global_f1: 0.9766397124887691 | global_precision: 0.973142345568487 | global_recall: 0.9801623083859333 | global_auc: 0.9843439501878214| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 98 | global_acc: 98.305% | global_loss: 1.6186964511871338 | global_f1: 0.9771197846567968 | global_precision: 0.9723214285714286 | global_recall: 0.981965734896303 | global_auc: 0.9836319338496698| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 99 | global_acc: 98.105% | global_loss: 1.6743710041046143 | global_f1: 0.9744966442953019 | global_precision: 0.9671403197158082 | global_recall: 0.981965734896303 | global_auc: 0.983366025780737| flobal_FPR: 0.018034265103697024 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedADMM 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, pgd_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and PGD attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                            train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                            Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.01\n",
    "                        alpha = 0.01\n",
    "                        num_iter = 5\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "\n",
    "                        # train_model(local_model, train_loader, loss, optimizer)\n",
    "                        # # FGSM attack\n",
    "                        # epsilon = 1\n",
    "                        # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                        # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-PGD-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-PGD-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-PGD-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb1f186",
   "metadata": {},
   "source": [
    "## non-iid FedADMM PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153b54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 361, (1,): 842}\n",
      "Client client_2: {(0,): 387, (1,): 816}\n",
      "Client client_3: {(0,): 973, (1,): 230}\n",
      "Client client_4: {(0,): 567, (1,): 636}\n",
      "Client client_5: {(0,): 541, (1,): 662}\n",
      "Client client_6: {(0,): 776, (1,): 427}\n",
      "Client client_7: {(0,): 662, (1,): 541}\n",
      "Client client_8: {(0,): 488, (1,): 715}\n",
      "Client client_9: {(0,): 377, (1,): 826}\n",
      "Client client_10: {(0,): 663, (1,): 540}\n",
      "|=======================|\n",
      "|Traditional FedADMM non iid pgd 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 39.362% | global_loss: 0.695409893989563 | global_f1: 0.5480673934588701 | global_precision: 0.3778612914246669 | global_recall: 0.9972948602344455 | global_auc: 0.8313810457879451| flobal_FPR: 0.002705139765554554 \n",
      "comm_round: 1 | global_acc: 74.568% | global_loss: 0.6588824391365051 | global_f1: 0.7353856796956071 | global_precision: 0.5965207631874299 | global_recall: 0.9585211902614968 | global_auc: 0.8916054247145406| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 2 | global_acc: 80.851% | global_loss: 0.5564587116241455 | global_f1: 0.7786318216756342 | global_precision: 0.6784996651038178 | global_recall: 0.9134355275022543 | global_auc: 0.9344009542300987| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 3 | global_acc: 83.012% | global_loss: 0.3517206907272339 | global_f1: 0.8075329566854991 | global_precision: 0.6934023285899095 | global_recall: 0.9666366095581606 | global_auc: 0.9739400595729042| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 4 | global_acc: 89.062% | global_loss: 0.23879291117191315 | global_f1: 0.8679245283018867 | global_precision: 0.7821997105643994 | global_recall: 0.9747520288548241 | global_auc: 0.9837325990471945| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 5 | global_acc: 94.116% | global_loss: 0.16897259652614594 | global_f1: 0.9200902934537245 | global_precision: 0.9213381555153707 | global_recall: 0.9188458070333634 | global_auc: 0.9859650397366371| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 6 | global_acc: 94.947% | global_loss: 0.20569685101509094 | global_f1: 0.9301470588235294 | global_precision: 0.9484536082474226 | global_recall: 0.9125338142470695 | global_auc: 0.9887326204148071| flobal_FPR: 0.08746618575293057 \n",
      "comm_round: 7 | global_acc: 96.243% | global_loss: 0.26043957471847534 | global_f1: 0.9486130059117781 | global_precision: 0.9568807339449541 | global_recall: 0.9404869251577999 | global_auc: 0.991010882762557| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 8 | global_acc: 96.277% | global_loss: 0.31522446870803833 | global_f1: 0.9493212669683257 | global_precision: 0.9527702089009991 | global_recall: 0.9458972046889089 | global_auc: 0.9906834834526832| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 9 | global_acc: 95.745% | global_loss: 0.3775254786014557 | global_f1: 0.9422903516681695 | global_precision: 0.9422903516681695 | global_recall: 0.9422903516681695 | global_auc: 0.9903437384110378| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 10 | global_acc: 96.642% | global_loss: 0.46171942353248596 | global_f1: 0.9543193125282678 | global_precision: 0.957350272232305 | global_recall: 0.951307484220018 | global_auc: 0.9909968751053541| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 11 | global_acc: 96.809% | global_loss: 0.6376137733459473 | global_f1: 0.9567567567567569 | global_precision: 0.9558955895589559 | global_recall: 0.957619477006312 | global_auc: 0.9907663423063061| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 12 | global_acc: 96.642% | global_loss: 0.8190704584121704 | global_f1: 0.9546882009869897 | global_precision: 0.95 | global_recall: 0.9594229035166817 | global_auc: 0.9900372318780091| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 13 | global_acc: 96.775% | global_loss: 0.9677072167396545 | global_f1: 0.9563652721547458 | global_precision: 0.9542190305206463 | global_recall: 0.9585211902614968 | global_auc: 0.9893245032861013| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 14 | global_acc: 96.742% | global_loss: 1.2402393817901611 | global_f1: 0.9557362240289069 | global_precision: 0.9574660633484163 | global_recall: 0.9540126239855726 | global_auc: 0.9868921567091218| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 15 | global_acc: 96.809% | global_loss: 1.356971025466919 | global_f1: 0.9563636363636364 | global_precision: 0.9642529789184234 | global_recall: 0.9486023444544635 | global_auc: 0.9863541677053701| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 16 | global_acc: 96.609% | global_loss: 1.5105443000793457 | global_f1: 0.953382084095064 | global_precision: 0.9666357738646896 | global_recall: 0.9404869251577999 | global_auc: 0.98530478050476| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 17 | global_acc: 96.676% | global_loss: 1.5460402965545654 | global_f1: 0.954954954954955 | global_precision: 0.9540954095409541 | global_recall: 0.9558160504959423 | global_auc: 0.9851214938715313| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 18 | global_acc: 96.908% | global_loss: 1.6607494354248047 | global_f1: 0.9580514208389717 | global_precision: 0.9584837545126353 | global_recall: 0.957619477006312 | global_auc: 0.984659003765923| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 19 | global_acc: 96.941% | global_loss: 1.7020798921585083 | global_f1: 0.9587443946188341 | global_precision: 0.9536128456735058 | global_recall: 0.9639314697926059 | global_auc: 0.9838005005719397| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 20 | global_acc: 93.883% | global_loss: 2.26587176322937 | global_f1: 0.9214346712211785 | global_precision: 0.8751013787510138 | global_recall: 0.9729486023444545 | global_auc: 0.9800212821422313| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 21 | global_acc: 97.241% | global_loss: 1.8261750936508179 | global_f1: 0.9628967367009387 | global_precision: 0.9547872340425532 | global_recall: 0.9711451758340848 | global_auc: 0.9829431844675499| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 22 | global_acc: 96.941% | global_loss: 1.984384536743164 | global_f1: 0.9591111111111111 | global_precision: 0.9456617002629273 | global_recall: 0.9729486023444545 | global_auc: 0.9813073750077754| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 23 | global_acc: 97.307% | global_loss: 1.9239132404327393 | global_f1: 0.9636281993713516 | global_precision: 0.9597495527728086 | global_recall: 0.9675383228133454 | global_auc: 0.98173971303771| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 24 | global_acc: 97.207% | global_loss: 1.9621268510818481 | global_f1: 0.9622302158273381 | global_precision: 0.9596412556053812 | global_recall: 0.9648331830477908 | global_auc: 0.9814448399826969| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 25 | global_acc: 97.174% | global_loss: 1.963010311126709 | global_f1: 0.9617977528089888 | global_precision: 0.9587813620071685 | global_recall: 0.9648331830477908 | global_auc: 0.9812337754529815| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 26 | global_acc: 97.241% | global_loss: 1.974407434463501 | global_f1: 0.9626294461954075 | global_precision: 0.9613309352517986 | global_recall: 0.9639314697926059 | global_auc: 0.9815758946738139| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 27 | global_acc: 97.141% | global_loss: 2.0654406547546387 | global_f1: 0.9614003590664273 | global_precision: 0.9571045576407506 | global_recall: 0.9657348963029756 | global_auc: 0.9805345796824393| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 28 | global_acc: 97.108% | global_loss: 2.109416961669922 | global_f1: 0.9610040340654415 | global_precision: 0.9554367201426025 | global_recall: 0.9666366095581606 | global_auc: 0.9799348620198283| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 29 | global_acc: 97.174% | global_loss: 2.2261950969696045 | global_f1: 0.9619346171070309 | global_precision: 0.9555160142348754 | global_recall: 0.9684400360685302 | global_auc: 0.9789726071953775| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 30 | global_acc: 97.141% | global_loss: 2.3246166706085205 | global_f1: 0.9615384615384615 | global_precision: 0.9538598047914818 | global_recall: 0.9693417493237151 | global_auc: 0.9782354245578448| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 31 | global_acc: 97.141% | global_loss: 2.390599489212036 | global_f1: 0.9615728328865059 | global_precision: 0.9530558015943312 | global_recall: 0.9702434625788999 | global_auc: 0.9777985755874552| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 32 | global_acc: 97.108% | global_loss: 2.489818572998047 | global_f1: 0.9610738255033558 | global_precision: 0.9538188277087034 | global_recall: 0.9684400360685302 | global_auc: 0.9777083567783529| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 33 | global_acc: 97.108% | global_loss: 2.5475504398345947 | global_f1: 0.9610738255033558 | global_precision: 0.9538188277087034 | global_recall: 0.9684400360685302 | global_auc: 0.977256313061167| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 34 | global_acc: 97.108% | global_loss: 2.5814120769500732 | global_f1: 0.9611086276262852 | global_precision: 0.9530141843971631 | global_recall: 0.9693417493237151 | global_auc: 0.9770554575019553| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 35 | global_acc: 97.108% | global_loss: 2.5633544921875 | global_f1: 0.9612472160356347 | global_precision: 0.9498239436619719 | global_recall: 0.9729486023444545 | global_auc: 0.9769393601397157| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 36 | global_acc: 96.576% | global_loss: 3.0581021308898926 | global_f1: 0.9545655050727834 | global_precision: 0.9343696027633851 | global_recall: 0.975653742110009 | global_auc: 0.9727156953662194| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 37 | global_acc: 96.809% | global_loss: 2.889514446258545 | global_f1: 0.9574468085106382 | global_precision: 0.941586748038361 | global_recall: 0.9738503155996393 | global_auc: 0.9732819371022953| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 38 | global_acc: 96.941% | global_loss: 2.718919038772583 | global_f1: 0.9587443946188341 | global_precision: 0.9536128456735058 | global_recall: 0.9639314697926059 | global_auc: 0.9737543987604885| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 39 | global_acc: 96.941% | global_loss: 2.7889838218688965 | global_f1: 0.958670260557053 | global_precision: 0.955237242614145 | global_recall: 0.9621280432822362 | global_auc: 0.9724222468187187| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 40 | global_acc: 96.908% | global_loss: 2.719449043273926 | global_f1: 0.9585746102449889 | global_precision: 0.9471830985915493 | global_recall: 0.9702434625788999 | global_auc: 0.9746413921047146| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 41 | global_acc: 96.443% | global_loss: 3.3207180500030518 | global_f1: 0.9530495831505045 | global_precision: 0.9282051282051282 | global_recall: 0.9792605951307484 | global_auc: 0.9705751354113099| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 42 | global_acc: 93.650% | global_loss: 6.161489486694336 | global_f1: 0.9193752638243985 | global_precision: 0.8642857142857143 | global_recall: 0.981965734896303 | global_auc: 0.9483801212825695| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 43 | global_acc: 96.476% | global_loss: 3.4012646675109863 | global_f1: 0.9532215357458075 | global_precision: 0.9334485738980121 | global_recall: 0.9738503155996393 | global_auc: 0.9682736061075284| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 44 | global_acc: 96.543% | global_loss: 3.3419666290283203 | global_f1: 0.9540229885057471 | global_precision: 0.9358196010407632 | global_recall: 0.9729486023444545 | global_auc: 0.9686567986282942| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 45 | global_acc: 96.642% | global_loss: 3.277188301086426 | global_f1: 0.9551708832667554 | global_precision: 0.9405594405594405 | global_recall: 0.9702434625788999 | global_auc: 0.9686029047607515| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 46 | global_acc: 96.676% | global_loss: 3.1138925552368164 | global_f1: 0.9554764024933216 | global_precision: 0.9437115215479331 | global_recall: 0.9675383228133454 | global_auc: 0.969366440787259| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 47 | global_acc: 96.609% | global_loss: 3.1922972202301025 | global_f1: 0.9543828264758497 | global_precision: 0.9467613132209406 | global_recall: 0.9621280432822362 | global_auc: 0.9685024769811457| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 48 | global_acc: 96.044% | global_loss: 3.360427141189575 | global_f1: 0.9465648854961831 | global_precision: 0.9427549194991055 | global_recall: 0.9504057709648331 | global_auc: 0.966912014343841| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 49 | global_acc: 96.543% | global_loss: 3.370290517807007 | global_f1: 0.9535299374441465 | global_precision: 0.9450841452612931 | global_recall: 0.9621280432822362 | global_auc: 0.9662284881559322| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 50 | global_acc: 96.343% | global_loss: 3.5182812213897705 | global_f1: 0.9508928571428572 | global_precision: 0.9416445623342176 | global_recall: 0.9603246167718665 | global_auc: 0.9649594893805338| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 51 | global_acc: 96.376% | global_loss: 3.535358190536499 | global_f1: 0.9511867442901926 | global_precision: 0.9448398576512456 | global_recall: 0.957619477006312 | global_auc: 0.9640022203323756| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 52 | global_acc: 96.509% | global_loss: 3.4171555042266846 | global_f1: 0.9529359031824294 | global_precision: 0.9474153297682709 | global_recall: 0.9585211902614968 | global_auc: 0.964601700577068| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 53 | global_acc: 96.543% | global_loss: 3.328248977661133 | global_f1: 0.9532794249775383 | global_precision: 0.9498657117278424 | global_recall: 0.9567177637511272 | global_auc: 0.9656128634927689| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 54 | global_acc: 96.543% | global_loss: 3.3932206630706787 | global_f1: 0.9531953195319534 | global_precision: 0.9514824797843666 | global_recall: 0.9549143372407575 | global_auc: 0.9644955747674135| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 55 | global_acc: 96.443% | global_loss: 3.497382879257202 | global_f1: 0.9520394441954281 | global_precision: 0.946524064171123 | global_recall: 0.957619477006312 | global_auc: 0.9636460934543406| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 56 | global_acc: 96.476% | global_loss: 3.5239362716674805 | global_f1: 0.9525089605734767 | global_precision: 0.9465716829919858 | global_recall: 0.9585211902614968 | global_auc: 0.9633863582512936| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 57 | global_acc: 95.512% | global_loss: 4.3867082595825195 | global_f1: 0.9407634927599825 | global_precision: 0.9162393162393162 | global_recall: 0.9666366095581606 | global_auc: 0.9590264155924693| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 58 | global_acc: 95.512% | global_loss: 4.395416259765625 | global_f1: 0.9409190371991247 | global_precision: 0.9141156462585034 | global_recall: 0.9693417493237151 | global_auc: 0.9588718565274021| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 59 | global_acc: 94.947% | global_loss: 5.036726474761963 | global_f1: 0.9343696027633851 | global_precision: 0.8964374482187241 | global_recall: 0.975653742110009 | global_auc: 0.9552806730892963| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 60 | global_acc: 94.781% | global_loss: 5.037232875823975 | global_f1: 0.932762312633833 | global_precision: 0.8882544861337683 | global_recall: 0.981965734896303 | global_auc: 0.9569741751033124| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 61 | global_acc: 95.479% | global_loss: 4.449222564697266 | global_f1: 0.9410745233968802 | global_precision: 0.9057547956630525 | global_recall: 0.9792605951307484 | global_auc: 0.9608977436275844| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 62 | global_acc: 95.778% | global_loss: 4.182387351989746 | global_f1: 0.9447104919460165 | global_precision: 0.9132996632996633 | global_recall: 0.9783588818755635 | global_auc: 0.9628960902491986| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 63 | global_acc: 95.678% | global_loss: 4.290846347808838 | global_f1: 0.9433304272013949 | global_precision: 0.9130801687763713 | global_recall: 0.975653742110009 | global_auc: 0.9611152184411045| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 64 | global_acc: 95.711% | global_loss: 4.288565635681152 | global_f1: 0.9434954007884362 | global_precision: 0.9173764906303237 | global_recall: 0.9711451758340848 | global_auc: 0.959994843282806| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 65 | global_acc: 95.612% | global_loss: 4.3155837059021 | global_f1: 0.9416445623342174 | global_precision: 0.9236773633998265 | global_recall: 0.9603246167718665 | global_auc: 0.9582612176405312| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 66 | global_acc: 95.678% | global_loss: 4.262516975402832 | global_f1: 0.9425795053003534 | global_precision: 0.9238095238095239 | global_recall: 0.9621280432822362 | global_auc: 0.9585565655313816| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 67 | global_acc: 95.678% | global_loss: 4.267241954803467 | global_f1: 0.9424269264836137 | global_precision: 0.9260226283724978 | global_recall: 0.9594229035166817 | global_auc: 0.958592415637104| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 68 | global_acc: 95.645% | global_loss: 4.289127349853516 | global_f1: 0.9417518897287683 | global_precision: 0.9289473684210526 | global_recall: 0.9549143372407575 | global_auc: 0.9574017647748733| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 69 | global_acc: 95.445% | global_loss: 4.429762840270996 | global_f1: 0.9387572641931158 | global_precision: 0.9308510638297872 | global_recall: 0.9467989179440938 | global_auc: 0.9543687508636076| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 70 | global_acc: 95.412% | global_loss: 4.507380485534668 | global_f1: 0.9381720430107527 | global_precision: 0.9323241317898486 | global_recall: 0.9440937781785392 | global_auc: 0.9537676086934844| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 71 | global_acc: 95.180% | global_loss: 4.795766830444336 | global_f1: 0.9366535605067715 | global_precision: 0.9084745762711864 | global_recall: 0.9666366095581606 | global_auc: 0.955280910507215| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 72 | global_acc: 90.426% | global_loss: 9.48304557800293 | global_f1: 0.8827361563517915 | global_precision: 0.8047512991833704 | global_recall: 0.9774571686203787 | global_auc: 0.9208980475225202| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 73 | global_acc: 90.359% | global_loss: 9.640957832336426 | global_f1: 0.8824006488240066 | global_precision: 0.8017686072218129 | global_recall: 0.9810640216411182 | global_auc: 0.919670359464974| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 74 | global_acc: 90.459% | global_loss: 9.484912872314453 | global_f1: 0.8834754364596021 | global_precision: 0.8035450516986706 | global_recall: 0.9810640216411182 | global_auc: 0.9209918276004029| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 75 | global_acc: 89.927% | global_loss: 10.07315731048584 | global_f1: 0.8779701973419252 | global_precision: 0.7933042212518195 | global_recall: 0.9828674481514879 | global_auc: 0.9166439932554317| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 76 | global_acc: 94.448% | global_loss: 5.551861763000488 | global_f1: 0.9281720430107527 | global_precision: 0.8873355263157895 | global_recall: 0.9729486023444545 | global_auc: 0.9503955619943295| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 77 | global_acc: 90.492% | global_loss: 9.445659637451172 | global_f1: 0.883834281072299 | global_precision: 0.804138950480414 | global_recall: 0.9810640216411182 | global_auc: 0.9213904522858836| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 78 | global_acc: 89.727% | global_loss: 10.208267211914062 | global_f1: 0.8758537565287263 | global_precision: 0.7898550724637681 | global_recall: 0.9828674481514879 | global_auc: 0.9155862964276676| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 79 | global_acc: 90.426% | global_loss: 9.574470520019531 | global_f1: 0.882640586797066 | global_precision: 0.8052044609665427 | global_recall: 0.9765554553651938 | global_auc: 0.9192693606003065| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 80 | global_acc: 90.691% | global_loss: 9.308510780334473 | global_f1: 0.8855273916598528 | global_precision: 0.8100224382946896 | global_recall: 0.9765554553651938 | global_auc: 0.9213572137772668| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 81 | global_acc: 94.149% | global_loss: 5.8182783126831055 | global_f1: 0.9235447437011295 | global_precision: 0.8910310142497905 | global_recall: 0.9585211902614968 | global_auc: 0.94528442904077| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 82 | global_acc: 94.781% | global_loss: 5.219414710998535 | global_f1: 0.9306843267108168 | global_precision: 0.9117647058823529 | global_recall: 0.9504057709648331 | global_auc: 0.9483074713994505| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 83 | global_acc: 94.714% | global_loss: 5.285904407501221 | global_f1: 0.9296771340114993 | global_precision: 0.9123263888888888 | global_recall: 0.9477006311992786 | global_auc: 0.9472573719450843| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 84 | global_acc: 94.648% | global_loss: 5.352394104003906 | global_f1: 0.9283489096573209 | global_precision: 0.9165202108963093 | global_recall: 0.9404869251577999 | global_auc: 0.9450273054348286| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 85 | global_acc: 94.182% | global_loss: 5.817819118499756 | global_f1: 0.9214894571556752 | global_precision: 0.9169642857142857 | global_recall: 0.9260595130748422 | global_auc: 0.9385431846574843| flobal_FPR: 0.0739404869251578 \n",
      "comm_round: 86 | global_acc: 94.082% | global_loss: 5.917553424835205 | global_f1: 0.9197475202885482 | global_precision: 0.9197475202885482 | global_recall: 0.9197475202885482 | global_auc: 0.9364403741516465| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 87 | global_acc: 93.318% | global_loss: 6.682180881500244 | global_f1: 0.9085116067364588 | global_precision: 0.9172794117647058 | global_recall: 0.8999098286744815 | global_auc: 0.9262582318727858| flobal_FPR: 0.10009017132551848 \n",
      "comm_round: 88 | global_acc: 92.553% | global_loss: 7.446808338165283 | global_f1: 0.8974358974358975 | global_precision: 0.9116279069767442 | global_recall: 0.8836789900811542 | global_auc: 0.9168263302169858| flobal_FPR: 0.11632100991884581 \n",
      "comm_round: 89 | global_acc: 93.085% | global_loss: 6.903632164001465 | global_f1: 0.9061371841155235 | global_precision: 0.9069557362240289 | global_recall: 0.9053201082055906 | global_auc: 0.925966920086553| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 90 | global_acc: 92.952% | global_loss: 7.020258903503418 | global_f1: 0.9045045045045045 | global_precision: 0.9036903690369037 | global_recall: 0.9053201082055906 | global_auc: 0.9253382374378617| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 91 | global_acc: 92.819% | global_loss: 7.180850982666016 | global_f1: 0.9035714285714287 | global_precision: 0.8947833775419982 | global_recall: 0.9125338142470695 | global_auc: 0.9249346269760887| flobal_FPR: 0.08746618575293057 \n",
      "comm_round: 92 | global_acc: 92.719% | global_loss: 7.280585289001465 | global_f1: 0.9024498886414254 | global_precision: 0.891725352112676 | global_recall: 0.9134355275022543 | global_auc: 0.924332297716372| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 93 | global_acc: 92.387% | global_loss: 7.61303186416626 | global_f1: 0.8970786516853932 | global_precision: 0.8942652329749103 | global_recall: 0.8999098286744815 | global_auc: 0.9188859306616222| flobal_FPR: 0.10009017132551848 \n",
      "comm_round: 94 | global_acc: 90.924% | global_loss: 9.042522430419922 | global_f1: 0.8753993610223643 | global_precision: 0.8863216266173752 | global_recall: 0.8647430117222723 | global_auc: 0.9008293482735682| flobal_FPR: 0.13525698827772767 \n",
      "comm_round: 95 | global_acc: 90.160% | global_loss: 9.840425491333008 | global_f1: 0.8638454461821528 | global_precision: 0.8816901408450705 | global_recall: 0.8467087466185753 | global_auc: 0.8901790178590507| flobal_FPR: 0.1532912533814247 \n",
      "comm_round: 96 | global_acc: 90.492% | global_loss: 9.507978439331055 | global_f1: 0.8675925925925927 | global_precision: 0.8915318744053282 | global_recall: 0.8449053201082056 | global_auc: 0.8923960263837784| flobal_FPR: 0.1550946798917944 \n",
      "comm_round: 97 | global_acc: 90.326% | global_loss: 9.674201965332031 | global_f1: 0.8647140864714087 | global_precision: 0.8925143953934741 | global_recall: 0.8385933273219116 | global_auc: 0.8898074588163007| flobal_FPR: 0.16140667267808836 \n",
      "comm_round: 98 | global_acc: 91.090% | global_loss: 8.909574508666992 | global_f1: 0.8764976958525346 | global_precision: 0.8963242224316682 | global_recall: 0.8575293056807936 | global_auc: 0.8998020409393963| flobal_FPR: 0.1424706943192065 \n",
      "comm_round: 99 | global_acc: 91.456% | global_loss: 8.543883323669434 | global_f1: 0.8816213726393367 | global_precision: 0.9011299435028248 | global_recall: 0.8629395852119026 | global_auc: 0.9037987341826248| flobal_FPR: 0.13706041478809738 \n",
      "===================================================================================================\n",
      "Working with: Malgenome\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 110, (1,): 194}\n",
      "Client client_2: {(0,): 224, (1,): 80}\n",
      "Client client_3: {(0,): 156, (1,): 148}\n",
      "Client client_4: {(0,): 99, (1,): 205}\n",
      "Client client_5: {(0,): 178, (1,): 126}\n",
      "Client client_6: {(0,): 81, (1,): 223}\n",
      "Client client_7: {(0,): 165, (1,): 139}\n",
      "Client client_8: {(0,): 99, (1,): 205}\n",
      "Client client_9: {(0,): 162, (1,): 142}\n",
      "Client client_10: {(0,): 115, (1,): 189}\n",
      "|=======================|\n",
      "|Traditional FedADMM non iid pgd 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 65.000% | global_loss: 0.685967206954956 | global_f1: 0.022058823529411763 | global_precision: 0.23076923076923078 | global_recall: 0.011583011583011582 | global_auc: 0.4919042224431446| flobal_FPR: 0.9884169884169884 \n",
      "comm_round: 1 | global_acc: 66.053% | global_loss: 0.6874552965164185 | global_f1: 0.556701030927835 | global_precision: 0.5015479876160991 | global_recall: 0.6254826254826255 | global_auc: 0.7279148267172219| flobal_FPR: 0.3745173745173745 \n",
      "comm_round: 2 | global_acc: 50.395% | global_loss: 0.6881858706474304 | global_f1: 0.5787709497206704 | global_precision: 0.40723270440251574 | global_recall: 1.0 | global_auc: 0.8931403602062286| flobal_FPR: 0.0 \n",
      "comm_round: 3 | global_acc: 45.789% | global_loss: 0.685254693031311 | global_f1: 0.556989247311828 | global_precision: 0.3859910581222057 | global_recall: 1.0 | global_auc: 0.9658289598409359| flobal_FPR: 0.0 \n",
      "comm_round: 4 | global_acc: 49.605% | global_loss: 0.6770713329315186 | global_f1: 0.5749167591564929 | global_precision: 0.40342679127725856 | global_recall: 1.0 | global_auc: 0.9815812390662689| flobal_FPR: 0.0 \n",
      "comm_round: 5 | global_acc: 62.895% | global_loss: 0.6572726368904114 | global_f1: 0.6475 | global_precision: 0.4787430683918669 | global_recall: 1.0 | global_auc: 0.9853960033600753| flobal_FPR: 0.0 \n",
      "comm_round: 6 | global_acc: 74.868% | global_loss: 0.614629328250885 | global_f1: 0.7306064880112836 | global_precision: 0.5755555555555556 | global_recall: 1.0 | global_auc: 0.9869219090775978| flobal_FPR: 0.0 \n",
      "comm_round: 7 | global_acc: 82.895% | global_loss: 0.5209261178970337 | global_f1: 0.7981366459627329 | global_precision: 0.6675324675324675 | global_recall: 0.9922779922779923 | global_auc: 0.989257007221079| flobal_FPR: 0.007722007722007722 \n",
      "comm_round: 8 | global_acc: 92.500% | global_loss: 0.37447404861450195 | global_f1: 0.8991150442477878 | global_precision: 0.8300653594771242 | global_recall: 0.9806949806949807 | global_auc: 0.99297929238049| flobal_FPR: 0.019305019305019305 \n",
      "comm_round: 9 | global_acc: 95.263% | global_loss: 0.23053666949272156 | global_f1: 0.933579335793358 | global_precision: 0.8939929328621908 | global_recall: 0.9768339768339769 | global_auc: 0.9950369531207854| flobal_FPR: 0.023166023166023165 \n",
      "comm_round: 10 | global_acc: 96.974% | global_loss: 0.12945948541164398 | global_f1: 0.9565217391304347 | global_precision: 0.937037037037037 | global_recall: 0.9768339768339769 | global_auc: 0.9959617444647384| flobal_FPR: 0.023166023166023165 \n",
      "comm_round: 11 | global_acc: 97.763% | global_loss: 0.07796479016542435 | global_f1: 0.9674952198852772 | global_precision: 0.9583333333333334 | global_recall: 0.9768339768339769 | global_auc: 0.996424140136715| flobal_FPR: 0.023166023166023165 \n",
      "comm_round: 12 | global_acc: 98.026% | global_loss: 0.06423164904117584 | global_f1: 0.9709864603481624 | global_precision: 0.9728682170542635 | global_recall: 0.9691119691119691 | global_auc: 0.996547445649242| flobal_FPR: 0.03088803088803089 \n",
      "comm_round: 13 | global_acc: 98.289% | global_loss: 0.07077407091856003 | global_f1: 0.9748549323017408 | global_precision: 0.9767441860465116 | global_recall: 0.972972972972973 | global_auc: 0.996701577539901| flobal_FPR: 0.02702702702702703 \n",
      "comm_round: 14 | global_acc: 98.289% | global_loss: 0.08920999616384506 | global_f1: 0.9748549323017408 | global_precision: 0.9767441860465116 | global_recall: 0.972972972972973 | global_auc: 0.9970560808884162| flobal_FPR: 0.02702702702702703 \n",
      "comm_round: 15 | global_acc: 98.158% | global_loss: 0.11373668164014816 | global_f1: 0.972972972972973 | global_precision: 0.972972972972973 | global_recall: 0.972972972972973 | global_auc: 0.9974259974259975| flobal_FPR: 0.02702702702702703 \n",
      "comm_round: 16 | global_acc: 98.026% | global_loss: 0.14192989468574524 | global_f1: 0.9710982658959538 | global_precision: 0.9692307692307692 | global_recall: 0.972972972972973 | global_auc: 0.9978652733143751| flobal_FPR: 0.02702702702702703 \n",
      "comm_round: 17 | global_acc: 98.026% | global_loss: 0.2780725061893463 | global_f1: 0.9710982658959538 | global_precision: 0.9692307692307692 | global_recall: 0.972972972972973 | global_auc: 0.9973566380752009| flobal_FPR: 0.02702702702702703 \n",
      "comm_round: 18 | global_acc: 98.289% | global_loss: 0.5200386643409729 | global_f1: 0.9751434034416827 | global_precision: 0.9659090909090909 | global_recall: 0.9845559845559846 | global_auc: 0.9957382532232831| flobal_FPR: 0.015444015444015444 \n",
      "comm_round: 19 | global_acc: 98.158% | global_loss: 0.647169291973114 | global_f1: 0.9732824427480916 | global_precision: 0.9622641509433962 | global_recall: 0.9845559845559846 | global_auc: 0.9949945668508544| flobal_FPR: 0.015444015444015444 \n",
      "comm_round: 20 | global_acc: 98.026% | global_loss: 0.7649486064910889 | global_f1: 0.97131931166348 | global_precision: 0.9621212121212122 | global_recall: 0.9806949806949807 | global_auc: 0.9941275749658982| flobal_FPR: 0.019305019305019305 \n",
      "comm_round: 21 | global_acc: 98.026% | global_loss: 0.7704886794090271 | global_f1: 0.97131931166348 | global_precision: 0.9621212121212122 | global_recall: 0.9806949806949807 | global_auc: 0.9941545480467637| flobal_FPR: 0.019305019305019305 \n",
      "comm_round: 22 | global_acc: 97.895% | global_loss: 0.8881813287734985 | global_f1: 0.9695817490494297 | global_precision: 0.9550561797752809 | global_recall: 0.9845559845559846 | global_auc: 0.9932104902164782| flobal_FPR: 0.015444015444015444 \n",
      "comm_round: 23 | global_acc: 98.026% | global_loss: 1.0156021118164062 | global_f1: 0.9715370018975333 | global_precision: 0.9552238805970149 | global_recall: 0.9884169884169884 | global_auc: 0.9921893664408634| flobal_FPR: 0.011583011583011582 \n",
      "comm_round: 24 | global_acc: 97.368% | global_loss: 1.0491232872009277 | global_f1: 0.9624060150375939 | global_precision: 0.9377289377289377 | global_recall: 0.9884169884169884 | global_auc: 0.9922818455752587| flobal_FPR: 0.011583011583011582 \n",
      "comm_round: 25 | global_acc: 97.368% | global_loss: 1.316997766494751 | global_f1: 0.9624060150375939 | global_precision: 0.9377289377289377 | global_recall: 0.9884169884169884 | global_auc: 0.9904823557518169| flobal_FPR: 0.011583011583011582 \n",
      "comm_round: 26 | global_acc: 97.368% | global_loss: 1.5774338245391846 | global_f1: 0.9625468164794008 | global_precision: 0.9345454545454546 | global_recall: 0.9922779922779923 | global_auc: 0.9886404796584437| flobal_FPR: 0.007722007722007722 \n",
      "comm_round: 27 | global_acc: 97.237% | global_loss: 2.0471887588500977 | global_f1: 0.960747663551402 | global_precision: 0.9311594202898551 | global_recall: 0.9922779922779923 | global_auc: 0.9847409428247752| flobal_FPR: 0.007722007722007722 \n",
      "comm_round: 28 | global_acc: 97.237% | global_loss: 2.282428026199341 | global_f1: 0.9608938547486033 | global_precision: 0.9280575539568345 | global_recall: 0.9961389961389961 | global_auc: 0.982775761218875| flobal_FPR: 0.003861003861003861 \n",
      "comm_round: 29 | global_acc: 97.237% | global_loss: 2.3977465629577637 | global_f1: 0.9608938547486033 | global_precision: 0.9280575539568345 | global_recall: 0.9961389961389961 | global_auc: 0.9818509698749219| flobal_FPR: 0.003861003861003861 \n",
      "comm_round: 30 | global_acc: 97.237% | global_loss: 2.3968348503112793 | global_f1: 0.9608938547486033 | global_precision: 0.9280575539568345 | global_recall: 0.9961389961389961 | global_auc: 0.981858676469455| flobal_FPR: 0.003861003861003861 \n",
      "comm_round: 31 | global_acc: 97.368% | global_loss: 2.4003446102142334 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9818663830639879| flobal_FPR: 0.0 \n",
      "comm_round: 32 | global_acc: 97.237% | global_loss: 2.5183398723602295 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9808760856665049| flobal_FPR: 0.0 \n",
      "comm_round: 33 | global_acc: 97.237% | global_loss: 2.6349732875823975 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9799628542143513| flobal_FPR: 0.0 \n",
      "comm_round: 34 | global_acc: 97.237% | global_loss: 2.6377627849578857 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9800399201596806| flobal_FPR: 0.0 \n",
      "comm_round: 35 | global_acc: 97.237% | global_loss: 2.640745162963867 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 36 | global_acc: 97.237% | global_loss: 2.5277493000030518 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 37 | global_acc: 97.237% | global_loss: 2.519770383834839 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 38 | global_acc: 97.237% | global_loss: 2.520826578140259 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 39 | global_acc: 97.105% | global_loss: 2.766810894012451 | global_f1: 0.9592592592592593 | global_precision: 0.9217081850533808 | global_recall: 1.0 | global_auc: 0.9790419161676647| flobal_FPR: 0.0 \n",
      "comm_round: 40 | global_acc: 97.105% | global_loss: 2.7694990634918213 | global_f1: 0.9592592592592593 | global_precision: 0.9217081850533808 | global_recall: 1.0 | global_auc: 0.9790419161676647| flobal_FPR: 0.0 \n",
      "comm_round: 41 | global_acc: 97.105% | global_loss: 2.7726542949676514 | global_f1: 0.9592592592592593 | global_precision: 0.9217081850533808 | global_recall: 1.0 | global_auc: 0.9790419161676647| flobal_FPR: 0.0 \n",
      "comm_round: 42 | global_acc: 97.105% | global_loss: 2.7761173248291016 | global_f1: 0.9592592592592593 | global_precision: 0.9217081850533808 | global_recall: 1.0 | global_auc: 0.9790419161676647| flobal_FPR: 0.0 \n",
      "comm_round: 43 | global_acc: 96.974% | global_loss: 2.7852609157562256 | global_f1: 0.9574861367837338 | global_precision: 0.9184397163120568 | global_recall: 1.0 | global_auc: 0.9790419161676647| flobal_FPR: 0.0 \n",
      "comm_round: 44 | global_acc: 96.842% | global_loss: 2.9304134845733643 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9780439121756487| flobal_FPR: 0.0 \n",
      "comm_round: 45 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 46 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 47 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916168| flobal_FPR: 0.0 \n",
      "comm_round: 48 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916168| flobal_FPR: 0.0 \n",
      "comm_round: 49 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916168| flobal_FPR: 0.0 \n",
      "comm_round: 50 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916168| flobal_FPR: 0.0 \n",
      "comm_round: 51 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 52 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 53 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 54 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 55 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 56 | global_acc: 96.842% | global_loss: 3.1578948497772217 | global_f1: 0.955719557195572 | global_precision: 0.9151943462897526 | global_recall: 1.0 | global_auc: 0.9760479041916167| flobal_FPR: 0.0 \n",
      "comm_round: 57 | global_acc: 96.974% | global_loss: 3.026315689086914 | global_f1: 0.9574861367837338 | global_precision: 0.9184397163120568 | global_recall: 1.0 | global_auc: 0.9770459081836328| flobal_FPR: 0.0 \n",
      "comm_round: 58 | global_acc: 96.974% | global_loss: 3.026315689086914 | global_f1: 0.9574861367837338 | global_precision: 0.9184397163120568 | global_recall: 1.0 | global_auc: 0.9770459081836328| flobal_FPR: 0.0 \n",
      "comm_round: 59 | global_acc: 96.974% | global_loss: 3.026315689086914 | global_f1: 0.9574861367837338 | global_precision: 0.9184397163120568 | global_recall: 1.0 | global_auc: 0.9770459081836328| flobal_FPR: 0.0 \n",
      "comm_round: 60 | global_acc: 96.974% | global_loss: 3.026315689086914 | global_f1: 0.9574861367837338 | global_precision: 0.9184397163120568 | global_recall: 1.0 | global_auc: 0.9770459081836328| flobal_FPR: 0.0 \n",
      "comm_round: 61 | global_acc: 96.974% | global_loss: 2.906545639038086 | global_f1: 0.9574861367837338 | global_precision: 0.9184397163120568 | global_recall: 1.0 | global_auc: 0.9780439121756487| flobal_FPR: 0.0 \n",
      "comm_round: 62 | global_acc: 97.105% | global_loss: 2.8947367668151855 | global_f1: 0.9592592592592593 | global_precision: 0.9217081850533808 | global_recall: 1.0 | global_auc: 0.9780439121756487| flobal_FPR: 0.0 \n",
      "comm_round: 63 | global_acc: 97.105% | global_loss: 2.768648862838745 | global_f1: 0.9592592592592593 | global_precision: 0.9217081850533808 | global_recall: 1.0 | global_auc: 0.9790419161676647| flobal_FPR: 0.0 \n",
      "comm_round: 64 | global_acc: 97.237% | global_loss: 2.651643753051758 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 65 | global_acc: 97.237% | global_loss: 2.6460535526275635 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9800399201596806| flobal_FPR: 0.0 \n",
      "comm_round: 66 | global_acc: 97.237% | global_loss: 2.6398377418518066 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9800399201596806| flobal_FPR: 0.0 \n",
      "comm_round: 67 | global_acc: 97.237% | global_loss: 2.633410930633545 | global_f1: 0.961038961038961 | global_precision: 0.925 | global_recall: 1.0 | global_auc: 0.9800399201596806| flobal_FPR: 0.0 \n",
      "comm_round: 68 | global_acc: 97.368% | global_loss: 2.6315929889678955 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 69 | global_acc: 97.368% | global_loss: 2.6315789222717285 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 70 | global_acc: 97.368% | global_loss: 2.6315789222717285 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 71 | global_acc: 97.368% | global_loss: 2.6315789222717285 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 72 | global_acc: 97.368% | global_loss: 2.6315789222717285 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9800399201596807| flobal_FPR: 0.0 \n",
      "comm_round: 73 | global_acc: 97.368% | global_loss: 2.503037214279175 | global_f1: 0.9628252788104089 | global_precision: 0.9283154121863799 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 74 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 75 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 76 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 77 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 78 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 79 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 80 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 81 | global_acc: 97.500% | global_loss: 2.5 | global_f1: 0.9646182495344506 | global_precision: 0.9316546762589928 | global_recall: 1.0 | global_auc: 0.9810379241516967| flobal_FPR: 0.0 \n",
      "comm_round: 82 | global_acc: 97.632% | global_loss: 2.3684685230255127 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437125| flobal_FPR: 0.0 \n",
      "comm_round: 83 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437125| flobal_FPR: 0.0 \n",
      "comm_round: 84 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 85 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 86 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 87 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 88 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 89 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 90 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 91 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 92 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 93 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 94 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 95 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 96 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 97 | global_acc: 97.632% | global_loss: 2.3684210777282715 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9820359281437127| flobal_FPR: 0.0 \n",
      "comm_round: 98 | global_acc: 97.632% | global_loss: 2.368421792984009 | global_f1: 0.9664179104477612 | global_precision: 0.9350180505415162 | global_recall: 1.0 | global_auc: 0.9819665687929161| flobal_FPR: 0.0 \n",
      "comm_round: 99 | global_acc: 97.500% | global_loss: 2.395444869995117 | global_f1: 0.9644859813084113 | global_precision: 0.9347826086956522 | global_recall: 0.9961389961389961 | global_auc: 0.9819665687929161| flobal_FPR: 0.003861003861003861 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0, 2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedADMM 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM non iid pgd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    selected_training_approach = random.choice([train_model, pgd_attack])\n",
    "\n",
    "                    # List of training approaches    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.1  # Set your desired value for epsilon\n",
    "                        alpha = 0.01   # Set your desired value for alpha\n",
    "                        num_iter = 10   # Set your desired number of iterations\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-non-iid-pgd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-non-iid-pgd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-non-iid-pgd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
