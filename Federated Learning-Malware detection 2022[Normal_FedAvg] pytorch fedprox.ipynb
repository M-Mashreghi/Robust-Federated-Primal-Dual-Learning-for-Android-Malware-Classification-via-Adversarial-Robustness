{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f8c85",
   "metadata": {},
   "source": [
    "## fedprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx 2018|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6741104125976562 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7021855743922932| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6610273122787476 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8119566987703175| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6473826169967651 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8709600373410902| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6319810748100281 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9015294937157852| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6140716075897217 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9182175992205095| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5929225087165833 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9292311790506227| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5677288174629211 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9389759975232563| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5379939675331116 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9482239002920716| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.398% | global_loss: 0.5036129355430603 | global_f1: 0.01432408236347359 | global_precision: 1.0 | global_recall: 0.007213706041478809 | global_auc: 0.9578098861771014| flobal_FPR: 0.9927862939585211 \n",
      "comm_round: 9 | global_acc: 74.601% | global_loss: 0.46598851680755615 | global_f1: 0.4752747252747253 | global_precision: 0.9971181556195965 | global_recall: 0.31199278629395855 | global_auc: 0.9666166664529906| flobal_FPR: 0.6880072137060415 \n",
      "comm_round: 10 | global_acc: 85.638% | global_loss: 0.42712125182151794 | global_f1: 0.7591973244147158 | global_precision: 0.9941605839416059 | global_recall: 0.6140667267808837 | global_auc: 0.9741717794615457| flobal_FPR: 0.3859332732191163 \n",
      "comm_round: 11 | global_acc: 90.160% | global_loss: 0.3879789113998413 | global_f1: 0.8478931140801643 | global_precision: 0.985663082437276 | global_recall: 0.7439134355275022 | global_auc: 0.9796490108457254| flobal_FPR: 0.2560865644724977 \n",
      "comm_round: 12 | global_acc: 92.354% | global_loss: 0.3487482964992523 | global_f1: 0.8868110236220473 | global_precision: 0.9761646803900325 | global_recall: 0.8124436429215509 | global_auc: 0.9827610849239147| flobal_FPR: 0.18755635707844906 \n",
      "comm_round: 13 | global_acc: 93.949% | global_loss: 0.31018659472465515 | global_f1: 0.9125840537944285 | global_precision: 0.9763617677286742 | global_recall: 0.8566275924256087 | global_auc: 0.9847378265149282| flobal_FPR: 0.14337240757439135 \n",
      "comm_round: 14 | global_acc: 94.814% | global_loss: 0.2730020582675934 | global_f1: 0.9260663507109005 | global_precision: 0.9760239760239761 | global_recall: 0.8809738503155996 | global_auc: 0.9859994653348472| flobal_FPR: 0.11902614968440037 \n",
      "comm_round: 15 | global_acc: 94.681% | global_loss: 0.2391003519296646 | global_f1: 0.9247412982126059 | global_precision: 0.9665683382497542 | global_recall: 0.8863841298467088 | global_auc: 0.9868532201704565| flobal_FPR: 0.11361587015329125 \n",
      "comm_round: 16 | global_acc: 94.814% | global_loss: 0.2102017104625702 | global_f1: 0.927170868347339 | global_precision: 0.9612778315585673 | global_recall: 0.8954012623985572 | global_auc: 0.9875303360745606| flobal_FPR: 0.10459873760144274 \n",
      "comm_round: 17 | global_acc: 95.213% | global_loss: 0.18668611347675323 | global_f1: 0.9333333333333332 | global_precision: 0.9590865842055185 | global_recall: 0.90892696122633 | global_auc: 0.9881281543938223| flobal_FPR: 0.09107303877366997 \n",
      "comm_round: 18 | global_acc: 95.512% | global_loss: 0.16806811094284058 | global_f1: 0.937759336099585 | global_precision: 0.9594339622641509 | global_recall: 0.9170423805229937 | global_auc: 0.9886908348611176| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 19 | global_acc: 95.545% | global_loss: 0.15354914963245392 | global_f1: 0.938305709023941 | global_precision: 0.9586077140169332 | global_recall: 0.9188458070333634 | global_auc: 0.9892501914775514| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 20 | global_acc: 95.977% | global_loss: 0.142134889960289 | global_f1: 0.9445716903344021 | global_precision: 0.9599627560521415 | global_recall: 0.9296663660955816 | global_auc: 0.9898546574985363| flobal_FPR: 0.0703336339044184 \n",
      "comm_round: 21 | global_acc: 96.044% | global_loss: 0.13303954899311066 | global_f1: 0.9455377574370709 | global_precision: 0.9600371747211895 | global_recall: 0.9314697926059513 | global_auc: 0.9903456377543873| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 22 | global_acc: 96.144% | global_loss: 0.1256507784128189 | global_f1: 0.9468864468864469 | global_precision: 0.9618604651162791 | global_recall: 0.9323715058611362 | global_auc: 0.9909553269695834| flobal_FPR: 0.06762849413886383 \n",
      "comm_round: 23 | global_acc: 96.343% | global_loss: 0.11954419314861298 | global_f1: 0.9496797804208601 | global_precision: 0.9637883008356546 | global_recall: 0.9359783588818755 | global_auc: 0.9915778367523886| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 24 | global_acc: 96.410% | global_loss: 0.11434759944677353 | global_f1: 0.950594693504117 | global_precision: 0.9647168059424327 | global_recall: 0.9368800721370604 | global_auc: 0.992050298410582| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 25 | global_acc: 96.476% | global_loss: 0.10987905412912369 | global_f1: 0.9515096065873742 | global_precision: 0.9656453110492108 | global_recall: 0.9377817853922452 | global_auc: 0.9923883815267968| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 26 | global_acc: 96.476% | global_loss: 0.10604508221149445 | global_f1: 0.9515539305301646 | global_precision: 0.9647822057460612 | global_recall: 0.9386834986474302 | global_auc: 0.9926685346708509| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 27 | global_acc: 96.576% | global_loss: 0.10273057967424393 | global_f1: 0.9529035208047554 | global_precision: 0.9666048237476809 | global_recall: 0.939585211902615 | global_auc: 0.9929629328900266| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 28 | global_acc: 96.609% | global_loss: 0.09984564036130905 | global_f1: 0.953382084095064 | global_precision: 0.9666357738646896 | global_recall: 0.9404869251577999 | global_auc: 0.993184681226083| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 29 | global_acc: 96.576% | global_loss: 0.09731233865022659 | global_f1: 0.9529465509365007 | global_precision: 0.9657407407407408 | global_recall: 0.9404869251577999 | global_auc: 0.9933902851436687| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 30 | global_acc: 96.676% | global_loss: 0.09510238468647003 | global_f1: 0.9544211485870556 | global_precision: 0.9649769585253456 | global_recall: 0.9440937781785392 | global_auc: 0.99355315383589| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 31 | global_acc: 96.842% | global_loss: 0.09315462410449982 | global_f1: 0.9567985447930878 | global_precision: 0.9651376146788991 | global_recall: 0.9486023444544635 | global_auc: 0.9937084251547135| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 32 | global_acc: 96.842% | global_loss: 0.09140188992023468 | global_f1: 0.9567985447930878 | global_precision: 0.9651376146788991 | global_recall: 0.9486023444544635 | global_auc: 0.9938204864123351| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 33 | global_acc: 96.975% | global_loss: 0.08983688056468964 | global_f1: 0.9586175534333787 | global_precision: 0.9669724770642202 | global_recall: 0.9504057709648331 | global_auc: 0.9939254251323962| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 34 | global_acc: 97.074% | global_loss: 0.08842137455940247 | global_f1: 0.9599271402550091 | global_precision: 0.9696412143514259 | global_recall: 0.9504057709648331 | global_auc: 0.993997600179678| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 35 | global_acc: 97.074% | global_loss: 0.08722646534442902 | global_f1: 0.9600000000000001 | global_precision: 0.9679193400549955 | global_recall: 0.9522091974752029 | global_auc: 0.9941557205135254| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 36 | global_acc: 97.074% | global_loss: 0.08597345650196075 | global_f1: 0.9599636032757052 | global_precision: 0.9687786960514233 | global_recall: 0.951307484220018 | global_auc: 0.9942312194116689| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 37 | global_acc: 97.141% | global_loss: 0.08487241715192795 | global_f1: 0.9609090909090909 | global_precision: 0.9688359303391384 | global_recall: 0.9531109107303878 | global_auc: 0.9943475541918271| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 38 | global_acc: 97.174% | global_loss: 0.08387786149978638 | global_f1: 0.9613811903680145 | global_precision: 0.9688644688644689 | global_recall: 0.9540126239855726 | global_auc: 0.9944410968517909| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 39 | global_acc: 97.241% | global_loss: 0.08292098343372345 | global_f1: 0.9622555707139608 | global_precision: 0.9706422018348624 | global_recall: 0.9540126239855726 | global_auc: 0.9945298911533809| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 40 | global_acc: 97.274% | global_loss: 0.08202885836362839 | global_f1: 0.9626933575978162 | global_precision: 0.9715335169880625 | global_recall: 0.9540126239855726 | global_auc: 0.9946072893948739| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 41 | global_acc: 97.274% | global_loss: 0.08119979500770569 | global_f1: 0.9626933575978162 | global_precision: 0.9715335169880625 | global_recall: 0.9540126239855726 | global_auc: 0.9946761405912941| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 42 | global_acc: 97.274% | global_loss: 0.08048037439584732 | global_f1: 0.9627272727272728 | global_precision: 0.9706691109074244 | global_recall: 0.9549143372407575 | global_auc: 0.9947540136686244| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 43 | global_acc: 97.307% | global_loss: 0.07974250614643097 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.9948233397008819| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 44 | global_acc: 97.340% | global_loss: 0.07902361452579498 | global_f1: 0.9636032757051866 | global_precision: 0.9724517906336089 | global_recall: 0.9549143372407575 | global_auc: 0.9948712981204573| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 45 | global_acc: 97.340% | global_loss: 0.07840899378061295 | global_f1: 0.9636363636363636 | global_precision: 0.9715857011915674 | global_recall: 0.9558160504959423 | global_auc: 0.9949434731677391| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 46 | global_acc: 97.374% | global_loss: 0.07777681201696396 | global_f1: 0.9641072239890959 | global_precision: 0.9716117216117216 | global_recall: 0.9567177637511272 | global_auc: 0.9949743374971688| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 47 | global_acc: 97.407% | global_loss: 0.0772588849067688 | global_f1: 0.9645776566757494 | global_precision: 0.9716376944190301 | global_recall: 0.957619477006312 | global_auc: 0.9950598079478972| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 48 | global_acc: 97.407% | global_loss: 0.07664873450994492 | global_f1: 0.9645454545454545 | global_precision: 0.9725022914757103 | global_recall: 0.9567177637511272 | global_auc: 0.9950773768738802| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 49 | global_acc: 97.374% | global_loss: 0.076152004301548 | global_f1: 0.9641072239890959 | global_precision: 0.9716117216117216 | global_recall: 0.9567177637511272 | global_auc: 0.9951172630842202| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 50 | global_acc: 97.440% | global_loss: 0.0756867304444313 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9952084315649972| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 51 | global_acc: 97.473% | global_loss: 0.07521247118711472 | global_f1: 0.9654859218891917 | global_precision: 0.9725526075022873 | global_recall: 0.9585211902614968 | global_auc: 0.9952407204019389| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 52 | global_acc: 97.473% | global_loss: 0.07480338960886002 | global_f1: 0.9654859218891917 | global_precision: 0.9725526075022873 | global_recall: 0.9585211902614968 | global_auc: 0.9953014993891237| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 53 | global_acc: 97.473% | global_loss: 0.07442725449800491 | global_f1: 0.9654859218891917 | global_precision: 0.9725526075022873 | global_recall: 0.9585211902614968 | global_auc: 0.995341860435301| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 54 | global_acc: 97.507% | global_loss: 0.07397014647722244 | global_f1: 0.9659245797364834 | global_precision: 0.9734432234432234 | global_recall: 0.9585211902614968 | global_auc: 0.9953603790329588| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 55 | global_acc: 97.473% | global_loss: 0.07364301383495331 | global_f1: 0.9654859218891917 | global_precision: 0.9725526075022873 | global_recall: 0.9585211902614968 | global_auc: 0.9954088122883715| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 56 | global_acc: 97.606% | global_loss: 0.07322919368743896 | global_f1: 0.9672429481346679 | global_precision: 0.9761248852157943 | global_recall: 0.9585211902614968 | global_auc: 0.9954140354825828| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 57 | global_acc: 97.507% | global_loss: 0.07295858860015869 | global_f1: 0.9659245797364834 | global_precision: 0.9734432234432234 | global_recall: 0.9585211902614968 | global_auc: 0.9954634184096702| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 58 | global_acc: 97.540% | global_loss: 0.0726475790143013 | global_f1: 0.9663636363636363 | global_precision: 0.9743354720439963 | global_recall: 0.9585211902614968 | global_auc: 0.9954676919322067| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 59 | global_acc: 97.573% | global_loss: 0.07230397313833237 | global_f1: 0.9668030923146885 | global_precision: 0.9752293577981651 | global_recall: 0.9585211902614968 | global_auc: 0.9954819370073282| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 60 | global_acc: 97.640% | global_loss: 0.0720011293888092 | global_f1: 0.9676832043695949 | global_precision: 0.9770220588235294 | global_recall: 0.9585211902614968 | global_auc: 0.9954976065899617| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 61 | global_acc: 97.573% | global_loss: 0.07174500077962875 | global_f1: 0.9668030923146885 | global_precision: 0.9752293577981651 | global_recall: 0.9585211902614968 | global_auc: 0.9955484140245614| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 62 | global_acc: 97.640% | global_loss: 0.07146243005990982 | global_f1: 0.9676832043695949 | global_precision: 0.9770220588235294 | global_recall: 0.9585211902614968 | global_auc: 0.9955536372187724| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 63 | global_acc: 97.640% | global_loss: 0.07119094580411911 | global_f1: 0.967712596634834 | global_precision: 0.9761467889908257 | global_recall: 0.9594229035166817 | global_auc: 0.9955688319655688| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 64 | global_acc: 97.640% | global_loss: 0.07098980247974396 | global_f1: 0.967712596634834 | global_precision: 0.9761467889908257 | global_recall: 0.9594229035166817 | global_auc: 0.9956191645643309| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 65 | global_acc: 97.640% | global_loss: 0.0708150789141655 | global_f1: 0.967741935483871 | global_precision: 0.9752747252747253 | global_recall: 0.9603246167718665 | global_auc: 0.9956405321770131| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 66 | global_acc: 97.640% | global_loss: 0.07062087953090668 | global_f1: 0.967741935483871 | global_precision: 0.9752747252747253 | global_recall: 0.9603246167718665 | global_auc: 0.9956656984763943| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 67 | global_acc: 97.640% | global_loss: 0.0704052522778511 | global_f1: 0.967741935483871 | global_precision: 0.9752747252747253 | global_recall: 0.9603246167718665 | global_auc: 0.9956770945364912| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 68 | global_acc: 97.640% | global_loss: 0.07011369615793228 | global_f1: 0.967712596634834 | global_precision: 0.9761467889908257 | global_recall: 0.9594229035166817 | global_auc: 0.9956894402682632| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 69 | global_acc: 97.739% | global_loss: 0.06987347453832626 | global_f1: 0.9690346083788706 | global_precision: 0.9788408463661453 | global_recall: 0.9594229035166817 | global_auc: 0.9957013111641979| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 70 | global_acc: 97.739% | global_loss: 0.06970992684364319 | global_f1: 0.9690627843494085 | global_precision: 0.977961432506887 | global_recall: 0.9603246167718665 | global_auc: 0.9957312258219526| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 71 | global_acc: 97.806% | global_loss: 0.06949923187494278 | global_f1: 0.9699453551912568 | global_precision: 0.9797608095676172 | global_recall: 0.9603246167718665 | global_auc: 0.9957521185987975| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 72 | global_acc: 97.739% | global_loss: 0.06936796009540558 | global_f1: 0.9690627843494085 | global_precision: 0.977961432506887 | global_recall: 0.9603246167718665 | global_auc: 0.9957782345698534| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 73 | global_acc: 97.806% | global_loss: 0.06914824992418289 | global_f1: 0.9699453551912568 | global_precision: 0.9797608095676172 | global_recall: 0.9603246167718665 | global_auc: 0.9957848822715766| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 74 | global_acc: 97.773% | global_loss: 0.06899646669626236 | global_f1: 0.9695038689121529 | global_precision: 0.9788602941176471 | global_recall: 0.9603246167718665 | global_auc: 0.9958195452877054| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 75 | global_acc: 97.872% | global_loss: 0.06890176981687546 | global_f1: 0.9709355131698456 | global_precision: 0.9780420860018298 | global_recall: 0.9639314697926059 | global_auc: 0.9958646546922565| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 76 | global_acc: 97.872% | global_loss: 0.06876704096794128 | global_f1: 0.9709355131698456 | global_precision: 0.9780420860018298 | global_recall: 0.9639314697926059 | global_auc: 0.995885072633264| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 77 | global_acc: 97.839% | global_loss: 0.06852710992097855 | global_f1: 0.9704411095952706 | global_precision: 0.9788990825688073 | global_recall: 0.9621280432822362 | global_auc: 0.9958751010806789| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.872% | global_loss: 0.06841278821229935 | global_f1: 0.9709090909090908 | global_precision: 0.9789184234647113 | global_recall: 0.9630297565374211 | global_auc: 0.9959016918875723| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 79 | global_acc: 98.039% | global_loss: 0.06853244453668594 | global_f1: 0.9732910819375283 | global_precision: 0.9772727272727273 | global_recall: 0.9693417493237151 | global_auc: 0.9959126131118319| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 80 | global_acc: 97.906% | global_loss: 0.06818264722824097 | global_f1: 0.9714026327734907 | global_precision: 0.9780621572212066 | global_recall: 0.9648331830477908 | global_auc: 0.9959054905742711| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 81 | global_acc: 97.939% | global_loss: 0.06804649531841278 | global_f1: 0.9718437783832881 | global_precision: 0.9789569990850869 | global_recall: 0.9648331830477908 | global_auc: 0.995922584664417| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 82 | global_acc: 97.906% | global_loss: 0.0679701566696167 | global_f1: 0.971376646978646 | global_precision: 0.9789377289377289 | global_recall: 0.9639314697926059 | global_auc: 0.9959382542470505| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 83 | global_acc: 97.906% | global_loss: 0.06784563511610031 | global_f1: 0.971376646978646 | global_precision: 0.9789377289377289 | global_recall: 0.9639314697926059 | global_auc: 0.9959496503071475| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 84 | global_acc: 98.039% | global_loss: 0.06780648976564407 | global_f1: 0.9732426303854875 | global_precision: 0.9790145985401459 | global_recall: 0.9675383228133454 | global_auc: 0.9959809894724148| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 85 | global_acc: 98.039% | global_loss: 0.067763552069664 | global_f1: 0.9732668781150884 | global_precision: 0.9781420765027322 | global_recall: 0.9684400360685302 | global_auc: 0.9960232498619415| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 86 | global_acc: 98.005% | global_loss: 0.06767915934324265 | global_f1: 0.9728014505893019 | global_precision: 0.9781221513217867 | global_recall: 0.9675383228133454 | global_auc: 0.9960403439520872| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 87 | global_acc: 98.039% | global_loss: 0.06750020384788513 | global_f1: 0.9732426303854875 | global_precision: 0.9790145985401459 | global_recall: 0.9675383228133454 | global_auc: 0.9960412936237619| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 88 | global_acc: 98.072% | global_loss: 0.06754542142152786 | global_f1: 0.9737318840579711 | global_precision: 0.978161965423112 | global_recall: 0.9693417493237151 | global_auc: 0.99606883410233| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 89 | global_acc: 98.039% | global_loss: 0.06731455028057098 | global_f1: 0.9732183386291421 | global_precision: 0.979890310786106 | global_recall: 0.9666366095581606 | global_auc: 0.996106346133483| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 90 | global_acc: 98.105% | global_loss: 0.06725470721721649 | global_f1: 0.9741496598639456 | global_precision: 0.9799270072992701 | global_recall: 0.9684400360685302 | global_auc: 0.9961305627611894| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 91 | global_acc: 98.138% | global_loss: 0.06721869856119156 | global_f1: 0.9746146872166819 | global_precision: 0.9799453053783045 | global_recall: 0.9693417493237151 | global_auc: 0.9961528800455461| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 92 | global_acc: 98.105% | global_loss: 0.06710520386695862 | global_f1: 0.9741496598639456 | global_precision: 0.9799270072992701 | global_recall: 0.9684400360685302 | global_auc: 0.9961552542247332| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 93 | global_acc: 97.972% | global_loss: 0.06702305376529694 | global_f1: 0.9722853248523398 | global_precision: 0.9798534798534798 | global_recall: 0.9648331830477908 | global_auc: 0.9961609522547817| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 94 | global_acc: 98.105% | global_loss: 0.06695766746997833 | global_f1: 0.9741496598639456 | global_precision: 0.9799270072992701 | global_recall: 0.9684400360685302 | global_auc: 0.9961789960166022| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 95 | global_acc: 98.105% | global_loss: 0.06693214178085327 | global_f1: 0.9741496598639456 | global_precision: 0.9799270072992701 | global_recall: 0.9684400360685302 | global_auc: 0.9961899172408619| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 96 | global_acc: 98.138% | global_loss: 0.06692571938037872 | global_f1: 0.9746146872166819 | global_precision: 0.9799453053783045 | global_recall: 0.9693417493237151 | global_auc: 0.9962022629726337| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 97 | global_acc: 98.138% | global_loss: 0.0668615847826004 | global_f1: 0.9746146872166819 | global_precision: 0.9799453053783045 | global_recall: 0.9693417493237151 | global_auc: 0.9962098603460319| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 98 | global_acc: 98.072% | global_loss: 0.06679840385913849 | global_f1: 0.9737080689029918 | global_precision: 0.9790337283500455 | global_recall: 0.9684400360685302 | global_auc: 0.9962160332119179| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 99 | global_acc: 98.105% | global_loss: 0.0667644813656807 | global_f1: 0.9741730856366108 | global_precision: 0.9790528233151184 | global_recall: 0.9693417493237151 | global_auc: 0.9962293286153645| flobal_FPR: 0.030658250676284943 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx 2018|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                # ...\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb904cc7",
   "metadata": {},
   "source": [
    "## fedprox non idd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b47632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Client client_1: {(0,): 6, (1,): 1197}\n",
      "Client client_2: {(0,): 978, (1,): 225}\n",
      "Client client_3: {(0,): 810, (1,): 393}\n",
      "Client client_4: {(0,): 367, (1,): 836}\n",
      "Client client_5: {(0,): 601, (1,): 602}\n",
      "Client client_6: {(0,): 757, (1,): 446}\n",
      "Client client_7: {(0,): 465, (1,): 738}\n",
      "Client client_8: {(0,): 813, (1,): 390}\n",
      "Client client_9: {(0,): 597, (1,): 606}\n",
      "Client client_10: {(0,): 842, (1,): 361}\n",
      "|=======================|\n",
      "|Traditional FedProx 2018|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 40.293% | global_loss: 0.6978632211685181 | global_f1: 0.5430025445292621 | global_precision: 0.3782346685572492 | global_recall: 0.9621280432822362 | global_auc: 0.6203537906857153| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 1 | global_acc: 52.128% | global_loss: 0.6901611685752869 | global_f1: 0.5871559633027522 | global_precision: 0.43043295502311896 | global_recall: 0.9233543733092876 | global_auc: 0.7452790633958075| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 2 | global_acc: 69.382% | global_loss: 0.6820592880249023 | global_f1: 0.6792058516196448 | global_precision: 0.5533484676503972 | global_recall: 0.8791704238052299 | global_auc: 0.8100993783924053| flobal_FPR: 0.12082957619477007 \n",
      "comm_round: 3 | global_acc: 76.064% | global_loss: 0.6730197072029114 | global_f1: 0.7204968944099379 | global_precision: 0.632583503749148 | global_recall: 0.8367899008115419 | global_auc: 0.8505430460054197| flobal_FPR: 0.16321009918845808 \n",
      "comm_round: 4 | global_acc: 79.056% | global_loss: 0.6621758341789246 | global_f1: 0.7381546134663343 | global_precision: 0.6846569005397071 | global_recall: 0.8007213706041478 | global_auc: 0.8768900246962119| flobal_FPR: 0.1992786293958521 \n",
      "comm_round: 5 | global_acc: 79.820% | global_loss: 0.648622453212738 | global_f1: 0.7404873877725524 | global_precision: 0.7040650406504065 | global_recall: 0.7808836789900812 | global_auc: 0.89543521316093| flobal_FPR: 0.21911632100991885 \n",
      "comm_round: 6 | global_acc: 81.017% | global_loss: 0.631080687046051 | global_f1: 0.7556696619597776 | global_precision: 0.7190553745928339 | global_recall: 0.7962128043282236 | global_auc: 0.9117073624721094| flobal_FPR: 0.2037871956717764 \n",
      "comm_round: 7 | global_acc: 82.181% | global_loss: 0.6079218983650208 | global_f1: 0.7707442258340462 | global_precision: 0.7331163547599675 | global_recall: 0.8124436429215509 | global_auc: 0.9246027167257601| flobal_FPR: 0.18755635707844906 \n",
      "comm_round: 8 | global_acc: 84.076% | global_loss: 0.5781079530715942 | global_f1: 0.7989928661351237 | global_precision: 0.7472527472527473 | global_recall: 0.8584310189359784 | global_auc: 0.9372219539399742| flobal_FPR: 0.14156898106402163 \n",
      "comm_round: 9 | global_acc: 84.840% | global_loss: 0.5411466956138611 | global_f1: 0.8117258464079273 | global_precision: 0.7486671744097486 | global_recall: 0.8863841298467088 | global_auc: 0.9495221014714689| flobal_FPR: 0.11361587015329125 \n",
      "comm_round: 10 | global_acc: 85.638% | global_loss: 0.49791473150253296 | global_f1: 0.8229508196721311 | global_precision: 0.754320060105184 | global_recall: 0.9053201082055906 | global_auc: 0.961035920856262| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 11 | global_acc: 86.835% | global_loss: 0.4509587287902832 | global_f1: 0.8391551584077986 | global_precision: 0.7634885439763488 | global_recall: 0.9314697926059513 | global_auc: 0.9706860095793381| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 12 | global_acc: 87.999% | global_loss: 0.4020342230796814 | global_f1: 0.8529531568228105 | global_precision: 0.7778603268945022 | global_recall: 0.9440937781785392 | global_auc: 0.977060205860329| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 13 | global_acc: 92.287% | global_loss: 0.35332930088043213 | global_f1: 0.9003436426116839 | global_precision: 0.859721082854799 | global_recall: 0.9449954914337241 | global_auc: 0.9811034330156206| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 14 | global_acc: 93.318% | global_loss: 0.30763038992881775 | global_f1: 0.9123419101613607 | global_precision: 0.8834459459459459 | global_recall: 0.9431920649233544 | global_auc: 0.9841585267933244| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 15 | global_acc: 93.850% | global_loss: 0.2660227119922638 | global_f1: 0.9185380889476001 | global_precision: 0.8975903614457831 | global_recall: 0.9404869251577999 | global_auc: 0.9862929138823481| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 16 | global_acc: 94.781% | global_loss: 0.23177163302898407 | global_f1: 0.9301911960871498 | global_precision: 0.9175438596491228 | global_recall: 0.9431920649233544 | global_auc: 0.9873408765754459| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 17 | global_acc: 94.980% | global_loss: 0.20357894897460938 | global_f1: 0.9327394209354121 | global_precision: 0.9216549295774648 | global_recall: 0.9440937781785392 | global_auc: 0.9881979552619171| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 18 | global_acc: 94.880% | global_loss: 0.1819087266921997 | global_f1: 0.9313113291703836 | global_precision: 0.9214474845542807 | global_recall: 0.9413886384129847 | global_auc: 0.9888836182110939| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 19 | global_acc: 95.047% | global_loss: 0.16524545848369598 | global_f1: 0.933392936969155 | global_precision: 0.925531914893617 | global_recall: 0.9413886384129847 | global_auc: 0.9897050842097617| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 20 | global_acc: 95.146% | global_loss: 0.15266165137290955 | global_f1: 0.9348795718108831 | global_precision: 0.9249779346866726 | global_recall: 0.9449954914337241 | global_auc: 0.9902986290064867| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 21 | global_acc: 95.445% | global_loss: 0.14256754517555237 | global_f1: 0.9386475593372144 | global_precision: 0.9323843416370107 | global_recall: 0.9449954914337241 | global_auc: 0.9907639681271193| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 22 | global_acc: 95.545% | global_loss: 0.13424000144004822 | global_f1: 0.9399103139013454 | global_precision: 0.9348795718108831 | global_recall: 0.9449954914337241 | global_auc: 0.9910783094514648| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 23 | global_acc: 95.944% | global_loss: 0.12750087678432465 | global_f1: 0.945193171608266 | global_precision: 0.9418084153983886 | global_recall: 0.9486023444544635 | global_auc: 0.9913503903862837| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 24 | global_acc: 96.210% | global_loss: 0.12201910465955734 | global_f1: 0.9487410071942445 | global_precision: 0.9461883408071748 | global_recall: 0.951307484220018 | global_auc: 0.9916362415603865| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 25 | global_acc: 96.210% | global_loss: 0.11761845648288727 | global_f1: 0.9487410071942445 | global_precision: 0.9461883408071748 | global_recall: 0.951307484220018 | global_auc: 0.9918024341034695| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 26 | global_acc: 96.243% | global_loss: 0.11391613632440567 | global_f1: 0.9491677912730543 | global_precision: 0.9470377019748654 | global_recall: 0.951307484220018 | global_auc: 0.9920094625285674| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 27 | global_acc: 96.343% | global_loss: 0.11069890111684799 | global_f1: 0.9504950495049506 | global_precision: 0.9487870619946092 | global_recall: 0.9522091974752029 | global_auc: 0.9921732808924635| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 28 | global_acc: 96.410% | global_loss: 0.10811721533536911 | global_f1: 0.9513513513513514 | global_precision: 0.9504950495049505 | global_recall: 0.9522091974752029 | global_auc: 0.9923532436748305| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 29 | global_acc: 96.376% | global_loss: 0.10619794577360153 | global_f1: 0.9509230076542098 | global_precision: 0.9496402877697842 | global_recall: 0.9522091974752029 | global_auc: 0.9924956944260446| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 30 | global_acc: 96.410% | global_loss: 0.10415706038475037 | global_f1: 0.9513951395139515 | global_precision: 0.949685534591195 | global_recall: 0.9531109107303878 | global_auc: 0.9926044318328046| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 31 | global_acc: 96.509% | global_loss: 0.10203823447227478 | global_f1: 0.952638700947226 | global_precision: 0.9530685920577617 | global_recall: 0.9522091974752029 | global_auc: 0.9927364361955964| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 32 | global_acc: 96.509% | global_loss: 0.10046226531267166 | global_f1: 0.9525530953456846 | global_precision: 0.9547101449275363 | global_recall: 0.9504057709648331 | global_auc: 0.9928418497514947| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 33 | global_acc: 96.576% | global_loss: 0.09929908812046051 | global_f1: 0.9534988713318284 | global_precision: 0.9547920433996383 | global_recall: 0.9522091974752029 | global_auc: 0.9929681560842378| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 34 | global_acc: 96.609% | global_loss: 0.0981154516339302 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9930560007141531| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 35 | global_acc: 96.609% | global_loss: 0.09717991948127747 | global_f1: 0.953929539295393 | global_precision: 0.9556561085972851 | global_recall: 0.9522091974752029 | global_auc: 0.9931552414041656| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 36 | global_acc: 96.576% | global_loss: 0.09636089950799942 | global_f1: 0.9535408209291836 | global_precision: 0.953971119133574 | global_recall: 0.9531109107303878 | global_auc: 0.9932350138248454| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 37 | global_acc: 96.609% | global_loss: 0.09551827609539032 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9932829722444207| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 38 | global_acc: 96.609% | global_loss: 0.09495963156223297 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9933575214708895| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 39 | global_acc: 96.609% | global_loss: 0.09446966648101807 | global_f1: 0.9539711191335741 | global_precision: 0.95483288166215 | global_recall: 0.9531109107303878 | global_auc: 0.9934838278036325| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 40 | global_acc: 96.576% | global_loss: 0.09375156462192535 | global_f1: 0.9535408209291836 | global_precision: 0.953971119133574 | global_recall: 0.9531109107303878 | global_auc: 0.9935640750601499| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 41 | global_acc: 96.642% | global_loss: 0.09294922649860382 | global_f1: 0.9544018058690744 | global_precision: 0.9556962025316456 | global_recall: 0.9531109107303878 | global_auc: 0.9936163070022616| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 42 | global_acc: 96.642% | global_loss: 0.09240669012069702 | global_f1: 0.9543605964753729 | global_precision: 0.9565217391304348 | global_recall: 0.9522091974752029 | global_auc: 0.9936846833628444| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 43 | global_acc: 96.642% | global_loss: 0.09207788109779358 | global_f1: 0.9543605964753729 | global_precision: 0.9565217391304348 | global_recall: 0.9522091974752029 | global_auc: 0.9937364404691189| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 44 | global_acc: 96.676% | global_loss: 0.09188410639762878 | global_f1: 0.95483288166215 | global_precision: 0.9565610859728507 | global_recall: 0.9531109107303878 | global_auc: 0.9937962697846286| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 45 | global_acc: 96.676% | global_loss: 0.09144190698862076 | global_f1: 0.95483288166215 | global_precision: 0.9565610859728507 | global_recall: 0.9531109107303878 | global_auc: 0.9938518255776021| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 46 | global_acc: 96.676% | global_loss: 0.09085413068532944 | global_f1: 0.9547920433996383 | global_precision: 0.957388939256573 | global_recall: 0.9522091974752029 | global_auc: 0.9938869634295684| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 47 | global_acc: 96.709% | global_loss: 0.0909595862030983 | global_f1: 0.9552643470402169 | global_precision: 0.957427536231884 | global_recall: 0.9531109107303878 | global_auc: 0.9939406198791922| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 48 | global_acc: 96.709% | global_loss: 0.09091772139072418 | global_f1: 0.9552643470402169 | global_precision: 0.957427536231884 | global_recall: 0.9531109107303878 | global_auc: 0.9939828802687192| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 49 | global_acc: 96.676% | global_loss: 0.09003544598817825 | global_f1: 0.9547920433996383 | global_precision: 0.957388939256573 | global_recall: 0.9522091974752029 | global_auc: 0.9939957008363284| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 50 | global_acc: 96.709% | global_loss: 0.08955671638250351 | global_f1: 0.955223880597015 | global_precision: 0.9582577132486388 | global_recall: 0.9522091974752029 | global_auc: 0.9940232413148963| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 51 | global_acc: 96.676% | global_loss: 0.09016560763120651 | global_f1: 0.95483288166215 | global_precision: 0.9565610859728507 | global_recall: 0.9531109107303878 | global_auc: 0.9940783222720325| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 52 | global_acc: 96.742% | global_loss: 0.0899265855550766 | global_f1: 0.9557362240289069 | global_precision: 0.9574660633484163 | global_recall: 0.9540126239855726 | global_auc: 0.994140050930892| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 53 | global_acc: 96.742% | global_loss: 0.0897253081202507 | global_f1: 0.9557362240289069 | global_precision: 0.9574660633484163 | global_recall: 0.9540126239855726 | global_auc: 0.9941713900961591| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 54 | global_acc: 96.775% | global_loss: 0.0890984833240509 | global_f1: 0.9561680976050609 | global_precision: 0.9583333333333334 | global_recall: 0.9540126239855726 | global_auc: 0.9941580946927123| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 55 | global_acc: 96.775% | global_loss: 0.08916162699460983 | global_f1: 0.9561680976050609 | global_precision: 0.9583333333333334 | global_recall: 0.9540126239855726 | global_auc: 0.9941837358279308| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 56 | global_acc: 96.775% | global_loss: 0.08958202600479126 | global_f1: 0.9562471808750563 | global_precision: 0.9566787003610109 | global_recall: 0.9558160504959423 | global_auc: 0.9942141253215233| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 57 | global_acc: 96.842% | global_loss: 0.08909104019403458 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942146001573606| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 58 | global_acc: 96.842% | global_loss: 0.0886472761631012 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942217226949213| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 59 | global_acc: 96.842% | global_loss: 0.08898452669382095 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942502128451641| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 60 | global_acc: 96.842% | global_loss: 0.08854187279939651 | global_f1: 0.9571106094808128 | global_precision: 0.9584086799276673 | global_recall: 0.9558160504959423 | global_auc: 0.9942649327561228| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 61 | global_acc: 96.908% | global_loss: 0.08881168067455292 | global_f1: 0.9580514208389717 | global_precision: 0.9584837545126353 | global_recall: 0.957619477006312 | global_auc: 0.9942777533237321| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 62 | global_acc: 96.908% | global_loss: 0.08857796341180801 | global_f1: 0.9580514208389717 | global_precision: 0.9584837545126353 | global_recall: 0.957619477006312 | global_auc: 0.9943119415040235| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 63 | global_acc: 96.908% | global_loss: 0.08878421783447266 | global_f1: 0.9580892293826048 | global_precision: 0.9576576576576576 | global_recall: 0.9585211902614968 | global_auc: 0.9943185892057468| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 64 | global_acc: 96.908% | global_loss: 0.08859418332576752 | global_f1: 0.9580892293826048 | global_precision: 0.9576576576576576 | global_recall: 0.9585211902614968 | global_auc: 0.9943380574750794| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 65 | global_acc: 96.875% | global_loss: 0.08877632766962051 | global_f1: 0.9576576576576576 | global_precision: 0.9567956795679567 | global_recall: 0.9585211902614968 | global_auc: 0.9943257117433076| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 66 | global_acc: 96.941% | global_loss: 0.08892667293548584 | global_f1: 0.9585585585585585 | global_precision: 0.9576957695769577 | global_recall: 0.9594229035166817 | global_auc: 0.9943485038635017| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 67 | global_acc: 97.008% | global_loss: 0.08875413984060287 | global_f1: 0.9594594594594594 | global_precision: 0.9585958595859586 | global_recall: 0.9603246167718665 | global_auc: 0.9943494535351766| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 68 | global_acc: 96.941% | global_loss: 0.08851204812526703 | global_f1: 0.9585585585585585 | global_precision: 0.9576957695769577 | global_recall: 0.9594229035166817 | global_auc: 0.994355151565225| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 69 | global_acc: 97.074% | global_loss: 0.08896404504776001 | global_f1: 0.960431654676259 | global_precision: 0.957847533632287 | global_recall: 0.9630297565374211 | global_auc: 0.9943893397455166| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 70 | global_acc: 97.041% | global_loss: 0.08914877474308014 | global_f1: 0.96 | global_precision: 0.956989247311828 | global_recall: 0.9630297565374211 | global_auc: 0.9944016854772885| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 71 | global_acc: 97.041% | global_loss: 0.08919905871152878 | global_f1: 0.96 | global_precision: 0.956989247311828 | global_recall: 0.9630297565374211 | global_auc: 0.9944107073581986| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 72 | global_acc: 97.041% | global_loss: 0.08935078978538513 | global_f1: 0.96 | global_precision: 0.956989247311828 | global_recall: 0.9630297565374211 | global_auc: 0.9944202040749461| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 73 | global_acc: 97.074% | global_loss: 0.08897389471530914 | global_f1: 0.960431654676259 | global_precision: 0.957847533632287 | global_recall: 0.9630297565374211 | global_auc: 0.9944187795674342| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 74 | global_acc: 97.041% | global_loss: 0.08949171006679535 | global_f1: 0.9600359227660531 | global_precision: 0.9561717352415027 | global_recall: 0.9639314697926059 | global_auc: 0.9944463200460021| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 75 | global_acc: 96.975% | global_loss: 0.08945586532354355 | global_f1: 0.9591011235955056 | global_precision: 0.9560931899641577 | global_recall: 0.9621280432822362 | global_auc: 0.9944515432402133| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 76 | global_acc: 96.975% | global_loss: 0.08943072706460953 | global_f1: 0.9591011235955056 | global_precision: 0.9560931899641577 | global_recall: 0.9621280432822362 | global_auc: 0.9944406220159535| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 77 | global_acc: 97.008% | global_loss: 0.08948964625597 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944520180760507| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.008% | global_loss: 0.08970987051725388 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944596154494487| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 79 | global_acc: 97.008% | global_loss: 0.08965525031089783 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944600902852861| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 80 | global_acc: 97.008% | global_loss: 0.08978056162595749 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9944491690610263| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 81 | global_acc: 96.809% | global_loss: 0.09050742536783218 | global_f1: 0.9569120287253142 | global_precision: 0.9526362823949955 | global_recall: 0.9612263300270514 | global_auc: 0.9944382478367666| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 82 | global_acc: 96.975% | global_loss: 0.09001866728067398 | global_f1: 0.9590643274853802 | global_precision: 0.9569120287253142 | global_recall: 0.9612263300270514 | global_auc: 0.9944562915985871| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 83 | global_acc: 96.975% | global_loss: 0.09027722477912903 | global_f1: 0.9590643274853802 | global_precision: 0.9569120287253142 | global_recall: 0.9612263300270514 | global_auc: 0.9944382478367666| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 84 | global_acc: 96.875% | global_loss: 0.0906435176730156 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944591406136113| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 85 | global_acc: 96.908% | global_loss: 0.09056797623634338 | global_f1: 0.9582772543741588 | global_precision: 0.9535714285714286 | global_recall: 0.9630297565374211 | global_auc: 0.9944520180760507| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 86 | global_acc: 96.875% | global_loss: 0.09119976311922073 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944743353604076| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 87 | global_acc: 96.875% | global_loss: 0.09136107563972473 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.994480983062131| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 88 | global_acc: 96.875% | global_loss: 0.0915374681353569 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944790837187814| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 89 | global_acc: 96.875% | global_loss: 0.09187585860490799 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944933287939027| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 90 | global_acc: 96.875% | global_loss: 0.0917397066950798 | global_f1: 0.9578475336322869 | global_precision: 0.9527207850133809 | global_recall: 0.9630297565374211 | global_auc: 0.9944833572413176| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 91 | global_acc: 96.908% | global_loss: 0.09139128774404526 | global_f1: 0.9582772543741588 | global_precision: 0.9535714285714286 | global_recall: 0.9630297565374211 | global_auc: 0.9944691121661964| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 92 | global_acc: 96.842% | global_loss: 0.09247473627328873 | global_f1: 0.9574563367666815 | global_precision: 0.951067615658363 | global_recall: 0.9639314697926059 | global_auc: 0.9945016384210569| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 93 | global_acc: 96.908% | global_loss: 0.09183191508054733 | global_f1: 0.9582772543741588 | global_precision: 0.9535714285714286 | global_recall: 0.9630297565374211 | global_auc: 0.9944790837187814| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 94 | global_acc: 96.908% | global_loss: 0.09235925227403641 | global_f1: 0.9583146571044374 | global_precision: 0.9527629233511586 | global_recall: 0.9639314697926059 | global_auc: 0.994486206256342| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 95 | global_acc: 96.908% | global_loss: 0.0924108549952507 | global_f1: 0.9583146571044374 | global_precision: 0.9527629233511586 | global_recall: 0.9639314697926059 | global_auc: 0.9944881055996915| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 96 | global_acc: 96.875% | global_loss: 0.09269677847623825 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9944895301072036| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 97 | global_acc: 96.875% | global_loss: 0.09285452216863632 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9944876307638543| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 98 | global_acc: 96.875% | global_loss: 0.09286542981863022 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9944942784655774| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 99 | global_acc: 96.908% | global_loss: 0.09295301884412766 | global_f1: 0.9583146571044374 | global_precision: 0.9527629233511586 | global_recall: 0.9639314697926059 | global_auc: 0.9944909546147158| flobal_FPR: 0.03606853020739405 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx 2018|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                # ...\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-nonidd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-nonidd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-nonidd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
