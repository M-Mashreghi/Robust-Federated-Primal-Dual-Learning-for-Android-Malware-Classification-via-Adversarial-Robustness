{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def create_clientsss(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list: a list of binarized labels for each image\n",
    "            num_client: number of federated members (clients)\n",
    "            initials: the clients' name prefix, e.g., clients_1 \n",
    "    '''\n",
    "\n",
    "    # create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "\n",
    "    # shuffle the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # shard data and place at each client with non-i.i.d distribution of classes\n",
    "    shards = [[] for _ in range(num_clients)]\n",
    "    classes = set(tuple(label) for label in label_list)\n",
    "    classes_per_client = max(1, len(classes) // num_clients)\n",
    "\n",
    "    # Count occurrences of each class within each client\n",
    "    class_counts_per_client = {client_name: {class_tuple: 0 for class_tuple in classes} for client_name in client_names}\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        # Convert the set to a list before using random.sample\n",
    "        class_list = list(classes)\n",
    "        \n",
    "        # Select classes for this client\n",
    "        client_classes = random.sample(class_list, min(classes_per_client, len(class_list)))\n",
    "        \n",
    "        # Select data points for the selected classes\n",
    "        for image, label in data:\n",
    "            if tuple(label) in client_classes:\n",
    "                shards[i].append((image, label))\n",
    "                class_counts_per_client[client_names[i]][tuple(label)] += 1\n",
    "\n",
    "        # Print the distribution of classes within the client\n",
    "        class_distribution = class_counts_per_client[client_names[i]]\n",
    "        print(f\"Client {client_names[i]}: {class_distribution}\")\n",
    "\n",
    "    # number of clients must equal the number of shards\n",
    "    assert len(shards) == len(client_names)\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96781342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_clients_non_iid(image_list, label_list, num_clients=10, alpha=5, min_samples_per_client=32, initial='clients'):\n",
    "    \"\"\"\n",
    "    Return a dictionary with keys as clients' names and values as data shards,\n",
    "    represented as tuples of images and label lists. The data distribution among\n",
    "    clients is non-identically distributed (non-IID), and each client has at least\n",
    "    32 data points.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): A list of numpy arrays of training images.\n",
    "        label_list (list): A list of binarized labels for each image.\n",
    "        num_clients (int): Number of federated members (clients).\n",
    "        alpha (float): Concentration parameter for the Dirichlet distribution.\n",
    "        min_samples_per_client (int): Minimum number of data points per client.\n",
    "        initial (str): The clients' name prefix, e.g., 'clients_1'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping client names to non-IID data shards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "\n",
    "    # Randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "    print(len(data))\n",
    "    # Define a Dirichlet distribution to sample proportions for each client\n",
    "    proportions = np.random.dirichlet([alpha] * num_clients)\n",
    "    print(\"num clients\" , num_clients)\n",
    "    print(\"proportions \" , proportions)\n",
    "    # Calculate the number of samples each client should get\n",
    "    total_samples = len(data)\n",
    "    client_samples = (proportions * total_samples).astype(int)\n",
    "    print(\"client_samples \" , client_samples)\n",
    "    # Ensure each client gets at least min_samples_per_client\n",
    "    for i in range(num_clients):\n",
    "        if client_samples[i] < min_samples_per_client:\n",
    "            client_samples[i] = min_samples_per_client\n",
    "\n",
    "    shards = [data[i:i + size] for i, size in enumerate(client_samples)]\n",
    "\n",
    "    # Number of clients must equal the number of shards\n",
    "    assert len(shards) == len(client_names)\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        # Print the number of data points for each class in the current client\n",
    "        class_counts = {cls: sum(1 for _, label in shards[i] if label == cls) for cls in set(label_list)}\n",
    "        print(f\"Client {client_names[i]}: {class_counts}\")\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(num_clients)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ac7b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_clients_non_iid(image_list, label_list, num_clients=10, min_samples_per_client=32, initial='clients'):\n",
    "    \"\"\"\n",
    "    Return a dictionary with keys as clients' names and values as data shards,\n",
    "    represented as tuples of images and label lists. The data distribution among\n",
    "    clients is non-identically distributed (non-IID), and each client has at least\n",
    "    32 data points.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): A list of numpy arrays of training images.\n",
    "        label_list (list): A list of binarized labels for each image.\n",
    "        num_clients (int): Number of federated members (clients).\n",
    "        min_samples_per_client (int): Minimum number of data points per client.\n",
    "        initial (str): The clients' name prefix, e.g., 'clients_1'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping client names to non-IID data shards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "\n",
    "    # Randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    shards = []\n",
    "    for i in range(num_clients):\n",
    "        # Generate a random alpha value for each client\n",
    "        alpha = random.uniform(0.1, 5.0)\n",
    "        \n",
    "        # Define a Dirichlet distribution to sample proportions for each class within the client\n",
    "        class_proportions = np.random.dirichlet([alpha] * len(set(label_list)))\n",
    "        \n",
    "        # Calculate the number of samples each client should get\n",
    "        total_samples = len(data)\n",
    "        client_samples = int((total_samples * 1.0 / num_clients) + 0.5)\n",
    "        \n",
    "        # Ensure each client gets at least min_samples_per_client\n",
    "        client_samples = max(client_samples, min_samples_per_client)\n",
    "        \n",
    "        # Sample data for the current client using the class proportions\n",
    "        client_data = []\n",
    "        for cls in set(label_list):\n",
    "            class_samples = int(client_samples * class_proportions[cls] + 0.5)\n",
    "            class_data = [item for item in data if item[1] == cls][:class_samples]\n",
    "            client_data.extend(class_data)\n",
    "        \n",
    "        shards.append(client_data)\n",
    "\n",
    "        # Print the number of data points for each class in the current client\n",
    "        class_counts = {cls: sum(1 for _, label in client_data if label == cls) for cls in set(label_list)}\n",
    "        print(f\"Client {client_names[i]}: {class_counts}\")\n",
    "\n",
    "    # Number of clients must equal the number of shards\n",
    "    assert len(shards) == len(client_names)\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c4682f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_clients_non_iid3(image_list, label_list, num_clients=10, min_samples_per_client=32, initial='clients'):\n",
    "    \"\"\"\n",
    "    Return a dictionary with keys as clients' names and values as data shards,\n",
    "    represented as tuples of images and label lists. The data distribution among\n",
    "    clients is non-identically distributed (non-IID), and each client has at least\n",
    "    32 data points.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): A list of numpy arrays of training images.\n",
    "        label_list (list): A list of binarized labels for each image.\n",
    "        num_clients (int): Number of federated members (clients).\n",
    "        min_samples_per_client (int): Minimum number of data points per client.\n",
    "        initial (str): The clients' name prefix, e.g., 'clients_1'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping client names to non-IID data shards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "\n",
    "    # Randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    shards = []\n",
    "    for i in range(num_clients):\n",
    "        # Generate a random alpha value for each client\n",
    "        alpha = random.uniform(0.1, 5.0)\n",
    "        \n",
    "        # Define a Dirichlet distribution to sample proportions for each class within the client\n",
    "        class_proportions = np.random.dirichlet([alpha] * len(set(label_list)))\n",
    "        \n",
    "        # Calculate the number of samples each client should get\n",
    "        total_samples = len(data)\n",
    "        random_factor = random.uniform(0.5, 1.5)  # Adjust the range as needed\n",
    "        client_samples = int((total_samples * random_factor / num_clients) + 0.5)\n",
    "        \n",
    "        # Ensure each client gets at least min_samples_per_client\n",
    "        client_samples = max(client_samples, min_samples_per_client)\n",
    "        \n",
    "        # Sample data for the current client using the class proportions\n",
    "        client_data = []\n",
    "        for cls in set(label_list):\n",
    "            class_samples = int(client_samples * class_proportions[cls] + 0.5)\n",
    "            class_data = [item for item in data if item[1] == cls][:class_samples]\n",
    "            client_data.extend(class_data)\n",
    "        \n",
    "        shards.append(client_data)\n",
    "\n",
    "        # Print the number of data points for each class in the current client\n",
    "        class_counts = {cls: sum(1 for _, label in client_data if label == cls) for cls in set(label_list)}\n",
    "        print(f\"Client {client_names[i]}: {class_counts}, Total Samples: {len(client_data)}\")\n",
    "\n",
    "    # Number of clients must equal the number of shards\n",
    "    assert len(shards) == len(client_names)\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9872c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients_non_iid3(image_list, label_list, num_clients=10, min_samples_per_client=32, initial='clients'):\n",
    "    \"\"\"\n",
    "    Return a dictionary with keys as clients' names and values as data shards,\n",
    "    represented as tuples of images and label lists. The data distribution among\n",
    "    clients is non-identically distributed (non-IID), and each client has at least\n",
    "    32 data points.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): A list of numpy arrays of training images.\n",
    "        label_list (list): A list of binarized labels for each image.\n",
    "        num_clients (int): Number of federated members (clients).\n",
    "        min_samples_per_client (int): Minimum number of data points per client.\n",
    "        initial (str): The clients' name prefix, e.g., 'clients_1'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping client names to non-IID data shards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "    # If X_train is a Pandas DataFrame\n",
    "    num_train_samples = len(image_list)\n",
    "    print(\"Number of data points in the training set: \", num_train_samples)\n",
    "\n",
    "    # Randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    shards = []\n",
    "    for i in range(num_clients):\n",
    "        # Generate a random alpha value for each client\n",
    "        alpha = random.uniform(0.1, 5.0)\n",
    "        \n",
    "        # Define a Dirichlet distribution to sample proportions for each class within the client\n",
    "        class_proportions = np.random.dirichlet([alpha] * len(set(label_list)))\n",
    "        \n",
    "        # Calculate the number of samples each client should get\n",
    "        total_samples = len(data)\n",
    "        random_factor = random.uniform(0.5, 1.5)  # Adjust the range as needed\n",
    "        client_samples = int((total_samples * random_factor / num_clients) + 0.5)\n",
    "        \n",
    "        # Ensure each client gets at least min_samples_per_client\n",
    "        client_samples = max(client_samples, min_samples_per_client)\n",
    "        \n",
    "        # Sample data for the current client using the class proportions without replacement\n",
    "        client_data = []\n",
    "        for cls in set(label_list):\n",
    "            class_samples = int(client_samples * class_proportions[cls] + 0.5)\n",
    "            class_data = random.sample([item for item in data if item[1] == cls], min(class_samples, len(data)))\n",
    "            client_data.extend(class_data)\n",
    "        \n",
    "        shards.append(client_data)\n",
    "\n",
    "        # Print the number of data points for each class in the current client\n",
    "        class_counts = {cls: sum(1 for _, label in client_data if label == cls) for cls in set(label_list)}\n",
    "        print(f\"Client {client_names[i]}: {class_counts}, Total Samples: {len(client_data)}\")\n",
    "\n",
    "    # Number of clients must equal the number of shards\n",
    "    assert len(shards) == len(client_names)\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(num_clients)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f99a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_clients_non_iid3(image_list, label_list, num_clients=10, min_samples_per_client=32, initial='clients'):\n",
    "    \"\"\"\n",
    "    Return a dictionary with keys as clients' names and values as data shards,\n",
    "    represented as tuples of images and label lists. The data distribution among\n",
    "    clients is non-identically distributed (non-IID), and each client has at least\n",
    "    32 data points.\n",
    "\n",
    "    Args:\n",
    "        image_list (list): A list of numpy arrays of training images.\n",
    "        label_list (list): A list of binarized labels for each image.\n",
    "        num_clients (int): Number of federated members (clients).\n",
    "        min_samples_per_client (int): Minimum number of data points per client.\n",
    "        initial (str): The clients' name prefix, e.g., 'clients_1'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping client names to non-IID data shards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i + 1) for i in range(num_clients)]\n",
    "\n",
    "    # Randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    shards = []\n",
    "    for i in range(num_clients - 1):  # For all clients except the last one\n",
    "        # Generate a random alpha value for each client\n",
    "        alpha = random.uniform(0.1, 5.0)\n",
    "        \n",
    "        # Define a Dirichlet distribution to sample proportions for each class within the client\n",
    "        class_proportions = np.random.dirichlet([alpha] * len(set(label_list)))\n",
    "        \n",
    "        # Calculate the number of samples each client should get\n",
    "        total_samples = len(data)\n",
    "        random_factor = random.uniform(0.5, 1.5)  # Adjust the range as needed\n",
    "        client_samples = int(min_samples_per_client + (total_samples - num_clients * min_samples_per_client) * random_factor)\n",
    "        \n",
    "        # Ensure each client gets at least min_samples_per_client\n",
    "        client_samples = max(client_samples, min_samples_per_client)\n",
    "        \n",
    "        # Sample data for the current client using the class proportions without replacement\n",
    "        client_data = []\n",
    "        for cls in set(label_list):\n",
    "            class_samples = int(client_samples * class_proportions[cls] + 0.5)\n",
    "            class_data = random.sample([item for item in data if item[1] == cls], max(0, min(class_samples, len(data))))\n",
    "            client_data.extend(class_data)\n",
    "\n",
    "        # Print the number of data points for each class in the current client\n",
    "        class_counts = {cls: sum(1 for _, label in client_data if label == cls) for cls in set(label_list)}\n",
    "        print(f\"Client {client_names[i]}: {class_counts}, Total Samples: {len(client_data)}\")\n",
    "\n",
    "        # Remove sampled data from the main list\n",
    "        data = [item for item in data if not any(np.array_equal(item[0], client_item[0]) and item[1] == client_item[1] for client_item in client_data)]\n",
    "\n",
    "    # The last client gets all the remaining data\n",
    "    shards.append(data)\n",
    "\n",
    "    # Print the number of data points for each class in the last client\n",
    "    class_counts = {cls: sum(1 for _, label in data if label == cls) for cls in set(label_list)}\n",
    "    print(f\"Client {client_names[-1]}: {class_counts}, Total Samples: {len(data)}\")\n",
    "\n",
    "    # Number of clients must equal the number of shards\n",
    "    assert len(shards) == len(client_names)\n",
    "\n",
    "    return {client_names[i]: shards[i] for i in range(num_clients)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0c136",
   "metadata": {},
   "source": [
    "## fedavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "Number of data points in the training set:  12028\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 78\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of data points in the training set: \u001b[39m\u001b[39m\"\u001b[39m, num_train_samples)\n\u001b[0;32m     77\u001b[0m \u001b[39m# create clients -- Horizontal FL\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m clients \u001b[39m=\u001b[39m create_clients_non_iid3(X_train, [\u001b[39mtuple\u001b[39;49m(label) \u001b[39mfor\u001b[39;49;00m label \u001b[39min\u001b[39;49;00m y_train\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m)\u001b[39m.\u001b[39;49mtolist()], num_clients\u001b[39m=\u001b[39;49mnumber_of_clients, initial\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclient\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     81\u001b[0m \u001b[39m# Calculate total number of data points\u001b[39;00m\n\u001b[0;32m     82\u001b[0m total_data_points \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mlen\u001b[39m(data) \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m clients\u001b[39m.\u001b[39mvalues())\n",
      "Cell \u001b[1;32mIn[110], line 50\u001b[0m, in \u001b[0;36mcreate_clients_non_iid3\u001b[1;34m(image_list, label_list, num_clients, min_samples_per_client, initial)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(label_list):\n\u001b[0;32m     49\u001b[0m     class_samples \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(client_samples \u001b[39m*\u001b[39m class_proportions[\u001b[39mcls\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     class_data \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49msample([item \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m data \u001b[39mif\u001b[39;49;00m item[\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39;49m \u001b[39mcls\u001b[39;49m], \u001b[39mmax\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mmin\u001b[39;49m(class_samples, \u001b[39mlen\u001b[39;49m(data))))\n\u001b[0;32m     51\u001b[0m     client_data\u001b[39m.\u001b[39mextend(class_data)\n\u001b[0;32m     53\u001b[0m \u001b[39m# Print the number of data points for each class in the current client\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\random.py:456\u001b[0m, in \u001b[0;36mRandom.sample\u001b[1;34m(self, population, k, counts)\u001b[0m\n\u001b[0;32m    454\u001b[0m randbelow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m k \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n:\n\u001b[1;32m--> 456\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSample larger than population or is negative\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    457\u001b[0m result \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m k\n\u001b[0;32m    458\u001b[0m setsize \u001b[39m=\u001b[39m \u001b[39m21\u001b[39m        \u001b[39m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # If X_train is a Pandas DataFrame\n",
    "            num_train_samples = len(X_train)\n",
    "\n",
    "            print(\"Number of data points in the training set: \", num_train_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients_non_iid3(X_train, [tuple(label) for label in y_train.astype(int).tolist()], num_clients=number_of_clients, initial='client')\n",
    "\n",
    "\n",
    "            # Calculate total number of data points\n",
    "            total_data_points = sum(len(data) for data in clients.values())\n",
    "            print(f\"Total number of data points: {total_data_points}\")\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
